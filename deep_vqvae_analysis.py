#!/usr/bin/env python3
"""
VQ-VAEæ·±åº¦åˆ†æå·¥å…·
å½»åº•æ£€æŸ¥VQ-VAEæ¨¡å‹çš„å„ä¸ªæ–¹é¢ï¼Œç¡®å®šé—®é¢˜æ‰€åœ¨
"""

import os
import sys
import torch
import torch.nn.functional as F
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.append('VAE')
from adv_vq_vae import AdvVQVAE
from dataset import build_dataloader

def check_model_weights(model_path):
    """æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶"""
    print("=== æ¨¡å‹æƒé‡åˆ†æ ===")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    try:
        # åŠ è½½æƒé‡
        state_dict = torch.load(model_path, map_location='cpu')
        
        print(f"âœ… æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"æƒé‡é”®æ•°é‡: {len(state_dict.keys())}")
        
        # æ£€æŸ¥å…³é”®æƒé‡
        key_weights = {
            'encoder': [k for k in state_dict.keys() if 'encoder' in k],
            'decoder': [k for k in state_dict.keys() if 'decoder' in k],
            'vq': [k for k in state_dict.keys() if 'vq' in k or 'embedding' in k],
            'discriminator': [k for k in state_dict.keys() if 'discriminator' in k]
        }
        
        for component, keys in key_weights.items():
            print(f"{component}: {len(keys)} ä¸ªæƒé‡")
            if len(keys) > 0:
                # æ£€æŸ¥æƒé‡ç»Ÿè®¡
                weights = torch.cat([state_dict[k].flatten() for k in keys[:3]])  # å‰3ä¸ªæƒé‡
                print(f"  æƒé‡èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]")
                print(f"  æƒé‡å‡å€¼: {weights.mean():.4f}, æ ‡å‡†å·®: {weights.std():.4f}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                if torch.isnan(weights).any():
                    print(f"  âŒ å‘ç°NaNå€¼")
                if torch.isinf(weights).any():
                    print(f"  âŒ å‘ç°æ— ç©·å€¼")
                if weights.std() < 1e-6:
                    print(f"  âš  æƒé‡æ–¹å·®è¿‡å°ï¼Œå¯èƒ½æœªè®­ç»ƒ")
                if weights.std() > 10:
                    print(f"  âš  æƒé‡æ–¹å·®è¿‡å¤§ï¼Œå¯èƒ½æ¢¯åº¦çˆ†ç‚¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def analyze_data_quality(dataloader):
    """åˆ†ææ•°æ®è´¨é‡"""
    print("\n=== æ•°æ®è´¨é‡åˆ†æ ===")
    
    sample_batch = next(iter(dataloader))
    images, labels = sample_batch
    
    print(f"æ‰¹æ¬¡å½¢çŠ¶: {images.shape}")
    print(f"æ•°æ®ç±»å‹: {images.dtype}")
    print(f"æ ‡ç­¾èŒƒå›´: {labels.min()} - {labels.max()}")
    
    # æ•°æ®ç»Ÿè®¡
    print(f"å›¾åƒç»Ÿè®¡:")
    print(f"  å‡å€¼: {images.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {images.std():.4f}")
    print(f"  æœ€å°å€¼: {images.min():.4f}")
    print(f"  æœ€å¤§å€¼: {images.max():.4f}")
    
    # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    if images.min() < -1.1 or images.max() > 1.1:
        print("  âš  æ•°æ®è¶…å‡ºé¢„æœŸèŒƒå›´[-1,1]")
    
    if abs(images.mean()) > 0.1:
        print("  âš  æ•°æ®å‡å€¼åç¦»0è¾ƒå¤š")
    
    if images.std() < 0.1 or images.std() > 1.0:
        print("  âš  æ•°æ®æ ‡å‡†å·®å¼‚å¸¸")
    
    # ä¿å­˜æ ·æœ¬å›¾åƒç”¨äºæ£€æŸ¥
    os.makedirs('vqvae_analysis', exist_ok=True)
    
    # åå½’ä¸€åŒ–åˆ°[0,1]ç”¨äºä¿å­˜
    sample_images = (images[:8] + 1) / 2
    grid = vutils.make_grid(sample_images, nrow=4, normalize=False, padding=2)
    vutils.save_image(grid, 'vqvae_analysis/data_samples.png')
    print(f"  æ•°æ®æ ·æœ¬å·²ä¿å­˜åˆ°: vqvae_analysis/data_samples.png")
    
    return images, labels

def analyze_model_architecture(model):
    """åˆ†ææ¨¡å‹æ¶æ„"""
    print("\n=== æ¨¡å‹æ¶æ„åˆ†æ ===")
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # åˆ†æå„ç»„ä»¶å‚æ•°
    encoder_params = sum(p.numel() for p in model.encoder.parameters())
    decoder_params = sum(p.numel() for p in model.decoder.parameters())
    vq_params = sum(p.numel() for p in model.vq.parameters())
    
    print(f"ç¼–ç å™¨å‚æ•°: {encoder_params:,} ({encoder_params/total_params*100:.1f}%)")
    print(f"è§£ç å™¨å‚æ•°: {decoder_params:,} ({decoder_params/total_params*100:.1f}%)")
    print(f"é‡åŒ–å™¨å‚æ•°: {vq_params:,} ({vq_params/total_params*100:.1f}%)")
    
    if hasattr(model, 'discriminator'):
        disc_params = sum(p.numel() for p in model.discriminator.parameters())
        print(f"åˆ¤åˆ«å™¨å‚æ•°: {disc_params:,} ({disc_params/total_params*100:.1f}%)")

def test_forward_pass(model, sample_images, device):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n=== å‰å‘ä¼ æ’­æµ‹è¯• ===")
    
    model.eval()
    sample_images = sample_images.to(device)
    
    with torch.no_grad():
        try:
            # ç¼–ç 
            z_e = model.encoder(sample_images)
            print(f"âœ… ç¼–ç æˆåŠŸ: {sample_images.shape} -> {z_e.shape}")
            
            # é‡åŒ–
            z_q, commitment_loss, codebook_loss = model.vq(z_e)
            print(f"âœ… é‡åŒ–æˆåŠŸ: {z_e.shape} -> {z_q.shape}")
            print(f"  Commitment Loss: {commitment_loss:.6f}")
            print(f"  Codebook Loss: {codebook_loss:.6f}")
            
            # è§£ç 
            reconstructed = model.decoder(z_q)
            print(f"âœ… è§£ç æˆåŠŸ: {z_q.shape} -> {reconstructed.shape}")
            
            # æ£€æŸ¥è¾“å‡º
            print(f"é‡å»ºå›¾åƒç»Ÿè®¡:")
            print(f"  å‡å€¼: {reconstructed.mean():.4f}")
            print(f"  æ ‡å‡†å·®: {reconstructed.std():.4f}")
            print(f"  èŒƒå›´: [{reconstructed.min():.4f}, {reconstructed.max():.4f}]")
            
            return z_e, z_q, reconstructed, commitment_loss, codebook_loss
            
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            return None

def analyze_reconstruction_quality(original, reconstructed):
    """åˆ†æé‡å»ºè´¨é‡"""
    print("\n=== é‡å»ºè´¨é‡åˆ†æ ===")
    
    # ç¡®ä¿tensorsåœ¨åŒä¸€è®¾å¤‡ä¸Š
    if original.device != reconstructed.device:
        reconstructed = reconstructed.to(original.device)
    
    # é€æ ·æœ¬åˆ†æ
    batch_size = original.shape[0]
    mse_list = []
    psnr_list = []
    
    for i in range(batch_size):
        orig = original[i:i+1]
        recon = reconstructed[i:i+1]
        
        mse = F.mse_loss(orig, recon).item()
        # å¯¹äº[-1,1]èŒƒå›´ï¼Œæœ€å¤§å€¼æ˜¯2
        psnr = 20 * torch.log10(torch.tensor(2.0) / torch.sqrt(torch.tensor(mse))).item()
        
        mse_list.append(mse)
        psnr_list.append(psnr)
    
    avg_mse = np.mean(mse_list)
    avg_psnr = np.mean(psnr_list)
    std_mse = np.std(mse_list)
    std_psnr = np.std(psnr_list)
    
    print(f"é‡å»ºè´¨é‡ç»Ÿè®¡:")
    print(f"  å¹³å‡MSE: {avg_mse:.6f} Â± {std_mse:.6f}")
    print(f"  å¹³å‡PSNR: {avg_psnr:.2f} Â± {std_psnr:.2f} dB")
    print(f"  MSEèŒƒå›´: [{min(mse_list):.6f}, {max(mse_list):.6f}]")
    print(f"  PSNRèŒƒå›´: [{min(psnr_list):.2f}, {max(psnr_list):.2f}]")
    
    # è´¨é‡è¯„ä¼°
    if avg_psnr > 25:
        print("âœ… é‡å»ºè´¨é‡ä¼˜ç§€")
    elif avg_psnr > 20:
        print("âœ… é‡å»ºè´¨é‡è‰¯å¥½")
    elif avg_psnr > 15:
        print("âš  é‡å»ºè´¨é‡ä¸€èˆ¬")
    else:
        print("âŒ é‡å»ºè´¨é‡å·®")
    
    # ä¿å­˜å¯¹æ¯”å›¾åƒ
    # ç§»åŠ¨åˆ°CPUè¿›è¡Œå›¾åƒä¿å­˜
    original_cpu = original[:4].cpu()
    reconstructed_cpu = reconstructed[:4].cpu()
    
    comparison = torch.cat([
        (original_cpu + 1) / 2,  # åŸå›¾
        (reconstructed_cpu + 1) / 2  # é‡å»ºå›¾
    ], dim=0)
    
    grid = vutils.make_grid(comparison, nrow=4, normalize=False, padding=2)
    vutils.save_image(grid, 'vqvae_analysis/reconstruction_comparison.png')
    print(f"  é‡å»ºå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: vqvae_analysis/reconstruction_comparison.png")
    
    return avg_mse, avg_psnr

def analyze_codebook_usage(model, z_e):
    """åˆ†æcodebookä½¿ç”¨æƒ…å†µ"""
    print("\n=== Codebookä½¿ç”¨åˆ†æ ===")
    
    with torch.no_grad():
        # è·å–ç¼–ç ç´¢å¼•
        z_e_flat = z_e.permute(0,2,3,1).contiguous().view(-1, model.vq.embedding_dim)
        
        # è®¡ç®—è·ç¦»
        distances = torch.sum((z_e_flat.unsqueeze(1) - model.vq.embedding.weight.unsqueeze(0))**2, dim=2)
        encoding_indices = torch.argmin(distances, dim=1)
        
        # ç»Ÿè®¡ä½¿ç”¨æƒ…å†µ
        unique_indices = torch.unique(encoding_indices)
        usage_count = torch.bincount(encoding_indices, minlength=model.vq.num_embeddings)
        
        used_embeddings = len(unique_indices)
        total_embeddings = model.vq.num_embeddings
        usage_rate = used_embeddings / total_embeddings
        
        print(f"Codebookä½¿ç”¨ç»Ÿè®¡:")
        print(f"  ä½¿ç”¨çš„embedding: {used_embeddings}/{total_embeddings}")
        print(f"  ä½¿ç”¨ç‡: {usage_rate:.2%}")
        print(f"  å¹³å‡ä½¿ç”¨æ¬¡æ•°: {usage_count[usage_count > 0].float().mean():.1f}")
        print(f"  æœ€å¤§ä½¿ç”¨æ¬¡æ•°: {usage_count.max().item()}")
        print(f"  ä½¿ç”¨æ¬¡æ•°æ ‡å‡†å·®: {usage_count[usage_count > 0].float().std():.1f}")
        
        # è¯„ä¼°codebookè´¨é‡
        if usage_rate < 0.05:
            print("âŒ Codebookå¡Œé™·ä¸¥é‡")
        elif usage_rate < 0.1:
            print("âš  Codebookå¡Œé™·")
        elif usage_rate < 0.3:
            print("âš  Codebookåˆ©ç”¨ç‡åä½")
        else:
            print("âœ… Codebookåˆ©ç”¨ç‡æ­£å¸¸")

def analyze_gradient_flow(model, sample_images, device):
    """åˆ†ææ¢¯åº¦æµåŠ¨"""
    print("\n=== æ¢¯åº¦æµåŠ¨åˆ†æ ===")
    
    model.train()
    sample_images = sample_images.to(device)
    sample_images.requires_grad_(True)
    
    try:
        # å‰å‘ä¼ æ’­
        z_e = model.encoder(sample_images)
        z_q, commitment_loss, codebook_loss = model.vq(z_e)
        reconstructed = model.decoder(z_q)
        
        # è®¡ç®—é‡å»ºæŸå¤±
        recon_loss = F.mse_loss(reconstructed, sample_images)
        total_loss = recon_loss + commitment_loss + codebook_loss
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        encoder_grad_norm = 0
        decoder_grad_norm = 0
        vq_grad_norm = 0
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                if 'encoder' in name:
                    encoder_grad_norm += grad_norm
                elif 'decoder' in name:
                    decoder_grad_norm += grad_norm
                elif 'vq' in name or 'embedding' in name:
                    vq_grad_norm += grad_norm
        
        print(f"æ¢¯åº¦èŒƒæ•°:")
        print(f"  ç¼–ç å™¨: {encoder_grad_norm:.6f}")
        print(f"  è§£ç å™¨: {decoder_grad_norm:.6f}")
        print(f"  é‡åŒ–å™¨: {vq_grad_norm:.6f}")
        
        # è¯„ä¼°æ¢¯åº¦
        if encoder_grad_norm < 1e-6 or decoder_grad_norm < 1e-6:
            print("âŒ æ¢¯åº¦æ¶ˆå¤±")
        elif encoder_grad_norm > 100 or decoder_grad_norm > 100:
            print("âŒ æ¢¯åº¦çˆ†ç‚¸")
        else:
            print("âœ… æ¢¯åº¦æ­£å¸¸")
            
        model.zero_grad()
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦åˆ†æå¤±è´¥: {e}")

def comprehensive_vqvae_analysis():
    """å…¨é¢çš„VQ-VAEåˆ†æ"""
    print("ğŸ” å¼€å§‹VQ-VAEæ·±åº¦åˆ†æ...\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. æ£€æŸ¥æ¨¡å‹æƒé‡ - é€‚åº”ä¸åŒç¯å¢ƒ
    possible_model_paths = [
        "/kaggle/input/adv-vqvae-best-fid-pth/adv_vqvae_best_fid.pth",  # Kaggleè·¯å¾„
        "VAE/adv_vqvae_best_fid.pth",  # æœ¬åœ°è·¯å¾„
        "VAE/checkpoints/adv_vqvae_best_fid.pth",  # å¤‡é€‰è·¯å¾„
        "adv_vqvae_best_fid.pth"  # å½“å‰ç›®å½•
    ]
    
    model_path = None
    for path in possible_model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        print("âŒ æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„ï¼š")
        for path in possible_model_paths:
            print(f"   {path}")
        print("\nè¯·å°†æ¨¡å‹æ–‡ä»¶æ”¾ç½®åœ¨ä¸Šè¿°è·¯å¾„ä¹‹ä¸€ï¼Œæˆ–ä¿®æ”¹ä»£ç ä¸­çš„è·¯å¾„")
        return
    
    print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
    
    if not check_model_weights(model_path):
        print("\nâŒ æ¨¡å‹æƒé‡æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
        return
    
    # 2. åŠ è½½æ•°æ® - é€‚åº”ä¸åŒç¯å¢ƒ
    possible_dataset_paths = [
        "/kaggle/input/dataset",  # Kaggleè·¯å¾„
        "VAE/dataset",  # æœ¬åœ°è·¯å¾„
        "dataset",  # å½“å‰ç›®å½•
        "../dataset"  # ä¸Šçº§ç›®å½•
    ]
    
    dataset_path = None
    for path in possible_dataset_paths:
        if os.path.exists(path):
            dataset_path = path
            break
    
    if dataset_path is None:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„ï¼š")
        for path in possible_dataset_paths:
            print(f"   {path}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºåˆ†æ
        print("\nâš  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œåˆ†æ...")
        sample_images = torch.randn(8, 3, 256, 256) * 0.5  # [-1, 1]èŒƒå›´
        sample_labels = torch.randint(0, 2, (8,))
        print(f"æ¨¡æ‹Ÿæ•°æ®: {sample_images.shape}, æ ‡ç­¾: {sample_labels.shape}")
    else:
        try:
            print(f"âœ… æ‰¾åˆ°æ•°æ®é›†: {dataset_path}")
            train_loader, val_loader, train_size, val_size = build_dataloader(
                dataset_path, batch_size=8, num_workers=2, val_split=0.2
            )
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: è®­ç»ƒé›† {train_size}, éªŒè¯é›† {val_size}")
            
            # 3. åˆ†ææ•°æ®è´¨é‡
            sample_images, sample_labels = analyze_data_quality(val_loader)
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œåˆ†æ...")
            sample_images = torch.randn(8, 3, 256, 256) * 0.5
            sample_labels = torch.randint(0, 2, (8,))
    
    # 4. åˆ›å»ºå’ŒåŠ è½½æ¨¡å‹
    model = AdvVQVAE(
        in_channels=3, latent_dim=256, num_embeddings=512,
        beta=0.25, decay=0.99, groups=32
    ).to(device)
    
    try:
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
        print(f"\nâœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
        return
    
    # 5. åˆ†ææ¨¡å‹æ¶æ„
    analyze_model_architecture(model)
    
    # 6. æµ‹è¯•å‰å‘ä¼ æ’­
    forward_results = test_forward_pass(model, sample_images, device)
    if forward_results is None:
        return
    
    z_e, z_q, reconstructed, commitment_loss, codebook_loss = forward_results
    
    # 7. åˆ†æé‡å»ºè´¨é‡
    avg_mse, avg_psnr = analyze_reconstruction_quality(sample_images, reconstructed)
    
    # 8. åˆ†æcodebookä½¿ç”¨
    analyze_codebook_usage(model, z_e)
    
    # 9. åˆ†ææ¢¯åº¦æµåŠ¨
    analyze_gradient_flow(model, sample_images[:4], device)  # ä½¿ç”¨å°‘é‡æ ·æœ¬
    
    # 10. æ€»ç»“åˆ†æç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("="*50)
    
    print(f"âœ… æ½œåœ¨ç©ºé—´å°ºå¯¸: {z_q.shape} (æ­£ç¡®)")
    print(f"{'âœ…' if avg_psnr > 15 else 'âŒ'} é‡å»ºè´¨é‡: PSNR = {avg_psnr:.2f} dB")
    print(f"âœ… æ¨¡å‹æ¶æ„: æ­£å¸¸è¿è¡Œ")
    print(f"âœ… å‰å‘ä¼ æ’­: æˆåŠŸ")
    
    if avg_psnr > 20:
        print("\nğŸ‰ VQ-VAEæ¨¡å‹è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ç”¨äºCLDMè®­ç»ƒ")
    elif avg_psnr > 15:
        print("\nâš  VQ-VAEæ¨¡å‹è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ”¹è¿›ä½†å¯ä»¥ç”¨äºCLDMè®­ç»ƒ")
    else:
        print("\nâŒ VQ-VAEæ¨¡å‹è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ")
        print("   ä½†ç”±äºæ½œåœ¨ç©ºé—´å°ºå¯¸æ­£ç¡®ï¼Œä»å¯å°è¯•CLDMè®­ç»ƒ")
    
    print(f"\nåˆ†æç»“æœå·²ä¿å­˜åˆ°: vqvae_analysis/ç›®å½•")

if __name__ == "__main__":
    comprehensive_vqvae_analysis() 