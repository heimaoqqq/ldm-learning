# LDM (Latent Diffusion Model) 配置文件

# VAE 相关配置 (使用预训练的VQ-VAE)
vae:
  model_path: "/kaggle/input/vae-best-fid/adv_vqvae_best_fid.pth"  # Kaggle VAE模型路径
  in_channels: 3
  latent_dim: 256
  num_embeddings: 512
  beta: 0.25
  vq_ema_decay: 0.99
  groups: 32
  freeze: true  # 冻结VAE参数

# 数据集配置
dataset:
  root_dir: "/kaggle/input/dataset"  # Kaggle数据集路径
  batch_size: 6  # 🎯 平衡点：保持合理batch size但避免OOM
  num_workers: 2  # 保持2个workers减少CPU内存占用
  val_split: 0.2
  image_size: 256

# 扩散模型配置
diffusion:
  timesteps: 1000  # 训练时总扩散步数
  noise_schedule: "cosine"  # 🎯 保持余弦调度
  beta_start: 0.00085  # 🎯 较小起始值
  beta_end: 0.012  # 🎯 较小终止值
  scheduler_type: "ddim"  # 🆕 使用DDIM调度器 (ddpm/ddim)
  clip_denoised: true

# U-Net 网络结构配置
unet:
  # 输入维度
  in_channels: 256  # 潜在空间通道数，与VAE的latent_dim一致
  out_channels: 256  # 输出通道数 (预测噪声)
  model_channels: 192  # 🔧 修复：192能被8,16,24,32等多种头数整除
  
  # 时间嵌入
  time_embed_dim: 768  # 🔧 修复：768 = 192 * 4，保持比例关系
  
  # 条件嵌入 (身份标签)
  num_classes: 31  # 类别数量 (ID_1到ID_31，共31个用户)
  class_embed_dim: 384  # 🔧 修复：384 = 192 * 2，保持比例关系
  
  # 网络层配置
  num_res_blocks: 3  # 🎯 保持3层，这对质量很重要
  attention_resolutions: [8, 16]  # 🎯 去掉最细粒度[4]，注意力很耗内存
  channel_mult: [1, 2, 4, 6]  # 🎯 降低最后一层：[1,2,4,8]→[1,2,4,6]
  num_heads: 8  # 🔧 修复：192能被8整除，192/8=24(每个头的维度)
  use_scale_shift_norm: true  # 使用条件归一化
  dropout: 0.1  # Dropout率
  
  # 条件机制
  use_cross_attention: true  # 🎯 保持交叉注意力，这对条件生成很重要
  use_self_attention: true   # 🎯 保持自注意力

# 训练配置
training:
  epochs: 500
  lr: 0.000005  # 🔧 进一步降低学习率，防止数值爆炸
  weight_decay: 0.01
  warmup_steps: 2000  # 🔧 增加预热步数，更稳定的开始
  
  # 🆕 梯度累积配置
  gradient_accumulation_steps: 1  # 🔧 先用1步累积，稳定后再增加
  
  # 保存设置
  save_dir: "ldm_models"
  save_interval: 50  # 每隔多少epoch保存一次
  sample_interval: 15  # 🎯 略微降低采样频率节省内存
  log_interval: 100  # 每隔多少步打印日志
  
  # 损失权重
  noise_loss_weight: 1.0
  
  # Classifier-Free Guidance
  use_cfg: true  # 使用无分类器引导
  cfg_dropout_prob: 0.1  # 🔧 降低CFG dropout，减少训练困难
  
  # 梯度裁剪
  grad_clip_norm: 0.2  # 🔧 更严格的梯度裁剪
  
  # 调度器
  use_scheduler: true
  scheduler_type: "cosine_with_restarts"  # 🎯 保持优化调度器

# FID评估配置
fid_evaluation:
  enabled: true  # 🎯 保持FID评估监控质量
  eval_interval: 5  # 🎯 高的评估频率
  num_samples: 2048  # 🎯 适中的样本数，平衡准确性和内存
  batch_size: 1  # 🔧 配合推理batch_size调整为1
  max_real_samples: 2048  # 🎯 适中的真实样本数
  save_best_fid_model: true  # 保存最佳FID模型

# 推理配置
inference:
  num_inference_steps: 250  # 🔧 提高DDIM步数到250，获得更高质量
  guidance_scale: 7.5  # CFG引导强度
  eta: 0.3  # 🆕 使用中等eta值，平衡质量和多样性 (0=确定性, 1=随机)
  adaptive_eta: true  # 🆕 启用改进的自适应eta
  
  # 采样配置
  sample_batch_size: 1  # 🔧 修改为仅生成1张图片
  num_samples_per_class: 1  # 🔧 每个类别只生成1个样本
  
  # 🆕 归一化配置
  normalization:
    # 数据范围定义
    data_range: [0, 1]        # 原始图像像素范围
    vae_input_range: [-1, 1]  # VAE输入范围（经过预处理）
    vae_output_range: [-1, 1] # VAE输出范围（tanh激活）
    final_output_range: [0, 1] # 最终输出图像范围
    
    # 数值稳定性配置
    latent_clamp_range: [-5, 5]    # 潜在表示裁剪范围
    latent_std_threshold: 3.0      # 潜在表示标准差阈值
    enable_range_checking: true    # 启用范围检查
    enable_adaptive_norm: true     # 启用自适应归一化
    
    # VAE解码安全性
    vae_decode_clamp: true         # 启用VAE解码输出裁剪
    vae_decode_range: [-1.1, 1.1] # VAE解码输出安全范围
  
  # 输出设置
  output_dir: "generated_samples"
  save_format: "png"

# 设备配置
device: "auto"  # auto, cpu, cuda
mixed_precision: true  # 🎯 启用混合精度训练节省内存

# 日志配置
logging:
  use_wandb: false  # 是否使用wandb
  project_name: "LDM-Gait"
  experiment_name: "ldm_micro_doppler_32x32_latent"  # 微多普勒特征图实验
  log_dir: "logs" 
