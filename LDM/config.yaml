# LDM (Latent Diffusion Model) 配置文件

# VAE 相关配置 (使用预训练的VQ-VAE)
vae:
  model_path: "/kaggle/input/vae-best-fid/adv_vqvae_best_fid.pth"  # Kaggle VAE模型路径
  in_channels: 3
  latent_dim: 256
  num_embeddings: 512
  beta: 0.25
  vq_ema_decay: 0.99
  groups: 32
  freeze: true  # 冻结VAE参数

# 数据集配置
dataset:
  root_dir: "/kaggle/input/dataset"  # Kaggle数据集路径
  batch_size: 6  # 🎯 平衡点：保持合理batch size但避免OOM
  num_workers: 2  # 保持2个workers减少CPU内存占用
  val_split: 0.2
  image_size: 256

# 扩散模型配置
diffusion:
  timesteps: 1000  # 扩散步数T
  noise_schedule: "cosine"  # linear 或 cosine (余弦调度更平滑)
  beta_start: 0.0001
  beta_end: 0.02
  clip_denoised: true

# U-Net 网络结构配置
unet:
  # 输入维度
  in_channels: 256  # 潜在空间通道数，与VAE的latent_dim一致
  out_channels: 256  # 输出通道数 (预测噪声)
  model_channels: 160  # 🎯 适中选择：介于128-192之间，保持表达能力
  
  # 时间嵌入
  time_embed_dim: 640  # 🎯 适中选择：介于512-768之间，保持时间表示能力
  
  # 条件嵌入 (身份标签)
  num_classes: 31  # 类别数量 (ID_1到ID_31，共31个用户)
  class_embed_dim: 320  # 🎯 适中选择：介于256-384之间，保持条件控制
  
  # 网络层配置
  num_res_blocks: 3  # 🎯 保持3层，这对质量很重要
  attention_resolutions: [8, 16]  # 🎯 去掉最细粒度[4]，注意力很耗内存
  channel_mult: [1, 2, 4, 6]  # 🎯 降低最后一层：[1,2,4,8]→[1,2,4,6]
  num_heads: 10  # 🎯 适中选择：介于8-12之间，保持注意力能力
  use_scale_shift_norm: true  # 使用条件归一化
  dropout: 0.1  # Dropout率
  
  # 条件机制
  use_cross_attention: true  # 🎯 保持交叉注意力，这对条件生成很重要
  use_self_attention: true   # 🎯 保持自注意力

# 训练配置
training:
  epochs: 500
  lr: 0.00005  # 🎯 保持较高学习率
  weight_decay: 0.01
  warmup_steps: 1000  # 🎯 保持预热步数
  
  # 保存设置
  save_dir: "ldm_models"
  save_interval: 50  # 每隔多少epoch保存一次
  sample_interval: 15  # 🎯 略微降低采样频率节省内存
  log_interval: 100  # 每隔多少步打印日志
  
  # 损失权重
  noise_loss_weight: 1.0
  
  # Classifier-Free Guidance
  use_cfg: true  # 使用无分类器引导
  cfg_dropout_prob: 0.2  # 🎯 保持强CFG训练
  
  # 梯度裁剪
  grad_clip_norm: 1.0  # 🎯 保持梯度裁剪
  
  # 调度器
  use_scheduler: true
  scheduler_type: "cosine_with_restarts"  # 🎯 保持优化调度器

# FID评估配置
fid_evaluation:
  enabled: true  # 🎯 保持FID评估监控质量
  eval_interval: 10  # 🎯 适中的评估频率
  num_samples: 200  # 🎯 适中的样本数，平衡准确性和内存
  batch_size: 3  # 🎯 降低到3，配合主训练batch_size
  max_real_samples: 800  # 🎯 适中的真实样本数
  save_best_fid_model: true  # 保存最佳FID模型

# 推理配置
inference:
  num_inference_steps: 40  # 🎯 适中的推理步数，平衡质量和内存
  guidance_scale: 7.5  # CFG引导强度
  eta: 0.0  # DDIM参数 (0为确定性采样)
  
  # 采样配置
  sample_batch_size: 3  # 🎯 配合训练batch_size
  num_samples_per_class: 6  # 🎯 适中的样本数量
  
  # 输出设置
  output_dir: "generated_samples"
  save_format: "png"

# 设备配置
device: "auto"  # auto, cpu, cuda
mixed_precision: true  # 🎯 启用混合精度训练节省内存

# 日志配置
logging:
  use_wandb: false  # 是否使用wandb
  project_name: "LDM-Gait"
  experiment_name: "ldm_micro_doppler_32x32_latent"  # 微多普勒特征图实验
  log_dir: "logs" 
