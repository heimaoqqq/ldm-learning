# LDM (Latent Diffusion Model) é…ç½®æ–‡ä»¶ - åŸºäºStable Diffusionè½»é‡åŒ–ç‰ˆæœ¬

# VAE ç›¸å…³é…ç½® (ä½¿ç”¨é¢„è®­ç»ƒçš„VQ-VAE)
vae:
  model_path: "/kaggle/input/vae-best-fid/adv_vqvae_best_fid.pth"  # æœ¬åœ°VAEæ¨¡å‹è·¯å¾„
  in_channels: 3
  latent_dim: 256  # VAEæ½œåœ¨ç»´åº¦
  num_embeddings: 512
  beta: 0.25
  vq_ema_decay: 0.99
  groups: 32
  freeze: true  # å†»ç»“VAEå‚æ•°
  # VAEè¾“å‡ºä¿¡æ¯ï¼ˆç”¨äºU-Neté€‚é…ï¼‰
  latent_height: 32  # 256/8 = 32
  latent_width: 32
  latent_channels: 256

# æ•°æ®é›†é…ç½®
dataset:
  root_dir: "/kaggle/input/dataset"  # Kaggleæ•°æ®é›†è·¯å¾„
  batch_size: 32  # ğŸš€ å¤§å¹…å¢åŠ ï¼š8->32 (16GBæ˜¾å­˜ä¼˜åŒ–)
  num_workers: 4  # ğŸš€ å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
  val_split: 0.2
  image_size: 256

# æ‰©æ•£æ¨¡å‹é…ç½®
diffusion:
  timesteps: 1000
  noise_schedule: "scaled_linear"  # ğŸ”§ SDä½¿ç”¨çš„è°ƒåº¦
  beta_start: 0.00085
  beta_end: 0.012
  scheduler_type: "ddim"
  clip_denoised: true
  prediction_type: "epsilon"  # é¢„æµ‹å™ªå£°

# U-Net ç½‘ç»œç»“æ„é…ç½® (åŸºäºStable Diffusionè½»é‡åŒ–)
unet:
  # è¾“å…¥è¾“å‡ºç»´åº¦
  in_channels: 256  # åŒ¹é…VAEæ½œåœ¨é€šé“æ•°
  out_channels: 256  # é¢„æµ‹å™ªå£°
  
  # åŸºç¡€æ¨¡å‹é…ç½® - 16GBæ˜¾å­˜ä¼˜åŒ–
  model_channels: 192  # ğŸš€ å¢å¼ºæ¨¡å‹ï¼š128->192
  num_res_blocks: 3  # ğŸš€ å¢åŠ ResNetå—ï¼š2->3
  attention_resolutions: [8, 4, 2]  # ğŸš€ å¢åŠ æ³¨æ„åŠ›å±‚ï¼š[4,2]->[8,4,2]
  channel_mult: [1, 2, 4, 6]  # ğŸš€ å¢å¼ºé€šé“å€æ•°ï¼š[1,2,4]->[1,2,4,6]
  
  # æ³¨æ„åŠ›é…ç½®
  num_head_channels: 64  # ğŸš€ å¢åŠ æ³¨æ„åŠ›å¤´ç»´åº¦ï¼š32->64
  use_spatial_transformer: true  # ä½¿ç”¨ç©ºé—´transformer
  transformer_depth: 2  # ğŸš€ å¢åŠ transformerå±‚ï¼š1->2
  
  # æ¡ä»¶åµŒå…¥
  num_classes: 31  # ç±»åˆ«æ•°é‡
  use_fp16: true  # ğŸš€ å¯ç”¨åŠç²¾åº¦
  
  # æ—¶é—´åµŒå…¥
  time_embed_dim: 768  # 192 * 4 (å¢å¤§åµŒå…¥ç»´åº¦)

# è®­ç»ƒé…ç½®
training:
  epochs: 200  # ğŸš€ å‡å°‘è®­ç»ƒè½®æ•°ï¼Œæ›´å¿«æ”¶æ•›
  lr: 0.0002  # ğŸš€ é€‚åº”æ›´å¤§batch sizeçš„å­¦ä¹ ç‡
  weight_decay: 0.01
  warmup_steps: 2000  # ğŸš€ å¢åŠ warmupæ­¥æ•°
  
  # æ¢¯åº¦ç´¯ç§¯
  gradient_accumulation_steps: 1
  
  # ä¿å­˜è®¾ç½®
  save_dir: "sd_ldm_models"
  save_interval: 25  # æ›´é¢‘ç¹ä¿å­˜
  sample_interval: 10  # æ›´é¢‘ç¹ç”Ÿæˆæ ·æœ¬
  log_interval: 50
  
  # æŸå¤±é…ç½®
  loss_type: "l2"  # MSEæŸå¤±
  
  # Classifier-Free Guidance
  use_cfg: true
  cfg_dropout_prob: 0.15  # SDå…¸å‹å€¼
  
  # ä¼˜åŒ–å™¨é…ç½®
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.999
  eps: 0.00000001
  
  # æ¢¯åº¦è£å‰ª
  grad_clip_norm: 1.0
  
  # è°ƒåº¦å™¨
  use_scheduler: true
  scheduler_type: "cosine"

# FIDè¯„ä¼°é…ç½®
fid_evaluation:
  enabled: true
  eval_interval: 20  # æ¯20ä¸ªepochè¯„ä¼°
  num_samples: 2000  # ğŸš€ å¢åŠ æ ·æœ¬æ•°ï¼š1000->2000
  batch_size: 32  # ğŸš€ å¢å¤§FIDè¯„ä¼°batch sizeï¼š16->32
  max_real_samples: 2000  # ğŸš€ å¢åŠ çœŸå®æ ·æœ¬æ•°
  save_best_fid_model: true

# æ¨ç†é…ç½®
inference:
  num_inference_steps: 100  # ğŸš€ å‡å°‘æ¨ç†æ­¥æ•°ï¼Œæ›´å¿«ç”Ÿæˆ
  guidance_scale: 7.5
  eta: 0.0  # DDIMç¡®å®šæ€§é‡‡æ ·
  
  # é‡‡æ ·é…ç½® - 16GBæ˜¾å­˜ä¼˜åŒ–
  sample_batch_size: 16  # ğŸš€ å¢å¤§æ¨ç†batch sizeï¼š4->16
  num_samples_per_class: 4  # ğŸš€ æ¯ç±»ç”Ÿæˆæ›´å¤šæ ·æœ¬ï¼š2->4
  
  # è¾“å‡ºè®¾ç½®
  output_dir: "generated_samples"
  save_format: "png"

# è®¾å¤‡é…ç½®
device: "auto"
mixed_precision: true
use_ema: true  # ğŸš€ ä½¿ç”¨EMAæé«˜ç”Ÿæˆè´¨é‡
ema_decay: 0.9999

# æ—¥å¿—é…ç½®
logging:
  use_wandb: false
  project_name: "SD-LDM-Gait"
  experiment_name: "sd_ldm_lightweight"
  log_dir: "logs" 
