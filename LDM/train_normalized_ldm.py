"""
é›†æˆçš„LDMè®­ç»ƒè„šæœ¬
æ”¯æŒå½’ä¸€åŒ–VQ-VAEï¼Œé€‚é…32Ã—32Ã—256æ½œåœ¨ç©ºé—´
"""

import torch
import torch.nn as nn
import torch.optim as optim
import yaml
import os
import sys
from tqdm import tqdm
import numpy as np
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torch.nn.functional as F

# æ·»åŠ VAEç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥å½’ä¸€åŒ–æ¨¡å—
sys.path.append('../VAE')
from normalized_vqvae import NormalizedVQVAE
from adv_vq_vae import AdvVQVAE

# å¯¼å…¥ç°æœ‰çš„CLDMæ¨¡å‹
from cldm_paper_standard import PaperStandardCLDM

class NormalizedVQVAEWrapper:
    """
    å½’ä¸€åŒ–VQ-VAEåŒ…è£…å™¨
    é›†æˆåˆ°LDMè®­ç»ƒä¸­ä½¿ç”¨
    """
    
    def __init__(self, vqvae_config, normalization_method='standardize'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½åŸå§‹VQ-VAE
        self.vqvae = AdvVQVAE(
            in_channels=vqvae_config['in_channels'],
            latent_dim=vqvae_config['latent_dim'],
            num_embeddings=vqvae_config['num_embeddings'],
            beta=vqvae_config['beta'],
            decay=vqvae_config.get('decay', 0.99),
            groups=vqvae_config['groups'],
            disc_ndf=64
        ).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        model_path = vqvae_config['model_path']
        if os.path.exists(model_path):
            self.vqvae.load_state_dict(torch.load(model_path, map_location=self.device))
            print(f"âœ… å·²åŠ è½½VQ-VAEæ¨¡å‹: {model_path}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹: {model_path}")
            
        # åˆ›å»ºå½’ä¸€åŒ–åŒ…è£…å™¨
        self.norm_vqvae = NormalizedVQVAE(
            self.vqvae, 
            normalization_method=normalization_method
        ).to(self.device)
        
        # å°è¯•åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡
        norm_model_path = f"../VAE/normalized_vqvae_{normalization_method}.pth"
        if os.path.exists(norm_model_path):
            self.norm_vqvae.load_state_dict(torch.load(norm_model_path, map_location=self.device))
            print(f"âœ… å·²åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡: {norm_model_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°é¢„è®¡ç®—çš„å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®¡ç®—")
            self._fit_normalization_stats()
    
    def _fit_normalization_stats(self):
        """ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ‹Ÿåˆå½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        print("ğŸ” ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡...")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        sample_batches = []
        for _ in range(20):  # 320ä¸ªæ ·æœ¬
            batch = torch.randn(16, 3, 256, 256)
            batch = (batch - batch.min()) / (batch.max() - batch.min())
            sample_batches.append(batch)
        
        # è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡
        all_encodings = []
        self.vqvae.eval()
        
        with torch.no_grad():
            for batch in sample_batches:
                batch = batch.to(self.device)
                z = self.vqvae.encoder(batch)
                all_encodings.append(z.cpu())
        
        all_encodings = torch.cat(all_encodings, dim=0)
        
        if self.norm_vqvae.normalization_method == 'standardize':
            self.norm_vqvae.mean = all_encodings.mean()
            self.norm_vqvae.std = all_encodings.std()
            print(f"ğŸ“Š æ ‡å‡†åŒ–ç»Ÿè®¡é‡: å‡å€¼={self.norm_vqvae.mean:.4f}, æ ‡å‡†å·®={self.norm_vqvae.std:.4f}")
        elif self.norm_vqvae.normalization_method == 'minmax':
            self.norm_vqvae.min_val = all_encodings.min()
            self.norm_vqvae.max_val = all_encodings.max()
            print(f"ğŸ“Š Min-Maxç»Ÿè®¡é‡: èŒƒå›´=[{self.norm_vqvae.min_val:.4f}, {self.norm_vqvae.max_val:.4f}]")
        
        self.norm_vqvae.is_fitted = torch.tensor(True)
        print("âœ… å½’ä¸€åŒ–ç»Ÿè®¡é‡è®¡ç®—å®Œæˆï¼")
    
    def encode_for_ldm(self, images):
        """ç¼–ç å›¾åƒç”¨äºLDMè®­ç»ƒ"""
        with torch.no_grad():
            return self.norm_vqvae.encode_without_vq(images)
    
    def decode_from_ldm(self, z_normalized):
        """ä»LDMç”Ÿæˆçš„ç¼–ç è§£ç å›å›¾åƒ"""
        with torch.no_grad():
            return self.norm_vqvae.decode_from_normalized(z_normalized)

class LDMTrainer:
    """
    LDMè®­ç»ƒå™¨ï¼Œæ”¯æŒå½’ä¸€åŒ–VQ-VAE
    """
    
    def __init__(self, config_path):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å½’ä¸€åŒ–VQ-VAE
        self.vqvae_wrapper = NormalizedVQVAEWrapper(
            self.config['vqvae'],
            self.config.get('normalization_method', 'standardize')
        )
        
        # åˆ›å»ºCLDMæ¨¡å‹ - é€‚é…32Ã—32Ã—256æ½œåœ¨ç©ºé—´
        cldm_config = self.config['cldm']
        self.model = PaperStandardCLDM(
            latent_dim=cldm_config['latent_dim'],
            num_classes=cldm_config['num_classes'],
            num_timesteps=cldm_config['num_timesteps'],
            beta_schedule=cldm_config['beta_schedule'],
            # U-Netå‚æ•° - é’ˆå¯¹32Ã—32è°ƒæ•´
            model_channels=cldm_config['model_channels'],
            time_emb_dim=cldm_config['time_emb_dim'],
            class_emb_dim=cldm_config['class_emb_dim'],
            channel_mult=cldm_config['channel_mult'],
            num_res_blocks=cldm_config['num_res_blocks'],
            attention_resolutions=cldm_config['attention_resolutions'],  # é€‚é…32Ã—32
            dropout=cldm_config['dropout']
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        train_config = self.config['training']
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=train_config['lr'],
            weight_decay=train_config.get('weight_decay', 0.0),
            betas=(train_config.get('beta1', 0.9), train_config.get('beta2', 0.999)),
            eps=train_config.get('eps', 1e-8)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if train_config.get('use_scheduler', False):
            if train_config.get('scheduler_type') == 'cosine':
                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer, 
                    T_max=train_config['epochs'],
                    eta_min=train_config.get('min_lr', 0.00001)
                )
            else:
                self.scheduler = None
        else:
            self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_loss = float('inf')
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = train_config['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
    
    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(dataloader)
        
        with tqdm(dataloader, desc=f"Epoch {self.epoch}") as pbar:
            for batch_idx, (images, labels) in enumerate(pbar):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # ä½¿ç”¨å½’ä¸€åŒ–VQ-VAEç¼–ç å›¾åƒ
                with torch.no_grad():
                    latents = self.vqvae_wrapper.encode_for_ldm(images)
                
                # æ£€æŸ¥ç¼–ç èŒƒå›´
                if batch_idx == 0:
                    print(f"ğŸ” æ½œåœ¨ç¼–ç ç»Ÿè®¡: å½¢çŠ¶={latents.shape}, "
                          f"èŒƒå›´=[{latents.min():.4f}, {latents.max():.4f}], "
                          f"å‡å€¼={latents.mean():.4f}, æ ‡å‡†å·®={latents.std():.4f}")
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                outputs = self.model(latents, labels)
                
                # å¤„ç†CLDMè¿”å›çš„ä¸‰å…ƒç»„ (loss, pred_noise, noise)
                if isinstance(outputs, tuple) and len(outputs) == 3:
                    loss, pred_noise, noise = outputs
                else:
                    raise ValueError(f"æ¨¡å‹è¿”å›æ ¼å¼é”™è¯¯: {type(outputs)}")
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                grad_clip = self.config['training'].get('grad_clip_norm')
                if grad_clip:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)
                
                self.optimizer.step()
                
                total_loss += loss.item()
                pbar.set_postfix({'loss': f"{loss.item():.6f}"})
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def validate(self, dataloader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        num_batches = len(dataloader)
        
        with torch.no_grad():
            for images, labels in dataloader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # ä½¿ç”¨å½’ä¸€åŒ–VQ-VAEç¼–ç å›¾åƒ
                latents = self.vqvae_wrapper.encode_for_ldm(images)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(latents, labels)
                
                # å¤„ç†CLDMè¿”å›çš„ä¸‰å…ƒç»„ (loss, pred_noise, noise)
                if isinstance(outputs, tuple) and len(outputs) == 3:
                    loss, pred_noise, noise = outputs
                else:
                    raise ValueError(f"æ¨¡å‹è¿”å›æ ¼å¼é”™è¯¯: {type(outputs)}")
                
                total_loss += loss.item()
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def sample_and_save(self, num_samples=16):
        """ç”Ÿæˆæ ·æœ¬å¹¶ä¿å­˜"""
        self.model.eval()
        
        with torch.no_grad():
            # éšæœºç”Ÿæˆç±»åˆ«æ ‡ç­¾
            class_labels = torch.randint(
                0, self.config['cldm']['num_classes'], 
                (num_samples,), device=self.device
            )
            
            # ç”Ÿæˆæ½œåœ¨ç¼–ç 
            shape = (num_samples, 256, 32, 32)  # 32Ã—32Ã—256
            generated_latents = self.model.sample(
                class_labels, 
                self.device,
                shape=shape,
                num_inference_steps=self.config['training']['sampling_steps'],
                eta=self.config['training'].get('eta', 0.0)
            )
            
            # è§£ç ä¸ºå›¾åƒ
            generated_images = self.vqvae_wrapper.decode_from_ldm(generated_latents)
            
            # ä¿å­˜å›¾åƒ
            import torchvision.utils as vutils
            save_path = os.path.join(self.save_dir, f"samples_epoch_{self.epoch}.png")
            vutils.save_image(
                generated_images, save_path,
                nrow=4, normalize=True, value_range=(0, 1)
            )
            
            print(f"âœ… å·²ä¿å­˜ç”Ÿæˆæ ·æœ¬: {save_path}")
    
    def save_model(self, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'config': self.config
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.save_dir, f"checkpoint_epoch_{self.epoch}.pth")
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.save_dir, "best_model.pth")
            torch.save(checkpoint, best_path)
            print(f"âœ… å·²ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒLDM...")
        print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
        print(f"  æ½œåœ¨ç©ºé—´ç»´åº¦: 32Ã—32Ã—256")
        print(f"  æ¨¡å‹é€šé“æ•°: {self.config['cldm']['model_channels']}")
        print(f"  æ³¨æ„åŠ›åˆ†è¾¨ç‡: {self.config['cldm']['attention_resolutions']}")
        print(f"  å½’ä¸€åŒ–æ–¹æ³•: {self.config.get('normalization_method', 'standardize')}")
        
        train_losses = []
        val_losses = []
        
        for epoch in range(self.config['training']['epochs']):
            self.epoch = epoch
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            train_losses.append(train_loss)
            
            # éªŒè¯
            if epoch % self.config['training']['val_interval'] == 0:
                val_loss = self.validate(val_loader)
                val_losses.append(val_loss)
                
                print(f"Epoch {epoch}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < self.best_loss:
                    self.best_loss = val_loss
                    self.save_model(is_best=True)
            
            # ç”Ÿæˆæ ·æœ¬
            if epoch % self.config['training']['sample_interval'] == 0:
                self.sample_and_save()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config['training']['save_interval'] == 0:
                self.save_model()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        return train_losses, val_losses

def main():
    """ä¸»å‡½æ•°"""
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = "config_normalized_ldm.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("è¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨ç°æœ‰çš„config_paper_standard.yaml")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LDMTrainer(config_path)
    
    # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ•°æ®é›†åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # ç¤ºä¾‹ä»£ç ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    """
    from your_dataset import YourDataset
    
    train_dataset = YourDataset(...)
    val_dataset = YourDataset(...)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=trainer.config['dataset']['batch_size'],
        shuffle=True,
        num_workers=trainer.config['dataset']['num_workers']
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=trainer.config['dataset']['batch_size'],
        shuffle=False,
        num_workers=trainer.config['dataset']['num_workers']
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, val_loader)
    """
    
    print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼")
    print("è¯·æ ¹æ®ä½ çš„æ•°æ®é›†æƒ…å†µä¿®æ”¹main()å‡½æ•°ä¸­çš„æ•°æ®åŠ è½½éƒ¨åˆ†ã€‚")

if __name__ == "__main__":
    main() 
