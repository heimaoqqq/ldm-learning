"""
é›†æˆçš„LDMè®­ç»ƒè„šæœ¬
æ”¯æŒå½’ä¸€åŒ–VQ-VAEï¼Œé€‚é…32Ã—32Ã—256æ½œåœ¨ç©ºé—´
ä¿®å¤Kaggleç¯å¢ƒæ•°æ®é›†ç»“æ„å’Œå‚æ•°é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
import yaml
import os
import sys
import glob
from tqdm import tqdm
import numpy as np
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torch.nn.functional as F
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split

# æ·»åŠ VAEç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥å½’ä¸€åŒ–æ¨¡å—
sys.path.append('../VAE')
from normalized_vqvae import NormalizedVQVAE
from adv_vq_vae import AdvVQVAE

# å¯¼å…¥ç°æœ‰çš„CLDMæ¨¡å‹
from cldm_paper_standard import PaperStandardCLDM

class SimpleGaitDataset(Dataset):
    """ç®€åŒ–çš„æ­¥æ€æ•°æ®é›†ç±»ï¼Œé€‚é…Kaggleæ•°æ®ç»“æ„"""
    def __init__(self, image_paths, class_ids, transform=None):
        self.image_paths = image_paths
        self.class_ids = class_ids
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            label = self.class_ids[idx]
            return image, label
        except Exception as e:
            print(f"é”™è¯¯åŠ è½½å›¾ç‰‡ {img_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤å›¾ç‰‡
            default_image = torch.zeros(3, 256, 256)
            return default_image, 0

def build_kaggle_dataloader(root_dir, batch_size=8, num_workers=2, val_split=0.2):
    """
    é€‚é…Kaggleæ•°æ®é›†ç»“æ„çš„æ•°æ®åŠ è½½å™¨
    æ”¯æŒID_Xç›®å½•ç»“æ„å’Œé€’å½’æ–‡ä»¶æŸ¥æ‰¾
    """
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    print(f"ğŸ” æ‰«ææ•°æ®é›†ç›®å½•: {root_dir}")
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡è·¯å¾„å’Œç±»åˆ«
    all_image_paths = []
    all_class_ids = []
    
    # æ‰«ææ‰€æœ‰IDç›®å½•
    id_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d)) and d.startswith('ID_')]
    id_dirs.sort()
    
    print(f"ğŸ“ æ‰¾åˆ° {len(id_dirs)} ä¸ªIDç›®å½•: {id_dirs[:5]}{'...' if len(id_dirs) > 5 else ''}")
    
    for id_dir in id_dirs:
        id_path = os.path.join(root_dir, id_dir)
        
        # æå–ç±»åˆ«ID
        try:
            class_id = int(id_dir.replace('ID_', '')) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
        except:
            print(f"âš ï¸ æ— æ³•è§£æIDç›®å½•: {id_dir}")
            continue
            
        # æ”¶é›†è¯¥IDä¸‹çš„æ‰€æœ‰jpgæ–‡ä»¶
        jpg_files = glob.glob(os.path.join(id_path, "*.jpg"))
        
        if jpg_files:
            all_image_paths.extend(jpg_files)
            all_class_ids.extend([class_id] * len(jpg_files))
            print(f"  ğŸ“‚ {id_dir}: {len(jpg_files)} å¼ å›¾ç‰‡, ç±»åˆ«ID: {class_id}")
    
    if not all_image_paths:
        # å¦‚æœæŒ‰ç›®å½•ç»“æ„æ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥åœ¨æ ¹ç›®å½•æŸ¥æ‰¾
        jpg_files = glob.glob(os.path.join(root_dir, "**/*.jpg"), recursive=True)
        print(f"ğŸ” é€’å½’æŸ¥æ‰¾åˆ° {len(jpg_files)} ä¸ªjpgæ–‡ä»¶")
        
        for img_path in jpg_files:
            filename = os.path.basename(img_path)
            try:
                # å°è¯•ä»æ–‡ä»¶åè§£æç±»åˆ«
                if 'ID' in filename:
                    class_id_str = filename.split('ID')[1].split('_')[0]
                    class_id = int(class_id_str) - 1
                    all_image_paths.append(img_path)
                    all_class_ids.append(class_id)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«
                all_image_paths.append(img_path)
                all_class_ids.append(0)
    
    if not all_image_paths:
        raise ValueError(f"åœ¨ç›®å½• {root_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶")
    
    print(f"âœ… æ€»å…±æ‰¾åˆ° {len(all_image_paths)} å¼ å›¾ç‰‡")
    print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {len(set(all_class_ids))} ä¸ªä¸åŒç±»åˆ«")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    if len(set(all_class_ids)) > 1:
        # æœ‰å¤šä¸ªç±»åˆ«ï¼Œä½¿ç”¨åˆ†å±‚é‡‡æ ·
        try:
            train_paths, val_paths, train_ids, val_ids = train_test_split(
                all_image_paths, all_class_ids, 
                test_size=val_split, 
                random_state=42, 
                stratify=all_class_ids
            )
        except:
            # å¦‚æœåˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šéšæœºåˆ’åˆ†
            train_paths, val_paths, train_ids, val_ids = train_test_split(
                all_image_paths, all_class_ids, 
                test_size=val_split, 
                random_state=42
            )
    else:
        # åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨æ™®é€šéšæœºåˆ’åˆ†
        train_paths, val_paths, train_ids, val_ids = train_test_split(
            all_image_paths, all_class_ids, 
            test_size=val_split, 
            random_state=42
        )
    
    print(f"ğŸ“ˆ æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_paths)} å¼ å›¾ç‰‡")
    print(f"  éªŒè¯é›†: {len(val_paths)} å¼ å›¾ç‰‡")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = SimpleGaitDataset(train_paths, train_ids, transform=transform)
    val_dataset = SimpleGaitDataset(val_paths, val_ids, transform=transform)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=min(num_workers, 2),  # åœ¨Kaggleä¸­é™åˆ¶workersæ•°é‡
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=min(num_workers, 2),
        pin_memory=True
    )
    
    return train_loader, val_loader, len(train_dataset), len(val_dataset)

class NormalizedVQVAEWrapper:
    """
    å½’ä¸€åŒ–VQ-VAEåŒ…è£…å™¨
    é›†æˆåˆ°LDMè®­ç»ƒä¸­ä½¿ç”¨ï¼Œæ”¯æŒçœŸå®æ•°æ®å½’ä¸€åŒ–ç»Ÿè®¡é‡è®¡ç®—
    """
    
    def __init__(self, vqvae_config, normalization_method='standardize'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½åŸå§‹VQ-VAE
        self.vqvae = AdvVQVAE(
            in_channels=vqvae_config['in_channels'],
            latent_dim=vqvae_config['latent_dim'],
            num_embeddings=vqvae_config['num_embeddings'],
            beta=vqvae_config['beta'],
            decay=vqvae_config.get('decay', 0.99),
            groups=vqvae_config['groups'],
            disc_ndf=64
        ).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        model_path = vqvae_config['model_path']
        if os.path.exists(model_path):
            self.vqvae.load_state_dict(torch.load(model_path, map_location=self.device))
            print(f"âœ… å·²åŠ è½½VQ-VAEæ¨¡å‹: {model_path}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹: {model_path}")
            
        # åˆ›å»ºå½’ä¸€åŒ–åŒ…è£…å™¨
        self.norm_vqvae = NormalizedVQVAE(
            self.vqvae, 
            normalization_method=normalization_method
        ).to(self.device)
        
        # å½’ä¸€åŒ–ç»Ÿè®¡é‡å°†åœ¨å®é™…æ•°æ®ä¸Šè®¡ç®—
        self.is_fitted = False
        self.normalization_method = normalization_method
    
    def fit_normalization_on_real_data(self, dataloader, max_samples=1000):
        """åœ¨çœŸå®æ•°æ®ä¸Šè®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        print("ğŸ” åœ¨çœŸå®æ•°æ®é›†ä¸Šè®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡...")
        
        self.vqvae.eval()
        all_encodings = []
        sample_count = 0
        
        with torch.no_grad():
            for batch_idx, (images, _) in enumerate(dataloader):
                if sample_count >= max_samples:
                    break
                    
                images = images.to(self.device)
                
                # è·å–ç¼–ç å™¨è¾“å‡ºï¼ˆVQä¹‹å‰çš„è¿ç»­è¡¨ç¤ºï¼‰
                z = self.vqvae.encoder(images)
                all_encodings.append(z.cpu())
                sample_count += images.size(0)
                
                if batch_idx % 10 == 0:
                    print(f"  å·²å¤„ç† {sample_count} ä¸ªæ ·æœ¬...")
        
        # åˆå¹¶æ‰€æœ‰ç¼–ç 
        all_encodings = torch.cat(all_encodings, dim=0)
        
        # æ£€æŸ¥åŸå§‹ç¼–ç èŒƒå›´
        print(f"ğŸ” åŸå§‹ç¼–ç èŒƒå›´: [{all_encodings.min():.4f}, {all_encodings.max():.4f}]")
        print(f"ğŸ” åŸå§‹ç¼–ç ç»Ÿè®¡: å‡å€¼={all_encodings.mean():.4f}, æ ‡å‡†å·®={all_encodings.std():.4f}")
        
        if self.norm_vqvae.normalization_method == 'standardize':
            # ç›´æ¥ä½¿ç”¨NormalizedVQVAEçš„fit_normalization_statsæ–¹æ³•
            # è¿™æ ·ç¡®ä¿ç»Ÿè®¡é‡æ­£ç¡®ä¿å­˜åˆ°æ¨¡å‹ä¸­
            self.norm_vqvae.mean = all_encodings.mean().to(self.device)
            self.norm_vqvae.std = all_encodings.std().to(self.device)
            self.norm_vqvae.is_fitted = torch.tensor(True).to(self.device)
            
            print(f"ğŸ“Š æ ‡å‡†åŒ–ç»Ÿè®¡é‡: å‡å€¼={self.norm_vqvae.mean:.4f}, æ ‡å‡†å·®={self.norm_vqvae.std:.4f}")
            
            # æµ‹è¯•å½’ä¸€åŒ–æ•ˆæœ
            test_sample = all_encodings[:100].to(self.device)
            normalized_test = (test_sample - self.norm_vqvae.mean) / (self.norm_vqvae.std + 1e-5)
            print(f"ğŸ§ª å½’ä¸€åŒ–æµ‹è¯•: [{normalized_test.min():.4f}, {normalized_test.max():.4f}], å‡å€¼={normalized_test.mean():.4f}, æ ‡å‡†å·®={normalized_test.std():.4f}")
            
        elif self.norm_vqvae.normalization_method == 'minmax':
            self.norm_vqvae.min_val = all_encodings.min().to(self.device)
            self.norm_vqvae.max_val = all_encodings.max().to(self.device)
            self.norm_vqvae.is_fitted = torch.tensor(True).to(self.device)
            
            print(f"ğŸ“Š Min-Maxç»Ÿè®¡é‡: èŒƒå›´=[{self.norm_vqvae.min_val:.4f}, {self.norm_vqvae.max_val:.4f}]")
            
            # æµ‹è¯•å½’ä¸€åŒ–æ•ˆæœ
            test_sample = all_encodings[:100].to(self.device)
            normalized_test = 2.0 * (test_sample - self.norm_vqvae.min_val) / (self.norm_vqvae.max_val - self.norm_vqvae.min_val + 1e-5) - 1.0
            print(f"ğŸ§ª å½’ä¸€åŒ–æµ‹è¯•: [{normalized_test.min():.4f}, {normalized_test.max():.4f}]")
        
        self.is_fitted = True
        print("âœ… çœŸå®æ•°æ®å½’ä¸€åŒ–ç»Ÿè®¡é‡è®¡ç®—å®Œæˆï¼")
        
        # å¼ºåˆ¶å¯ç”¨è°ƒè¯•æ¨¡å¼è¿›è¡Œä¸‹æ¬¡éªŒè¯
        self.norm_vqvae._debug = True
        print("ğŸ”§ å·²å¯ç”¨å½’ä¸€åŒ–è°ƒè¯•æ¨¡å¼")
    
    def _fit_normalization_stats(self):
        """ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ‹Ÿåˆå½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        print("âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆå»ºè®®ä½¿ç”¨çœŸå®æ•°æ®ï¼‰...")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        sample_batches = []
        for _ in range(20):  # 320ä¸ªæ ·æœ¬
            batch = torch.randn(16, 3, 256, 256)
            batch = (batch - batch.min()) / (batch.max() - batch.min())
            sample_batches.append(batch)
        
        # è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡
        all_encodings = []
        self.vqvae.eval()
        
        with torch.no_grad():
            for batch in sample_batches:
                batch = batch.to(self.device)
                z = self.vqvae.encoder(batch)
                all_encodings.append(z.cpu())
        
        all_encodings = torch.cat(all_encodings, dim=0)
        
        if self.norm_vqvae.normalization_method == 'standardize':
            self.norm_vqvae.mean = all_encodings.mean()
            self.norm_vqvae.std = all_encodings.std()
            print(f"ğŸ“Š ç¤ºä¾‹æ•°æ®æ ‡å‡†åŒ–ç»Ÿè®¡é‡: å‡å€¼={self.norm_vqvae.mean:.4f}, æ ‡å‡†å·®={self.norm_vqvae.std:.4f}")
        elif self.norm_vqvae.normalization_method == 'minmax':
            self.norm_vqvae.min_val = all_encodings.min()
            self.norm_vqvae.max_val = all_encodings.max()
            print(f"ğŸ“Š ç¤ºä¾‹æ•°æ®Min-Maxç»Ÿè®¡é‡: èŒƒå›´=[{self.norm_vqvae.min_val:.4f}, {self.norm_vqvae.max_val:.4f}]")
        
        self.norm_vqvae.is_fitted = torch.tensor(True)
        self.is_fitted = True
        print("âœ… ç¤ºä¾‹æ•°æ®å½’ä¸€åŒ–ç»Ÿè®¡é‡è®¡ç®—å®Œæˆï¼")
    
    def encode_for_ldm(self, images):
        """ç¼–ç å›¾åƒç”¨äºLDMè®­ç»ƒ"""
        if not self.is_fitted:
            print("âš ï¸ å½’ä¸€åŒ–ç»Ÿè®¡é‡æœªè®¡ç®—ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è®¡ç®—...")
            self._fit_normalization_stats()
        
        with torch.no_grad():
            # è°ƒç”¨NormalizedVQVAEçš„encode_without_vqæ–¹æ³•
            z_normalized = self.norm_vqvae.encode_without_vq(images)
            return z_normalized
    
    def decode_from_ldm(self, z_normalized):
        """ä»LDMç”Ÿæˆçš„ç¼–ç è§£ç å›å›¾åƒ"""
        if not self.is_fitted:
            raise RuntimeError("è¯·å…ˆè®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼")
            
        with torch.no_grad():
            # è°ƒç”¨NormalizedVQVAEçš„decode_from_normalizedæ–¹æ³•
            return self.norm_vqvae.decode_from_normalized(z_normalized)

class LDMTrainer:
    """
    LDMè®­ç»ƒå™¨ï¼Œæ”¯æŒå½’ä¸€åŒ–VQ-VAE
    """
    
    def __init__(self, config_path):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å½’ä¸€åŒ–VQ-VAE
        self.vqvae_wrapper = NormalizedVQVAEWrapper(
            self.config['vqvae'],
            self.config.get('normalization_method', 'standardize')
        )
        
        # åˆ›å»ºCLDMæ¨¡å‹ - é€‚é…32Ã—32Ã—256æ½œåœ¨ç©ºé—´
        cldm_config = self.config['cldm']
        self.model = PaperStandardCLDM(
            latent_dim=cldm_config['latent_dim'],
            num_classes=cldm_config['num_classes'],
            num_timesteps=cldm_config['num_timesteps'],
            beta_schedule=cldm_config['beta_schedule'],
            # U-Netå‚æ•° - é’ˆå¯¹32Ã—32è°ƒæ•´
            model_channels=cldm_config['model_channels'],
            time_emb_dim=cldm_config['time_emb_dim'],
            class_emb_dim=cldm_config['class_emb_dim'],
            channel_mult=cldm_config['channel_mult'],
            num_res_blocks=cldm_config['num_res_blocks'],
            attention_resolutions=cldm_config['attention_resolutions'],  # é€‚é…32Ã—32
            dropout=cldm_config['dropout']
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        train_config = self.config['training']
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=train_config['lr'],
            weight_decay=train_config.get('weight_decay', 0.0),
            betas=(train_config.get('beta1', 0.9), train_config.get('beta2', 0.999)),
            eps=train_config.get('eps', 1e-8)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if train_config.get('use_scheduler', False):
            if train_config.get('scheduler_type') == 'cosine':
                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer, 
                    T_max=train_config['epochs'],
                    eta_min=train_config.get('min_lr', 0.00001)
                )
            else:
                self.scheduler = None
        else:
            self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_loss = float('inf')
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = train_config['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
    
    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(dataloader)
        
        # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼Œåªåœ¨ç¬¬ä¸€ä¸ªbatchæ˜¾ç¤ºç¼–ç ç»Ÿè®¡
        for batch_idx, (images, labels) in enumerate(dataloader):
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # ä½¿ç”¨å½’ä¸€åŒ–VQ-VAEç¼–ç å›¾åƒ
            with torch.no_grad():
                latents = self.vqvae_wrapper.encode_for_ldm(images)
            
            # åªåœ¨ç¬¬ä¸€ä¸ªepochçš„ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºç¼–ç ç»Ÿè®¡
            if self.epoch == 0 and batch_idx == 0:
                print(f"ğŸ” æ½œåœ¨ç¼–ç ç»Ÿè®¡: å½¢çŠ¶={latents.shape}, "
                      f"èŒƒå›´=[{latents.min():.4f}, {latents.max():.4f}], "
                      f"å‡å€¼={latents.mean():.4f}, æ ‡å‡†å·®={latents.std():.4f}")
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(latents, labels)
            
            # å¤„ç†CLDMè¿”å›çš„ä¸‰å…ƒç»„ (loss, pred_noise, noise)
            if isinstance(outputs, tuple) and len(outputs) == 3:
                loss, pred_noise, noise = outputs
            else:
                raise ValueError(f"æ¨¡å‹è¿”å›æ ¼å¼é”™è¯¯: {type(outputs)}")
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            grad_clip = self.config['training'].get('grad_clip_norm')
            if grad_clip:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)
            
            self.optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def validate(self, dataloader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        num_batches = len(dataloader)
        
        with torch.no_grad():
            for images, labels in dataloader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # ä½¿ç”¨å½’ä¸€åŒ–VQ-VAEç¼–ç å›¾åƒ
                latents = self.vqvae_wrapper.encode_for_ldm(images)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(latents, labels)
                
                # å¤„ç†CLDMè¿”å›çš„ä¸‰å…ƒç»„ (loss, pred_noise, noise)
                if isinstance(outputs, tuple) and len(outputs) == 3:
                    loss, pred_noise, noise = outputs
                else:
                    raise ValueError(f"æ¨¡å‹è¿”å›æ ¼å¼é”™è¯¯: {type(outputs)}")
                
                total_loss += loss.item()
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def sample_and_save(self, num_samples=16):
        """ç”Ÿæˆæ ·æœ¬å¹¶ä¿å­˜"""
        self.model.eval()
        
        with torch.no_grad():
            # éšæœºç”Ÿæˆç±»åˆ«æ ‡ç­¾
            class_labels = torch.randint(
                0, self.config['cldm']['num_classes'], 
                (num_samples,), device=self.device
            )
            
            # ç”Ÿæˆæ½œåœ¨ç¼–ç 
            shape = (num_samples, 256, 32, 32)  # 32Ã—32Ã—256
            generated_latents = self.model.sample(
                class_labels, 
                self.device,
                shape=shape,
                num_inference_steps=self.config['training']['sampling_steps'],
                eta=self.config['training'].get('eta', 0.0)
            )
            
            # è§£ç ä¸ºå›¾åƒ
            generated_images = self.vqvae_wrapper.decode_from_ldm(generated_latents)
            
            # ä¿å­˜å›¾åƒ
            import torchvision.utils as vutils
            save_path = os.path.join(self.save_dir, f"samples_epoch_{self.epoch}.png")
            vutils.save_image(
                generated_images, save_path,
                nrow=4, normalize=True, value_range=(0, 1)
            )
            
            print(f"âœ… å·²ä¿å­˜ç”Ÿæˆæ ·æœ¬: {save_path}")
    
    def save_model(self, is_best=False):
        """ä¿å­˜æ¨¡å‹å¹¶åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'config': self.config
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘3ä¸ªï¼‰
        checkpoint_pattern = os.path.join(self.save_dir, "checkpoint_epoch_*.pth")
        existing_checkpoints = glob.glob(checkpoint_pattern)
        existing_checkpoints.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        # åˆ é™¤æ—§äºç¬¬3ä¸ªçš„æ£€æŸ¥ç‚¹
        if len(existing_checkpoints) >= 3:
            for old_checkpoint in existing_checkpoints[2:]:
                try:
                    os.remove(old_checkpoint)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {os.path.basename(old_checkpoint)}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å¤±è´¥ {old_checkpoint}: {e}")
        
        # ä¿å­˜æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.save_dir, f"checkpoint_epoch_{self.epoch}.pth")
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_epoch_{self.epoch}.pth")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.save_dir, "best_model.pth")
            torch.save(checkpoint, best_path)
            print(f"âœ… å·²ä¿å­˜æœ€ä½³æ¨¡å‹: best_model.pth")
    
    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒLDM...")
        print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
        print(f"  æ½œåœ¨ç©ºé—´ç»´åº¦: 32Ã—32Ã—256")
        print(f"  æ¨¡å‹é€šé“æ•°: {self.config['cldm']['model_channels']}")
        print(f"  æ³¨æ„åŠ›åˆ†è¾¨ç‡: {self.config['cldm']['attention_resolutions']}")
        print(f"  å½’ä¸€åŒ–æ–¹æ³•: {self.config.get('normalization_method', 'standardize')}")
        
        train_losses = []
        val_losses = []
        
        for epoch in range(self.config['training']['epochs']):
            self.epoch = epoch
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            train_losses.append(train_loss)
            
            # éªŒè¯
            if epoch % self.config['training']['val_interval'] == 0:
                val_loss, eval_metrics = self.comprehensive_evaluation(val_loader, save_samples=True)
                val_losses.append(val_loss)
                
                print(f"âœ… Epoch {epoch}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
                if val_loss < self.best_loss:
                    self.best_loss = val_loss
                    self.save_model(is_best=True)
                    print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.6f}")
                
            else:
                # åªæ˜¾ç¤ºè®­ç»ƒæŸå¤±ï¼Œä¸è¿›è¡Œè¯¦ç»†è¯„ä¼°
                print(f"Epoch {epoch}: Train Loss = {train_loss:.6f}")
            
            # å¸¸è§„æ ·æœ¬ç”Ÿæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            if epoch % self.config['training']['sample_interval'] == 0 and epoch % self.config['training']['val_interval'] != 0:
                self.sample_and_save()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config['training']['save_interval'] == 0:
                self.save_model()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        return train_losses, val_losses

    def evaluate_generation_quality(self, num_samples=100, num_classes=None):
        """
        è¯„ä¼°LDMç”Ÿæˆè´¨é‡
        åŒ…æ‹¬FIDã€å¤šæ ·æ€§ã€ç±»åˆ«æ¡ä»¶å‡†ç¡®æ€§ç­‰æŒ‡æ ‡
        """
        self.model.eval()
        print(f"ğŸ” å¼€å§‹è¯„ä¼°ç”Ÿæˆè´¨é‡ (æ ·æœ¬æ•°: {num_samples})...")
        
        if num_classes is None:
            num_classes = self.config['cldm']['num_classes']
        
        generated_images = []
        class_labels = []
        
        with torch.no_grad():
            # ç”ŸæˆæŒ‡å®šæ•°é‡çš„æ ·æœ¬
            samples_per_batch = min(16, num_samples)
            num_batches = (num_samples + samples_per_batch - 1) // samples_per_batch
            
            for batch_idx in range(num_batches):
                current_batch_size = min(samples_per_batch, num_samples - batch_idx * samples_per_batch)
                
                # éšæœºç”Ÿæˆç±»åˆ«æ ‡ç­¾
                batch_labels = torch.randint(0, num_classes, (current_batch_size,), device=self.device)
                
                # ç”Ÿæˆæ½œåœ¨ç¼–ç 
                shape = (current_batch_size, 256, 32, 32)  # 32Ã—32Ã—256
                generated_latents = self.model.sample(
                    batch_labels, 
                    self.device,
                    shape=shape,
                    num_inference_steps=self.config['training']['sampling_steps'],
                    eta=self.config['training'].get('eta', 0.0)
                )
                
                # è§£ç ä¸ºå›¾åƒ
                batch_images = self.vqvae_wrapper.decode_from_ldm(generated_latents)
                
                generated_images.append(batch_images.cpu())
                class_labels.extend(batch_labels.cpu().tolist())
                
                if (batch_idx + 1) % 5 == 0:
                    print(f"  å·²ç”Ÿæˆ {(batch_idx + 1) * samples_per_batch} / {num_samples} æ ·æœ¬")
        
        # åˆå¹¶æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒ
        generated_images = torch.cat(generated_images, dim=0)
        
        # è¯„ä¼°æŒ‡æ ‡
        metrics = {}
        
        # 1. å›¾åƒè´¨é‡ç»Ÿè®¡
        metrics['image_stats'] = {
            'mean_pixel_value': generated_images.mean().item(),
            'std_pixel_value': generated_images.std().item(),
            'min_pixel_value': generated_images.min().item(),
            'max_pixel_value': generated_images.max().item()
        }
        
        # 2. ç±»åˆ«åˆ†å¸ƒå‡åŒ€æ€§
        from collections import Counter
        class_distribution = Counter(class_labels)
        metrics['class_distribution'] = {
            'entropy': self._calculate_entropy(list(class_distribution.values())),
            'max_class_ratio': max(class_distribution.values()) / len(class_labels),
            'num_unique_classes': len(class_distribution)
        }
        
        # 3. å›¾åƒå¤šæ ·æ€§ (é€šè¿‡åƒç´ å·®å¼‚ä¼°ç®—)
        if len(generated_images) > 1:
            # éšæœºé€‰æ‹©æ ·æœ¬å¯¹è®¡ç®—å·®å¼‚
            num_pairs = min(100, len(generated_images) * (len(generated_images) - 1) // 2)
            differences = []
            for _ in range(num_pairs):
                idx1, idx2 = torch.randint(0, len(generated_images), (2,))
                if idx1 != idx2:
                    diff = torch.mean((generated_images[idx1] - generated_images[idx2]) ** 2).item()
                    differences.append(diff)
            
            metrics['diversity'] = {
                'mean_pairwise_mse': np.mean(differences),
                'std_pairwise_mse': np.std(differences)
            }
        
        print("âœ… ç”Ÿæˆè´¨é‡è¯„ä¼°å®Œæˆ")
        return metrics, generated_images
    
    def _calculate_entropy(self, counts):
        """è®¡ç®—åˆ†å¸ƒç†µ"""
        counts = np.array(counts)
        probs = counts / counts.sum()
        probs = probs[probs > 0]  # é¿å…log(0)
        return -np.sum(probs * np.log2(probs))
    
    def comprehensive_evaluation(self, val_loader, save_samples=True):
        """
        ç»¼åˆè¯„ä¼°LDMè´¨é‡
        """
        print("ğŸ” å¼€å§‹ç»¼åˆè´¨é‡è¯„ä¼°...")
        
        # 1. éªŒè¯æŸå¤±
        val_loss = self.validate(val_loader)
        print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.6f}")
        
        # 2. ç”Ÿæˆè´¨é‡è¯„ä¼°
        metrics, generated_images = self.evaluate_generation_quality(
            num_samples=self.config['training'].get('eval_samples', 64)
        )
        
        # 3. æ‰“å°è¯„ä¼°ç»“æœ
        print(f"\nğŸ“ˆ ç”Ÿæˆè´¨é‡æŒ‡æ ‡:")
        print(f"  å›¾åƒç»Ÿè®¡:")
        stats = metrics['image_stats']
        print(f"    åƒç´ å€¼èŒƒå›´: [{stats['min_pixel_value']:.3f}, {stats['max_pixel_value']:.3f}]")
        print(f"    åƒç´ å‡å€¼: {stats['mean_pixel_value']:.3f} Â± {stats['std_pixel_value']:.3f}")
        
        print(f"  ç±»åˆ«åˆ†å¸ƒ:")
        dist = metrics['class_distribution']
        print(f"    åˆ†å¸ƒç†µ: {dist['entropy']:.3f} (ç†æƒ³å€¼: {np.log2(self.config['cldm']['num_classes']):.3f})")
        print(f"    æœ€å¤§ç±»åˆ«å æ¯”: {dist['max_class_ratio']:.3f}")
        print(f"    ç”Ÿæˆç±»åˆ«æ•°: {dist['num_unique_classes']}/{self.config['cldm']['num_classes']}")
        
        if 'diversity' in metrics:
            div = metrics['diversity']
            print(f"  å›¾åƒå¤šæ ·æ€§:")
            print(f"    å¹³å‡æˆå¯¹å·®å¼‚: {div['mean_pairwise_mse']:.6f}")
        
        # 4. ä¿å­˜ç”Ÿæˆæ ·æœ¬
        if save_samples:
            import torchvision.utils as vutils
            save_path = os.path.join(self.save_dir, f"eval_samples_epoch_{self.epoch}.png")
            vutils.save_image(
                generated_images[:min(64, len(generated_images))], save_path,
                nrow=8, normalize=True, value_range=(0, 1)
            )
            print(f"âœ… å·²ä¿å­˜è¯„ä¼°æ ·æœ¬: {save_path}")
        
        return val_loss, metrics

def main():
    """ä¸»å‡½æ•°"""
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = "config_normalized_ldm.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("è¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨ç°æœ‰çš„config_paper_standard.yaml")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LDMTrainer(config_path)
    
    # é…ç½®æ•°æ®é›†è·¯å¾„ - ä½¿ç”¨Kaggleé€‚é…çš„æ•°æ®åŠ è½½å™¨
    print("ğŸ” é…ç½®æ•°æ®é›†...")
    
    try:
        # ä½¿ç”¨Kaggleé€‚é…çš„æ•°æ®åŠ è½½å™¨
        train_loader, val_loader, train_dataset_len, val_dataset_len = build_kaggle_dataloader(
            root_dir=trainer.config['dataset']['root_dir'],
            batch_size=trainer.config['dataset']['batch_size'],
            num_workers=trainer.config['dataset']['num_workers'],
            val_split=0.2  # 20%éªŒè¯é›†ï¼Œ80%è®­ç»ƒé›†
        )
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ:")
        print(f"  è®­ç»ƒé›†: {train_dataset_len} å¼ å›¾ç‰‡")
        print(f"  éªŒè¯é›†: {val_dataset_len} å¼ å›¾ç‰‡")
        print(f"  æ‰¹æ¬¡å¤§å°: {trainer.config['dataset']['batch_size']}")
        
        # åœ¨çœŸå®æ•°æ®ä¸Šè®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡
        print("ğŸ”§ åœ¨çœŸå®æ•°æ®é›†ä¸Šè®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡...")
        trainer.vqvae_wrapper.fit_normalization_on_real_data(train_loader, max_samples=500)
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹LDMè®­ç»ƒ...")
        train_losses, val_losses = trainer.train(train_loader, val_loader)
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ“‹ è¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:")
        print(f"  æ•°æ®é›†è·¯å¾„: {trainer.config['dataset']['root_dir']}")
        print(f"  æ‰¹æ¬¡å¤§å°: {trainer.config['dataset']['batch_size']}")
        print(f"  å·¥ä½œè¿›ç¨‹æ•°: {trainer.config['dataset']['num_workers']}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®ä¿å›¾ç‰‡æ ¼å¼ä¸º .jpg")
        print("3. æ£€æŸ¥Kaggleè¾“å…¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®æŒ‚è½½")
        print("4. å¯ä»¥å°è¯•å‡å°æ‰¹æ¬¡å¤§å°æˆ–å·¥ä½œè¿›ç¨‹æ•°")
        
        # æä¾›è°ƒè¯•ä¿¡æ¯
        print(f"\nğŸ” å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        if os.path.exists("/kaggle/input"):
            print("ğŸ“ Kaggleè¾“å…¥ç›®å½•:")
            for item in os.listdir("/kaggle/input"):
                item_path = f"/kaggle/input/{item}"
                if os.path.isdir(item_path):
                    print(f"  ğŸ“‚ {item}/")
                    try:
                        sub_items = os.listdir(item_path)[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
                        for sub_item in sub_items:
                            print(f"    ğŸ“„ {sub_item}")
                        if len(os.listdir(item_path)) > 5:
                            print(f"    ... å’Œå…¶ä»– {len(os.listdir(item_path)) - 5} ä¸ªæ–‡ä»¶")
                    except:
                        print(f"    âŒ æ— æ³•è®¿é—®")
                else:
                    print(f"  ğŸ“„ {item}")
        
        return
    
    print("âœ… LDMè®­ç»ƒæµç¨‹é…ç½®å®Œæˆï¼")

if __name__ == "__main__":
    main() 
