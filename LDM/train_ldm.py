"""
Train LDM - åŸºäºCompVis/latent-diffusion
ä½¿ç”¨å¾®è°ƒåçš„AutoencoderKLè®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹
"""
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import yaml
import json
from typing import Dict, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# FIDè¯„ä¼°ç›¸å…³å¯¼å…¥
from torchvision import models
from scipy import linalg
from torchvision.transforms.functional import resize, to_tensor

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from ldm_model import create_ldm_model, LatentDiffusionModel
# ä¿®æ”¹æ•°æ®åŠ è½½å™¨å¯¼å…¥
try:
    # å°è¯•å¯¼å…¥VAEçš„æ•°æ®åŠ è½½å™¨
    sys.path.append('../VAE')
    from dataset import build_dataloader as vae_build_dataloader
    print("âœ… æˆåŠŸå¯¼å…¥VAEæ•°æ®åŠ è½½å™¨")
except ImportError:
    print("âš ï¸  VAEæ•°æ®åŠ è½½å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åˆæˆæ•°æ®")
    vae_build_dataloader = None

# äº‘æœåŠ¡å™¨ç¯å¢ƒè®¾ç½®
print("ğŸŒ LDMè®­ç»ƒç¯å¢ƒåˆå§‹åŒ–...")
print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# æ£€æŸ¥CUDA
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸš€ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    device = 'cuda'
else:
    print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    device = 'cpu'

# å†…å­˜ä¼˜åŒ–
torch.backends.cudnn.benchmark = True
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'


class FIDEvaluator:
    """FIDè¯„ä¼°å™¨ï¼Œç”¨äºè®¡ç®—ç”Ÿæˆå›¾åƒçš„FIDåˆ†æ•°"""
    
    def __init__(self, device='cuda', inception_batch_size=64):
        self.device = device
        self.inception_batch_size = inception_batch_size
        
        # åŠ è½½é¢„è®­ç»ƒçš„Inception-v3æ¨¡å‹
        self.inception = models.inception_v3(pretrained=True, transform_input=False)
        self.inception.fc = nn.Identity()  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.inception.eval()
        self.inception.to(device)
        
        # å›¾åƒé¢„å¤„ç†ï¼ˆç”¨äºInception-v3ï¼‰
        self.preprocess = transforms.Compose([
            transforms.Resize((299, 299)),  # Inception-v3éœ€è¦299x299è¾“å…¥
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print("âœ… FIDè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_features(self, images):
        """
        æå–å›¾åƒç‰¹å¾
        è¾“å…¥: [N, 3, H, W] çš„RGBå›¾åƒï¼ŒèŒƒå›´[0, 1]
        """
        features = []
        
        with torch.no_grad():
            for i in range(0, len(images), self.inception_batch_size):
                batch = images[i:i + self.inception_batch_size]
                
                # ç¡®ä¿æ˜¯3é€šé“RGBå›¾åƒï¼ŒèŒƒå›´[0, 1]
                if batch.size(1) != 3:
                    raise ValueError(f"æœŸæœ›3é€šé“RGBå›¾åƒï¼Œå¾—åˆ°{batch.size(1)}é€šé“")
                
                # æ£€æŸ¥æ•°å€¼èŒƒå›´
                if batch.min() < -0.1 or batch.max() > 1.1:
                    print(f"âš ï¸  å›¾åƒåƒç´ èŒƒå›´å¼‚å¸¸: [{batch.min():.3f}, {batch.max():.3f}]")
                
                # è£å‰ªåˆ°[0, 1]èŒƒå›´
                batch = torch.clamp(batch, 0.0, 1.0)
                
                # è°ƒæ•´å¤§å°å¹¶æ ‡å‡†åŒ–
                batch_processed = []
                for img in batch:
                    img_resized = resize(img, (299, 299))
                    img_normalized = self.preprocess(img_resized)
                    batch_processed.append(img_normalized)
                
                batch_processed = torch.stack(batch_processed).to(self.device)
                
                # æå–ç‰¹å¾
                feat = self.inception(batch_processed)
                features.append(feat.cpu())
        
        return torch.cat(features, dim=0)
    
    def calculate_fid(self, real_features, fake_features):
        """è®¡ç®—FIDåˆ†æ•°"""
        # è½¬æ¢ä¸ºnumpy
        real_features = real_features.numpy()
        fake_features = fake_features.numpy()
        
        # è®¡ç®—å‡å€¼å’Œåæ–¹å·®
        mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
        mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)
        
        # è®¡ç®—FID
        diff = mu1 - mu2
        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
        return fid
    
    def evaluate_fid(self, ldm_model, real_loader, vae_model, config):
        """
        è¯„ä¼°FIDåˆ†æ•°
        
        Args:
            ldm_model: æ½œåœ¨æ‰©æ•£æ¨¡å‹
            real_loader: çœŸå®å›¾åƒæ•°æ®åŠ è½½å™¨
            vae_model: VAEæ¨¡å‹ï¼ˆå®é™…ä¸Šä¸éœ€è¦ï¼Œå› ä¸ºLDM.sample()å·²ç»åŒ…å«è§£ç ï¼‰
            config: é…ç½®å­—å…¸
        """
        fid_config = config.get('evaluation', {}).get('fid_evaluation', {})
        num_samples = fid_config.get('num_samples', 1000)
        batch_size = fid_config.get('batch_size', 32)
        num_inference_steps = fid_config.get('num_inference_steps', 50)
        num_classes = config.get('unet', {}).get('num_classes', 31)
        
        ldm_model.eval()
        
        # æ”¶é›†çœŸå®å›¾åƒï¼ˆRGBï¼ŒèŒƒå›´[0,1]ï¼‰
        print(f"ğŸ” æ”¶é›†çœŸå®å›¾åƒ ({num_samples} å¼ )...")
        real_images = []
        for batch_idx, batch in enumerate(real_loader):
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images = batch[0]  # [B, 3, H, W], èŒƒå›´[-1, 1]
            else:
                images = batch
            
            # è½¬æ¢åˆ°[0, 1]èŒƒå›´
            images = (images + 1.0) / 2.0
            images = torch.clamp(images, 0.0, 1.0)
            
            real_images.append(images)
            if len(real_images) * images.size(0) >= num_samples:
                break
        
        real_images = torch.cat(real_images, dim=0)[:num_samples]
        
        # ç”Ÿæˆå‡å›¾åƒ
        print(f"ğŸ¨ ç”Ÿæˆå‡å›¾åƒ ({num_samples} å¼ )...")
        fake_images = []
        
        with torch.no_grad():
            num_batches = (num_samples + batch_size - 1) // batch_size
            
            for i in tqdm(range(num_batches), desc="ç”Ÿæˆå›¾åƒ"):
                current_batch_size = min(batch_size, num_samples - i * batch_size)
                
                # éšæœºç”Ÿæˆç±»åˆ«æ ‡ç­¾
                if num_classes > 1:
                    class_labels = torch.randint(0, num_classes, (current_batch_size,), device=self.device)
                else:
                    class_labels = None
                
                # LDMç”ŸæˆRGBå›¾åƒï¼ˆå·²ç»åŒ…å«äº†æ½œå˜é‡é‡‡æ ·å’ŒVAEè§£ç ï¼‰
                generated_images = ldm_model.sample(
                    num_samples=current_batch_size,
                    class_labels=class_labels,
                    num_inference_steps=num_inference_steps,
                    eta=0.0
                )  # è¿”å› [B, 3, H, W]ï¼ŒèŒƒå›´[-1, 1]
                
                # è½¬æ¢åˆ°[0, 1]èŒƒå›´
                generated_images = (generated_images + 1.0) / 2.0
                generated_images = torch.clamp(generated_images, 0.0, 1.0)
                
                fake_images.append(generated_images.cpu())
        
        fake_images = torch.cat(fake_images, dim=0)[:num_samples]
        
        # æå–ç‰¹å¾
        print(f"ğŸ§  æå–çœŸå®å›¾åƒç‰¹å¾...")
        real_features = self.extract_features(real_images)
        
        print(f"ğŸ§  æå–ç”Ÿæˆå›¾åƒç‰¹å¾...")
        fake_features = self.extract_features(fake_images)
        
        # è®¡ç®—FID
        fid_score = self.calculate_fid(real_features, fake_features)
        
        return fid_score, fake_images[:8]  # è¿”å›FIDåˆ†æ•°å’Œä¸€äº›ç”Ÿæˆæ ·æœ¬ç”¨äºå¯è§†åŒ–


class DataLoaderWrapper:
    """æ•°æ®åŠ è½½å™¨åŒ…è£…å™¨ï¼Œå°†VAEçš„(image, label)æ ¼å¼é€‚é…ä¸ºLDMéœ€è¦çš„æ ¼å¼"""
    
    def __init__(self, original_loader):
        self.original_loader = original_loader
        self.batch_size = original_loader.batch_size
        self.dataset = original_loader.dataset
    
    def __iter__(self):
        for batch in self.original_loader:
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images, labels = batch[0], batch[1]
                yield images, labels
            else:
                raise ValueError(f"æ— æ³•å¤„ç†çš„æ‰¹æ¬¡æ ¼å¼: {type(batch)}")
    
    def __len__(self):
        return len(self.original_loader)


class LDMTrainer:
    """LDMè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        config: Dict[str, Any],
        vae_checkpoint_path: str = None,
        device: str = 'cuda'
    ):
        self.config = config
        self.device = device
        self.vae_checkpoint_path = vae_checkpoint_path
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = config.get('output_dir', './ldm_outputs')
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'checkpoints'), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'samples'), exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_optimizer()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self._init_dataloader()
        
        # åˆå§‹åŒ–FIDè¯„ä¼°å™¨
        fid_config = self.config.get('evaluation', {}).get('fid_evaluation', {})
        inception_batch_size = fid_config.get('inception_batch_size', 64)
        self.fid_evaluator = FIDEvaluator(device=self.device, inception_batch_size=inception_batch_size)
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'val_loss': [],
            'fid_score': [],
            'lr': []
        }
        
        # æœ€ä½³éªŒè¯æŸå¤±å’ŒFID
        self.best_val_loss = float('inf')
        self.best_fid_score = float('inf')
        
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("ğŸ”§ åˆå§‹åŒ–LDMæ¨¡å‹...")
        
        # è·å–é…ç½®
        unet_config = self.config.get('unet', {})
        diffusion_config = self.config.get('diffusion', {})
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_ldm_model(
            vae_checkpoint_path=self.vae_checkpoint_path,
            unet_config=unet_config,
            diffusion_config=diffusion_config,
            device=self.device
        )
        
        # ä¿å­˜VAEæ¨¡å‹å¼•ç”¨ï¼Œç”¨äºFIDè¯„ä¼°æ—¶çš„è§£ç 
        self.vae_model = self.model.vae
        
        print(f"âœ… LDMæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   è®­ç»ƒæ¯”ä¾‹: {trainable_params / total_params * 100:.1f}%")
        
    def _init_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        train_config = self.config.get('training', {})
        
        # ä¼˜åŒ–å™¨
        lr = train_config.get('learning_rate', 1e-4)
        weight_decay = train_config.get('weight_decay', 0.01)
        
        self.optimizer = self.model.configure_optimizers(lr=lr, weight_decay=weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_type = train_config.get('scheduler', 'cosine')
        if scheduler_type == 'cosine':
            # ç¡®ä¿eta_minæ˜¯æµ®ç‚¹æ•°ç±»å‹
            min_lr = train_config.get('min_lr', 1e-6)
            if isinstance(min_lr, str):
                min_lr = float(min_lr)
            
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=train_config.get('epochs', 100),
                eta_min=min_lr
            )
        elif scheduler_type == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=train_config.get('step_size', 30),
                gamma=train_config.get('gamma', 0.1)
            )
        else:
            self.scheduler = None
            
        print(f"ğŸ“ˆ ä¼˜åŒ–å™¨: AdamW (lr={lr}, weight_decay={weight_decay})")
        if self.scheduler:
            print(f"ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")
        
    def _init_dataloader(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        data_config = self.config.get('data', {})
        
        # æ•°æ®è·¯å¾„
        data_dir = data_config.get('data_dir', '/kaggle/input/dataset/dataset')
        
        # æ•°æ®å¢å¼º - ä¿æŒä¸VAEè®­ç»ƒä¸€è‡´çš„å½’ä¸€åŒ–
        train_transforms = transforms.Compose([
            transforms.Resize((256, 256)),  # åŒ¹é…VAEè®­ç»ƒæ—¶çš„å°ºå¯¸
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]
        ])
        
        val_transforms = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        # æ‰¹æ¬¡å¤§å°
        batch_size = data_config.get('batch_size', 8)
        num_workers = data_config.get('num_workers', 2)
        
        try:
            if vae_build_dataloader is not None:
                # ä½¿ç”¨VAEçš„æ•°æ®åŠ è½½å™¨
                print(f"ğŸ”„ ä½¿ç”¨VAEæ•°æ®åŠ è½½å™¨")
                train_loader, val_loader, train_size, val_size = vae_build_dataloader(
                    root_dir=data_dir,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    shuffle_train=True,
                    shuffle_val=False,
                    val_split=0.2,  # 20%éªŒè¯é›†
                    random_state=42
                )
                
                # åŒ…è£…æ•°æ®åŠ è½½å™¨ä»¥é€‚é…LDMæ ¼å¼
                self.train_loader = DataLoaderWrapper(train_loader)
                self.val_loader = DataLoaderWrapper(val_loader)
                
                print(f"âœ… VAEæ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
                print(f"   è®­ç»ƒé›†: {train_size} æ ·æœ¬, {len(self.train_loader)} æ‰¹æ¬¡")
                print(f"   éªŒè¯é›†: {val_size} æ ·æœ¬, {len(self.val_loader)} æ‰¹æ¬¡")
                print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            else:
                # å›é€€åˆ°åˆæˆæ•°æ®
                self._create_synthetic_dataloaders(batch_size, num_workers)
                
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
            print(f"ğŸ”„ å›é€€åˆ°åˆæˆæ•°æ®...")
            self._create_synthetic_dataloaders(batch_size, num_workers)
    
    def _create_synthetic_dataloaders(self, batch_size: int, num_workers: int):
        """åˆ›å»ºåˆæˆæ•°æ®åŠ è½½å™¨ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        from torch.utils.data import Dataset, DataLoader
        
        class SyntheticDataset(Dataset):
            def __init__(self, num_samples: int, num_classes: int):
                self.num_samples = num_samples
                self.num_classes = num_classes
            
            def __len__(self):
                return self.num_samples
            
            def __getitem__(self, idx):
                # ç”Ÿæˆéšæœºå›¾åƒï¼ŒèŒƒå›´[-1, 1]
                image = torch.randn(3, 256, 256)
                label = torch.randint(0, self.num_classes, (1,)).item()
                return image, label
        
        num_classes = self.config.get('unet', {}).get('num_classes', 31)
        
        train_dataset = SyntheticDataset(1000, num_classes)
        val_dataset = SyntheticDataset(200, num_classes)
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,  # åˆæˆæ•°æ®ä¸éœ€è¦å¤šè¿›ç¨‹
            pin_memory=True,
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True,
            drop_last=False
        )
        
        print(f"âœ… åˆæˆæ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, {len(self.train_loader)} æ‰¹æ¬¡")
        print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬, {len(self.val_loader)} æ‰¹æ¬¡")
        print(f"   ç±»åˆ«æ•°: {num_classes}")

    def train_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {epoch+1} - Training",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # è§£åŒ…æ‰¹æ¬¡æ•°æ®
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images, class_labels = batch[0], batch[1]
            else:
                images = batch
                class_labels = None
            
            images = images.to(self.device)
            if class_labels is not None:
                class_labels = class_labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            loss_dict = self.model(images, class_labels)
            loss = loss_dict['loss']
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.unet.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            avg_loss = total_loss / (batch_idx + 1)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg': f'{avg_loss:.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # æ¸…ç†æ˜¾å­˜
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()
        
        return total_loss / num_batches
    
    @torch.no_grad()
    def validate_epoch(self, epoch: int) -> float:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        num_batches = len(self.val_loader)
        
        progress_bar = tqdm(
            self.val_loader,
            desc=f"Epoch {epoch+1} - Validation",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # è§£åŒ…æ‰¹æ¬¡æ•°æ®
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images, class_labels = batch[0], batch[1]
            else:
                images = batch
                class_labels = None
                
            images = images.to(self.device)
            if class_labels is not None:
                class_labels = class_labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            loss_dict = self.model(images, class_labels)
            loss = loss_dict['loss']
            
            total_loss += loss.item()
            avg_loss = total_loss / (batch_idx + 1)
            
            progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})
        
        return total_loss / num_batches
    
    @torch.no_grad()
    def evaluate_fid(self, epoch: int) -> float:
        """è¯„ä¼°FIDåˆ†æ•°"""
        print(f"ğŸ¯ å¼€å§‹FIDè¯„ä¼° (Epoch {epoch + 1})...")
        
        try:
            fid_score, sample_images = self.fid_evaluator.evaluate_fid(
                ldm_model=self.model,
                real_loader=self.val_loader,
                vae_model=None,  # ä¸å†éœ€è¦ï¼Œå› ä¸ºmodel.sample()å·²ç»åŒ…å«è§£ç 
                config=self.config
            )
            
            print(f"ğŸ“Š FID Score: {fid_score:.2f}")
            
            # ä¿å­˜FIDè¯„ä¼°çš„æ ·æœ¬å›¾åƒ
            if len(sample_images) > 0:
                sample_images = (sample_images + 1.0) / 2.0  # [-1,1] -> [0,1]
                sample_images = torch.clamp(sample_images, 0.0, 1.0)
                
                fig, axes = plt.subplots(2, 4, figsize=(12, 6))
                axes = axes.flatten()
                
                for i in range(min(8, len(sample_images))):
                    img = sample_images[i].permute(1, 2, 0).cpu().numpy()
                    axes[i].imshow(img)
                    axes[i].axis('off')
                    axes[i].set_title(f'Generated')
                
                plt.tight_layout()
                fid_sample_path = os.path.join(self.output_dir, 'samples', f'fid_epoch_{epoch+1:03d}.png')
                plt.savefig(fid_sample_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                print(f"ğŸ“¸ FIDæ ·æœ¬å›¾åƒå·²ä¿å­˜: {fid_sample_path}")
            
            return fid_score
            
        except Exception as e:
            print(f"âŒ FIDè¯„ä¼°å¤±è´¥: {e}")
            return float('inf')
    
    @torch.no_grad()
    def generate_samples(self, epoch: int, num_samples: int = 4):
        """ç”Ÿæˆæ ·æœ¬å›¾åƒ"""
        self.model.eval()
        
        # ä»é…ç½®è¯»å–å‚æ•°
        sample_config = self.config.get('evaluation', {}).get('sample_generation', {})
        inference_steps = sample_config.get('inference_steps', 50)
        
        # éšæœºé€‰æ‹©ç±»åˆ«æ ‡ç­¾
        if self.model.unet.num_classes and self.model.unet.num_classes > 1:
            class_labels = torch.randint(0, self.model.unet.num_classes, (num_samples,), device=self.device)
        else:
            class_labels = None
        
        # ç”Ÿæˆå›¾åƒ
        generated_images = self.model.sample(
            num_samples=num_samples,
            class_labels=class_labels,
            num_inference_steps=inference_steps,
            eta=0.0  # DDIMç¡®å®šæ€§é‡‡æ ·
        )
        
        # è½¬æ¢åˆ°[0, 1]èŒƒå›´
        generated_images = (generated_images + 1.0) / 2.0
        generated_images = torch.clamp(generated_images, 0.0, 1.0)
        
        # ä¿å­˜å›¾åƒ
        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 3, 3))
        if num_samples == 1:
            axes = [axes]
        
        for i in range(num_samples):
            img = generated_images[i].cpu().permute(1, 2, 0).numpy()
            axes[i].imshow(img)
            axes[i].axis('off')
            if class_labels is not None:
                axes[i].set_title(f'Class {class_labels[i].item()}')
        
        plt.tight_layout()
        sample_path = os.path.join(self.output_dir, 'samples', f'epoch_{epoch+1:03d}.png')
        plt.savefig(sample_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“¸ æ ·æœ¬å›¾åƒå·²ä¿å­˜: {sample_path}")
    
    def save_checkpoint(self, epoch: int, is_best: bool = False, is_best_fid: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.output_dir, 'checkpoints', 'latest.pth')
        self.model.save_checkpoint(
            filepath=checkpoint_path,
            epoch=epoch,
            optimizer_state=self.optimizer.state_dict(),
            scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
            metrics={'train_history': self.train_history}
        )
        
        # ä¿å­˜æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.output_dir, 'checkpoints', 'best_val_loss.pth')
            
            # åˆ é™¤æ—§çš„æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹
            old_best_path = os.path.join(self.output_dir, 'checkpoints', 'best_val_loss.pth')
            if os.path.exists(old_best_path):
                try:
                    os.remove(old_best_path)
                    print(f"ğŸ—‘ï¸  åˆ é™¤æ—§çš„æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤æ—§æ¨¡å‹å¤±è´¥: {e}")
            
            self.model.save_checkpoint(
                filepath=best_path,
                epoch=epoch,
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                metrics={'train_history': self.train_history}
            )
            print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹å·²ä¿å­˜: {best_path}")
        
        # ä¿å­˜æœ€ä½³FIDæ¨¡å‹
        if is_best_fid:
            best_fid_path = os.path.join(self.output_dir, 'checkpoints', 'best_fid.pth')
            
            # åˆ é™¤æ—§çš„æœ€ä½³FIDæ¨¡å‹
            old_best_fid_path = os.path.join(self.output_dir, 'checkpoints', 'best_fid.pth')
            if os.path.exists(old_best_fid_path):
                try:
                    os.remove(old_best_fid_path)
                    print(f"ğŸ—‘ï¸  åˆ é™¤æ—§çš„æœ€ä½³FIDæ¨¡å‹")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤æ—§FIDæ¨¡å‹å¤±è´¥: {e}")
            
            self.model.save_checkpoint(
                filepath=best_fid_path,
                epoch=epoch,
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                metrics={'train_history': self.train_history}
            )
            print(f"ğŸ¯ æœ€ä½³FIDæ¨¡å‹å·²ä¿å­˜: {best_fid_path}")
        
        # å®šæœŸä¿å­˜ï¼ˆæ ¹æ®é…ç½®ï¼‰
        save_every_epochs = self.config.get('save_every_epochs', 10)
        if (epoch + 1) % save_every_epochs == 0:
            periodic_path = os.path.join(self.output_dir, 'checkpoints', f'epoch_{epoch+1:03d}.pth')
            self.model.save_checkpoint(
                filepath=periodic_path,
                epoch=epoch,
                metrics={'train_history': self.train_history}
            )
            
            # åˆ é™¤æ—§çš„å®šæœŸæ£€æŸ¥ç‚¹ï¼ˆä¿ç•™æœ€è¿‘3ä¸ªï¼‰
            checkpoints_dir = os.path.join(self.output_dir, 'checkpoints')
            try:
                # è·å–æ‰€æœ‰epochæ£€æŸ¥ç‚¹æ–‡ä»¶
                epoch_files = []
                for filename in os.listdir(checkpoints_dir):
                    if filename.startswith('epoch_') and filename.endswith('.pth'):
                        try:
                            epoch_num = int(filename.split('_')[1].split('.')[0])
                            epoch_files.append((epoch_num, filename))
                        except (ValueError, IndexError):
                            continue
                
                # æŒ‰epochç¼–å·æ’åº
                epoch_files.sort(key=lambda x: x[0])
                
                # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹ï¼Œä¿ç•™æœ€è¿‘3ä¸ª
                if len(epoch_files) > 3:
                    files_to_delete = epoch_files[:-3]  # é™¤äº†æœ€å3ä¸ª
                    for epoch_num, filename in files_to_delete:
                        old_path = os.path.join(checkpoints_dir, filename)
                        if os.path.exists(old_path):
                            os.remove(old_path)
                            print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {filename}")
                            
            except Exception as e:
                print(f"âš ï¸  æ¸…ç†æ—§æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def train(self, start_epoch: int = 0):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        train_config = self.config.get('training', {})
        epochs = train_config.get('epochs', 100)
        
        print(f"ğŸš€ å¼€å§‹LDMè®­ç»ƒ...")
        print(f"   æ€»è½®æ¬¡: {epochs}")
        print(f"   èµ·å§‹è½®æ¬¡: {start_epoch + 1}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        for epoch in range(start_epoch, epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss = self.validate_epoch(epoch)
            
            # FIDè¯„ä¼°ï¼ˆæ ¹æ®é…ç½®ï¼‰
            fid_config = self.config.get('evaluation', {}).get('fid_evaluation', {})
            sample_config = self.config.get('evaluation', {}).get('sample_generation', {})
            
            fid_score = None
            if fid_config.get('enabled', True) and (epoch + 1) % fid_config.get('eval_every_epochs', 5) == 0:
                fid_score = self.evaluate_fid(epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_history['epoch'].append(epoch + 1)
            self.train_history['train_loss'].append(train_loss)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['fid_score'].append(fid_score if fid_score is not None else None)
            self.train_history['lr'].append(self.optimizer.param_groups[0]['lr'])
            
            # æ‰“å°ç»Ÿè®¡
            print(f"ğŸ“Š Train Loss: {train_loss:.4f}")
            print(f"ğŸ“Š Val Loss: {val_loss:.4f}")
            if fid_score is not None:
                print(f"ğŸ“Š FID Score: {fid_score:.2f}")
            print(f"ğŸ“Š Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            is_best_fid = False
            
            if is_best:
                self.best_val_loss = val_loss
                print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            if fid_score is not None and fid_score < self.best_fid_score:
                self.best_fid_score = fid_score
                is_best_fid = True
                print(f"ğŸ¯ æ–°çš„æœ€ä½³FIDåˆ†æ•°: {fid_score:.2f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best, is_best_fid)
            
            # ç”Ÿæˆæ ·æœ¬ï¼ˆæ ¹æ®é…ç½®ï¼‰
            if (epoch + 1) % sample_config.get('sample_every_epochs', 5) == 0:
                num_samples = sample_config.get('num_sample_images', 4)
                self.generate_samples(epoch, num_samples)
            
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"ğŸ¯ æœ€ä½³FIDåˆ†æ•°: {self.best_fid_score:.2f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(self.output_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.train_history, f, indent=2)
        print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


def load_config(config_path: str = None) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path and os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config = yaml.safe_load(f)
            else:
                config = json.load(f)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
    else:
        # é»˜è®¤é…ç½®
        config = {
            'data': {
                'data_dir': '/kaggle/input/dataset/dataset',
                'batch_size': 8,  # ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°
                'num_workers': 2
            },
            'unet': {
                'in_channels': 4,
                'out_channels': 4,
                'model_channels': 320,
                'num_res_blocks': 2,
                'attention_resolutions': [4, 2, 1],
                'channel_mult': [1, 2, 4, 4],
                'num_classes': 31,
                'dropout': 0.1,
                'num_heads': 8,
                'use_scale_shift_norm': True,
                'resblock_updown': True
            },
            'diffusion': {
                'timesteps': 1000,
                'beta_schedule': 'cosine',
                'beta_start': 0.0001,  # 0.0001
                'beta_end': 0.02
            },
            'training': {
                'epochs': 50,
                'learning_rate': 0.0001,  # 1e-4
                'weight_decay': 0.01,
                'scheduler': 'cosine',
                'min_lr': 0.000001  # 1e-6
            },
            'output_dir': './ldm_outputs'
        }
        print("âš ï¸  ä½¿ç”¨é»˜è®¤é…ç½®")
    
    return config


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    config_path = "ldm_config.yaml"  # å¦‚æœå­˜åœ¨çš„è¯
    vae_checkpoint_path = None  # å°†æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
    
    # æŸ¥æ‰¾VAEæ£€æŸ¥ç‚¹
    possible_vae_paths = [
        "/kaggle/input/vae-model/vae_finetuned_epoch_23.pth",  # ç”¨æˆ·æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹
        "VAE/vae_model_best.pth",
        "VAE/vae_model_final.pth",
        "VAE/checkpoints/best_vae_checkpoint.pth",
        "VAE/checkpoints/vae_epoch_025.pth",
        "../VAE/vae_model_best.pth"
    ]
    
    for path in possible_vae_paths:
        if os.path.exists(path):
            vae_checkpoint_path = path
            print(f"ğŸ” æ‰¾åˆ°VAEæ£€æŸ¥ç‚¹: {path}")
            break
    
    if vae_checkpoint_path is None:
        print("âš ï¸  æœªæ‰¾åˆ°VAEæ£€æŸ¥ç‚¹ï¼Œå°†ä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusion VAE")
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LDMTrainer(
        config=config,
        vae_checkpoint_path=vae_checkpoint_path,
        device=device
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main() 
