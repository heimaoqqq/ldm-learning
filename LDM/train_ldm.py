import os
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import sys
import time
import math
from tqdm import tqdm
import matplotlib.pyplot as plt
import torchvision.utils as vutils
from typing import Dict, Any
import json

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from ldm import LatentDiffusionModel
# ä»VAEç›®å½•å¯¼å…¥datasetåŠŸèƒ½
from dataset import build_dataloader
from fid_evaluation import FIDEvaluator
from metrics import DiffusionMetrics, denormalize_for_metrics

def check_tensor_health(tensor, name="tensor"):
    """æ£€æŸ¥å¼ é‡çš„æ•°å€¼å¥åº·çŠ¶å†µ"""
    if torch.any(torch.isnan(tensor)):
        print(f"âŒ {name} åŒ…å« NaN å€¼")
        return False
    if torch.any(torch.isinf(tensor)):
        print(f"âŒ {name} åŒ…å« Inf å€¼")
        return False
    if tensor.abs().max() > 1000:
        print(f"âš ï¸ {name} åŒ…å«æå¤§å€¼: max={tensor.abs().max().item():.2f}")
        return False
    return True

def safe_mse_loss(pred, target, name="loss"):
    """å®‰å…¨çš„MSEæŸå¤±è®¡ç®—ï¼Œé˜²æ­¢NaN/Inf"""
    # æ£€æŸ¥è¾“å…¥
    if not check_tensor_health(pred, f"{name}_pred"):
        pred = torch.clamp(pred, -10, 10)
    if not check_tensor_health(target, f"{name}_target"):
        target = torch.clamp(target, -10, 10)
    
    # è®¡ç®—æŸå¤±
    loss = torch.nn.functional.mse_loss(pred, target)
    
    # æ£€æŸ¥æŸå¤±å€¼
    if torch.isnan(loss) or torch.isinf(loss):
        print(f"âš ï¸ {name} è®¡ç®—å‡ºç°å¼‚å¸¸ï¼Œä½¿ç”¨å®‰å…¨å€¼")
        loss = torch.tensor(1.0, device=pred.device, requires_grad=True)
    
    # é™åˆ¶æŸå¤±èŒƒå›´
    loss = torch.clamp(loss, 0.0, 100.0)
    return loss

def get_cosine_schedule_with_warmup(
    optimizer: torch.optim.Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    num_cycles: float = 0.5,
    last_epoch: int = -1,
):
    """
    åˆ›å»ºå¸¦é¢„çƒ­çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)

def get_cosine_schedule_with_restarts(
    optimizer: torch.optim.Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    num_cycles: int = 3,  # ğŸ”§ é‡å¯æ¬¡æ•°
    restart_decay: float = 0.8,  # ğŸ”§ æ¯æ¬¡é‡å¯çš„å­¦ä¹ ç‡è¡°å‡
    last_epoch: int = -1,
):
    """
    ğŸ”§ åˆ›å»ºå¸¦é‡å¯çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´å¼ºçš„å­¦ä¹ èƒ½åŠ›
    """
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        
        # è®¡ç®—å½“å‰åœ¨ç¬¬å‡ ä¸ªå‘¨æœŸ
        cycle_length = 1.0 / num_cycles
        current_cycle = int(progress / cycle_length)
        cycle_progress = (progress % cycle_length) / cycle_length
        
        # æ¯æ¬¡é‡å¯æ—¶é™ä½åŸºç¡€å­¦ä¹ ç‡
        base_lr = restart_decay ** current_cycle
        
        # ä½™å¼¦è¡°å‡
        cosine_factor = 0.5 * (1.0 + math.cos(math.pi * cycle_progress))
        
        return max(0.0, base_lr * cosine_factor)

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)

def denormalize(x):
    """åå½’ä¸€åŒ–å›¾åƒï¼Œä»[-1,1]è½¬æ¢åˆ°[0,1]"""
    return (x * 0.5 + 0.5).clamp(0, 1)

def save_sample_images(model, device, config, epoch, save_dir):
    """ç”Ÿæˆå¹¶ä¿å­˜æ ·æœ¬å›¾åƒ"""
    model.eval()
    with torch.no_grad():
        # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆæ ·æœ¬
        num_classes = config['unet']['num_classes']
        num_samples_per_class = min(4, config['inference']['num_samples_per_class'])
        
        all_images = []
        for class_id in range(min(8, num_classes)):  # æœ€å¤šæ˜¾ç¤º8ä¸ªç±»åˆ«
            class_labels = torch.tensor([class_id] * num_samples_per_class, device=device)
            
            generated_images = model.sample(
                batch_size=num_samples_per_class,
                class_labels=class_labels,
                num_inference_steps=config['inference']['num_inference_steps'],
                guidance_scale=config['inference']['guidance_scale'],
                eta=config['inference']['eta'],
            )
            
            # åå½’ä¸€åŒ–
            generated_images = denormalize(generated_images)
            all_images.append(generated_images)
        
        # æ‹¼æ¥æ‰€æœ‰å›¾åƒ
        if all_images:
            all_images = torch.cat(all_images, dim=0)
            
            # ä¿å­˜å›¾åƒç½‘æ ¼
            grid = vutils.make_grid(all_images, nrow=num_samples_per_class, normalize=False, padding=2)
            save_path = os.path.join(save_dir, f"samples_epoch_{epoch:04d}.png")
            vutils.save_image(grid, save_path)
            print(f"æ ·æœ¬å›¾åƒå·²ä¿å­˜: {save_path}")

def save_model_checkpoint(model, save_path, epoch=None, extra_info=None):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹çš„ç»Ÿä¸€å‡½æ•°"""
    os.makedirs(save_path, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹æƒé‡
    model_path = os.path.join(save_path, 'model.pth')
    torch.save(model.state_dict(), model_path)
    
    # ä¿å­˜æ¨¡å‹é…ç½®å’Œé¢å¤–ä¿¡æ¯
    checkpoint_info = {
        'model_type': 'LatentDiffusionModel',
        'epoch': epoch,
        'model_path': model_path,
        'timestamp': time.time()
    }
    
    if extra_info:
        checkpoint_info.update(extra_info)
    
    info_path = os.path.join(save_path, 'checkpoint_info.json')
    with open(info_path, 'w') as f:
        json.dump(checkpoint_info, f, indent=2)
    
    return model_path

def train_ldm():
    """LDMè®­ç»ƒä¸»å‡½æ•°"""
    
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è®¾å¤‡é…ç½®
    if config['device'] == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(config['device'])
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ğŸ†• æ¢¯åº¦ç´¯ç§¯é…ç½®
    accumulation_steps = config['training'].get('gradient_accumulation_steps', 4)  # é»˜è®¤ç´¯ç§¯4æ­¥
    effective_batch_size = config['dataset']['batch_size'] * accumulation_steps
    print(f"ğŸ”§ æ¢¯åº¦ç´¯ç§¯é…ç½®: ç´¯ç§¯æ­¥æ•°={accumulation_steps}, æœ‰æ•ˆbatch size={effective_batch_size}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = config['training']['save_dir']
    os.makedirs(save_dir, exist_ok=True)
    
    sample_dir = os.path.join(save_dir, "samples")
    os.makedirs(sample_dir, exist_ok=True)
    
    log_dir = os.path.join(save_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, train_dataset_len, val_dataset_len = build_dataloader(
        config['dataset']['root_dir'],
        batch_size=config['dataset']['batch_size'],
        num_workers=config['dataset']['num_workers'],
        shuffle_train=True,
        shuffle_val=False,
        val_split=config['dataset']['val_split']
    )
    
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆ: è®­ç»ƒé›† {train_dataset_len}, éªŒè¯é›† {val_dataset_len}")
    
    # åˆå§‹åŒ–LDMæ¨¡å‹
    print("åˆå§‹åŒ–LDMæ¨¡å‹...")
    
    # å¤„ç†VAEè·¯å¾„
    vae_path = config['vae']['model_path']
    if vae_path.startswith('../'):
        vae_path = os.path.join(os.path.dirname(__file__), vae_path)
    
    # æ£€æŸ¥VAEæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(vae_path):
        print(f"âŒ é”™è¯¯: VAEæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {vae_path}")
        print(f"ğŸ’¡ è¯·å…ˆè®­ç»ƒVAEæ¨¡å‹ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤:")
        print(f"   cd VAE")
        print(f"   python train_adv_vqvae.py")
        print(f"è®­ç»ƒå®Œæˆåï¼ŒVAEæ¨¡å‹å°†ä¿å­˜ä¸º vae_models/adv_vqvae_best_fid.pth")
        print(f"ç„¶åå†è¿è¡ŒLDMè®­ç»ƒ")
        return
    
    print(f"âœ“ å‘ç°VAEæƒé‡æ–‡ä»¶: {vae_path}")
    
    # ğŸ”§ æ›´æ–°VAEé…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„
    config['vae']['model_path'] = vae_path
    
    model = LatentDiffusionModel(config).to(device)
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡å¥åº·çŠ¶å†µ
    print("ğŸ” æ£€æŸ¥æ¨¡å‹æƒé‡...")
    total_params = 0
    healthy_params = 0
    for name, param in model.unet.named_parameters():
        total_params += param.numel()
        if torch.any(torch.isnan(param)) or torch.any(torch.isinf(param)):
            print(f"âš ï¸ å‚æ•° {name} åŒ…å«å¼‚å¸¸å€¼")
            # é‡æ–°åˆå§‹åŒ–å¼‚å¸¸å‚æ•°
            nn.init.xavier_uniform_(param)
        else:
            healthy_params += param.numel()
    
    print(f"âœ… æ¨¡å‹æƒé‡æ£€æŸ¥å®Œæˆ: {healthy_params}/{total_params} å‚æ•°æ­£å¸¸")
    
    # åˆå§‹åŒ–æ‰©æ•£æŒ‡æ ‡è®¡ç®—å™¨
    print("åˆå§‹åŒ–æ‰©æ•£æŒ‡æ ‡è®¡ç®—å™¨...")
    diffusion_metrics = DiffusionMetrics(device=device)
    
    # åªæœ‰å¯ç”¨FIDè¯„ä¼°æ—¶æ‰åˆå§‹åŒ–
    fid_evaluator = None
    if config['fid_evaluation']['enabled']:
        print("åˆå§‹åŒ–FIDè¯„ä¼°å™¨...")
        fid_evaluator = FIDEvaluator(device=device)
        # è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾
        print("è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾ç”¨äºFIDè¯„ä¼°...")
        fid_evaluator.compute_real_features(val_loader, max_samples=1000)
    
    # ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
    learning_rate = float(config['training']['lr'])
    weight_decay = float(config['training']['weight_decay'])
    
    # ğŸ†• è°ƒæ•´å­¦ä¹ ç‡ä»¥é€‚åº”æ¢¯åº¦ç´¯ç§¯
    effective_lr = learning_rate * accumulation_steps
    print(f"ğŸ“Š è®­ç»ƒå‚æ•°: åŸºç¡€LR={learning_rate:.6f}, æœ‰æ•ˆLR={effective_lr:.6f}, æ¢¯åº¦è£å‰ª={config['training']['grad_clip_norm']}")
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨è°ƒæ•´åçš„å­¦ä¹ ç‡
    optimizer = optim.AdamW(
        model.unet.parameters(),  # åªä¼˜åŒ–U-Netå‚æ•°
        lr=effective_lr,  # ğŸ†• ä½¿ç”¨æœ‰æ•ˆå­¦ä¹ ç‡
        weight_decay=weight_decay,
        eps=1e-8,  # å¢åŠ æ•°å€¼ç¨³å®šæ€§
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['training']['use_scheduler']:
        # ğŸ†• è°ƒæ•´æ€»æ­¥æ•°ä»¥é€‚åº”æ¢¯åº¦ç´¯ç§¯
        total_steps = (len(train_loader) // accumulation_steps) * config['training']['epochs']
        
        # ğŸ”§ æ ¹æ®é…ç½®é€‰æ‹©è°ƒåº¦å™¨ç±»å‹
        scheduler_type = config['training'].get('scheduler_type', 'cosine')
        
        if scheduler_type == "cosine_with_restarts":
            print(f"ğŸ“ˆ ä½¿ç”¨å¸¦é‡å¯çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨")
            scheduler = get_cosine_schedule_with_restarts(
                optimizer,
                num_warmup_steps=config['training']['warmup_steps'] // accumulation_steps,  # ğŸ†• è°ƒæ•´é¢„çƒ­æ­¥æ•°
                num_training_steps=total_steps,
                num_cycles=3,  # 3æ¬¡é‡å¯å‘¨æœŸ
                restart_decay=0.8,  # æ¯æ¬¡é‡å¯è¡°å‡åˆ°80%
            )
        else:
            print(f"ğŸ“ˆ ä½¿ç”¨æ ‡å‡†ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨")
            scheduler = get_cosine_schedule_with_warmup(
                optimizer,
                num_warmup_steps=config['training']['warmup_steps'] // accumulation_steps,  # ğŸ†• è°ƒæ•´é¢„çƒ­æ­¥æ•°
                num_training_steps=total_steps,
            )
    else:
        scheduler = None
    
    # è®­ç»ƒçŠ¶æ€å’Œæ—¥å¿—
    start_epoch = 0
    best_loss = float('inf')
    best_fid = float('inf')
    global_step = 0
    
    # è®­ç»ƒæ—¥å¿—
    training_log = {
        'epochs': [],
        'train_loss': [],
        'val_loss': [],
        'fid_scores': [],
        'inception_scores': [],
        'noise_metrics': [],
        'learning_rates': []
    }
    
    # è®­ç»ƒå¾ªç¯
    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {config['training']['epochs']} ä¸ªepoch...")
    
    for epoch in range(start_epoch, config['training']['epochs']):
        model.train()
        epoch_loss = 0.0
        epoch_start_time = time.time()
        valid_batches = 0  # è·Ÿè¸ªæœ‰æ•ˆæ‰¹æ¬¡æ•°
        
        # ğŸ†• æ¢¯åº¦ç´¯ç§¯ç›¸å…³å˜é‡
        accumulated_loss = 0.0
        accumulation_count = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['training']['epochs']}")
        
        for batch_idx, (images, labels) in enumerate(progress_bar):
            try:
                images = images.to(device)
                labels = labels.to(device)
                
                # æ£€æŸ¥è¾“å…¥æ•°æ®å¥åº·çŠ¶å†µ
                if not check_tensor_health(images, "input_images"):
                    print(f"è·³è¿‡æ‰¹æ¬¡ {batch_idx}ï¼šè¾“å…¥å›¾åƒå¼‚å¸¸")
                    continue
                
                # å‰å‘ä¼ æ’­
                outputs = model(images, class_labels=labels)
                loss = outputs['total_loss']  # ä½¿ç”¨æ–°çš„æŸå¤±é”®å
                
                # ğŸ†• è°ƒæ•´æŸå¤±ä»¥é€‚åº”æ¢¯åº¦ç´¯ç§¯
                loss = loss / accumulation_steps
                
                # æ£€æŸ¥æŸå¤±
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âŒ æ‰¹æ¬¡ {batch_idx} æŸå¤±å¼‚å¸¸: {loss.item()}")
                    # å°è¯•ä½¿ç”¨å™ªå£°æŸå¤±ä½œä¸ºå¤‡é€‰
                    if 'noise_loss' in outputs:
                        loss = outputs['noise_loss'] / accumulation_steps
                        print(f"  ä½¿ç”¨å™ªå£°æŸå¤±: {loss.item():.4f}")
                    else:
                        print(f"è·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                        continue
                
                # é™åˆ¶æŸå¤±å€¼
                if loss.item() > 100:
                    print(f"âš ï¸ æŸå¤±è¿‡å¤§ ({loss.item():.2f})ï¼Œé™åˆ¶ä¸º100")
                    loss = torch.clamp(loss, max=100.0)
                
                # åå‘ä¼ æ’­ - ğŸ†• ç´¯ç§¯æ¢¯åº¦
                loss.backward()
                
                # ğŸ†• ç´¯ç§¯æŸå¤±å’Œè®¡æ•°
                accumulated_loss += loss.item()
                accumulation_count += 1
                
                # ğŸ†• åªåœ¨ç´¯ç§¯è¶³å¤Ÿæ­¥æ•°åè¿›è¡Œä¼˜åŒ–å™¨æ›´æ–°
                if accumulation_count >= accumulation_steps or batch_idx == len(train_loader) - 1:
                    # æ£€æŸ¥æ¢¯åº¦å¥åº·
                    total_norm = 0
                    gradient_healthy = True
                    for p in model.unet.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            if torch.isnan(param_norm) or torch.isinf(param_norm):
                                print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦ï¼Œè·³è¿‡æ›´æ–°")
                                gradient_healthy = False
                                break
                            total_norm += param_norm.item() ** 2
                    
                    if gradient_healthy:
                        total_norm = total_norm ** (1. / 2)
                        
                        # æ¢¯åº¦è£å‰ª
                        if config['training']['grad_clip_norm'] > 0:
                            torch.nn.utils.clip_grad_norm_(model.unet.parameters(), config['training']['grad_clip_norm'])
                        
                        # ğŸ†• æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
                        optimizer.step()
                        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
                        
                        # æ›´æ–°å­¦ä¹ ç‡
                        if scheduler is not None:
                            scheduler.step()
                        
                        # ğŸ†• è®°å½•ç´¯ç§¯æŸå¤±
                        avg_accumulated_loss = accumulated_loss
                        epoch_loss += avg_accumulated_loss
                        valid_batches += 1
                        global_step += 1
                        
                        # é‡ç½®ç´¯ç§¯å˜é‡
                        accumulated_loss = 0.0
                        accumulation_count = 0
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        current_lr = optimizer.param_groups[0]['lr']
                        progress_bar.set_postfix({
                            'loss': f'{avg_accumulated_loss:.4f}',
                            'lr': f'{current_lr:.2e}',
                            'grad_norm': f'{total_norm:.2f}',
                            'acc_step': f'{accumulation_count}/{accumulation_steps}'
                        })
                    else:
                        # æ¢¯åº¦å¼‚å¸¸æ—¶æ¸…é›¶æ¢¯åº¦
                        optimizer.zero_grad()
                        accumulated_loss = 0.0
                        accumulation_count = 0
                else:
                    # ğŸ†• æœªè¾¾åˆ°ç´¯ç§¯æ­¥æ•°æ—¶ï¼Œæ›´æ–°è¿›åº¦æ¡ä½†ä¸æ‰§è¡Œä¼˜åŒ–
                    current_lr = optimizer.param_groups[0]['lr']
                    progress_bar.set_postfix({
                        'acc_loss': f'{accumulated_loss:.4f}',
                        'lr': f'{current_lr:.2e}',
                        'acc_step': f'{accumulation_count}/{accumulation_steps}'
                    })
            
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                # ğŸ†• å‡ºé”™æ—¶ä¹Ÿè¦æ¸…é›¶ç´¯ç§¯çš„æ¢¯åº¦
                if accumulation_count > 0:
                    optimizer.zero_grad()
                    accumulated_loss = 0.0
                    accumulation_count = 0
                continue
        
        # ğŸ†• ç¡®ä¿epochç»“æŸæ—¶æ¸…ç†ä»»ä½•å‰©ä½™çš„æ¢¯åº¦
        if accumulation_count > 0:
            optimizer.zero_grad()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        if valid_batches > 0:
            avg_epoch_loss = epoch_loss / valid_batches
        else:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ‰¹æ¬¡ï¼")
            avg_epoch_loss = float('inf')
        
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\nEpoch {epoch+1}/{config['training']['epochs']} - Loss: {avg_epoch_loss:.4f} - Valid Batches: {valid_batches}/{len(train_loader)//accumulation_steps} - Time: {epoch_time:.1f}s - LR: {current_lr:.2e}")
        
        # éªŒè¯
        avg_val_loss = None
        noise_metrics_epoch = {}
        if val_loader:
            model.eval()
            val_loss = 0.0
            val_steps = 0
            
            # ç”¨äºå™ªå£°æŒ‡æ ‡è®¡ç®—çš„ç´¯åŠ å™¨
            total_noise_metrics = {
                'noise_mse': 0.0,
                'noise_mae': 0.0,
                'noise_psnr': 0.0,
                'noise_cosine_similarity': 0.0,
                'noise_correlation': 0.0
            }
            
            with torch.no_grad():
                for images, labels in val_loader:
                    images = images.to(device)
                    labels = labels.to(device)
                    
                    outputs = model(images, class_labels=labels)
                    loss = outputs['total_loss']  # ä½¿ç”¨æ–°çš„æŸå¤±é”®å
                    
                    val_loss += loss.item()
                    
                    # è®¡ç®—å™ªå£°é¢„æµ‹æŒ‡æ ‡
                    if 'predicted_noise' in outputs and 'target_noise' in outputs:
                        batch_noise_metrics = diffusion_metrics.calculate_all_metrics(
                            outputs['predicted_noise'], 
                            outputs['target_noise']
                        )
                        
                        # ç´¯åŠ æŒ‡æ ‡
                        for key in total_noise_metrics.keys():
                            if key in batch_noise_metrics:
                                total_noise_metrics[key] += batch_noise_metrics[key]
                    
                    val_steps += 1
            
            avg_val_loss = val_loss / val_steps
            
            # è®¡ç®—å¹³å‡å™ªå£°æŒ‡æ ‡
            if val_steps > 0:
                for key in total_noise_metrics.keys():
                    noise_metrics_epoch[key] = total_noise_metrics[key] / val_steps
            
            # æ•´åˆéªŒè¯ç»“æœåˆ°ä¸€è¡Œ
            if noise_metrics_epoch:
                noise_mse = noise_metrics_epoch.get('noise_mse', 0)
                noise_psnr = noise_metrics_epoch.get('noise_psnr', 0)
                print(f"Val Loss: {avg_val_loss:.4f} | Noise MSE: {noise_mse:.6f} | PSNR: {noise_psnr:.2f}")
            else:
                print(f"Val Loss: {avg_val_loss:.4f}")
        else:
            avg_val_loss = avg_epoch_loss
        
        # FIDè¯„ä¼° - ğŸ†• ä½¿ç”¨é…ç½®çš„è¯„ä¼°é—´éš”
        fid_score = None
        inception_score = None
        eval_interval = config['fid_evaluation'].get('eval_interval', 20)  # é»˜è®¤20ä¸ªepoch
        if fid_evaluator and (epoch + 1) % eval_interval == 0:
            print(f"è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
            try:
                fid_start_time = time.time()
                
                # ğŸ†• ä½¿ç”¨é…ç½®çš„FIDè¯„ä¼°å‚æ•°
                num_samples = config['fid_evaluation'].get('num_samples', 2048)
                batch_size = config['fid_evaluation'].get('batch_size', 3)
                num_inference_steps = config['inference'].get('num_inference_steps', 250)  # ğŸ”§ ä»é…ç½®è¯»å–æ¨ç†æ­¥æ•°
                guidance_scale = config['inference'].get('guidance_scale', 7.5)  # ğŸ”§ ä»é…ç½®è¯»å–å¼•å¯¼å¼ºåº¦
                
                fid_score = fid_evaluator.evaluate_model(
                    model, 
                    num_samples=num_samples,
                    batch_size=batch_size,
                    num_classes=config['unet']['num_classes'],
                    num_inference_steps=num_inference_steps,  # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¨ç†æ­¥æ•°
                    guidance_scale=guidance_scale  # ğŸ”§ ä½¿ç”¨é…ç½®çš„å¼•å¯¼å¼ºåº¦
                )
                fid_time = time.time() - fid_start_time
                
                # è®¡ç®—ISåˆ†æ•°ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
                is_start_time = time.time()
                
                # è®¡ç®—ISåˆ†æ•°
                print(f"  è®¡ç®—ISåˆ†æ•°...")
                is_start_time = time.time()
                
                # ç”Ÿæˆæ ·æœ¬ç”¨äºISè®¡ç®—
                model.eval()
                with torch.no_grad():
                    generated_samples = []
                    num_is_samples = min(200, val_dataset_len)  # ISåˆ†æ•°æ ·æœ¬æ•°
                    
                    for i in range(0, num_is_samples, 8):  # æ¯æ¬¡ç”Ÿæˆ8ä¸ªæ ·æœ¬
                        batch_size_is = min(8, num_is_samples - i)
                        # éšæœºé€‰æ‹©ç±»åˆ«
                        class_labels = torch.randint(0, config['unet']['num_classes'], 
                                                    (batch_size_is,), device=device)
                        
                        generated_batch = model.sample(
                            batch_size=batch_size_is,
                            class_labels=class_labels,
                            num_inference_steps=config['inference']['num_inference_steps'],  # ğŸ†• ä½¿ç”¨é…ç½®çš„æ¨ç†æ­¥æ•°
                            guidance_scale=config['inference']['guidance_scale'],
                        )
                        
                        # åå½’ä¸€åŒ–åˆ°[0,1]
                        generated_batch = denormalize_for_metrics(generated_batch)
                        generated_samples.append(generated_batch)
                    
                    if generated_samples:
                        all_generated = torch.cat(generated_samples, dim=0)
                        is_mean, is_std = diffusion_metrics.inception_calculator.calculate_inception_score(
                            all_generated, splits=5
                        )
                        inception_score = {'mean': is_mean, 'std': is_std}
                        
                        is_time = time.time() - is_start_time
                        total_eval_time = fid_time + is_time
                        print(f"FID: {fid_score:.2f} | IS: {is_mean:.2f}Â±{is_std:.2f} | Time: {total_eval_time:.1f}s")
                
                # æ›´æ–°æœ€ä½³FID
                if fid_score < best_fid:
                    best_fid = fid_score
                    best_fid_model_path = os.path.join(save_dir, 'best_fid_model')
                    save_model_checkpoint(
                        model, 
                        best_fid_model_path, 
                        epoch=epoch+1, 
                        extra_info={'fid_score': fid_score, 'best_fid': True}
                    )
                    print(f"ğŸ‰ æ–°çºªå½•ï¼FID: {best_fid:.2f}")
                
            except Exception as e:
                print(f"âš ï¸ æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        # è®°å½•è®­ç»ƒæ—¥å¿—
        training_log['epochs'].append(epoch + 1)
        training_log['train_loss'].append(avg_epoch_loss)
        training_log['val_loss'].append(avg_val_loss)
        training_log['fid_scores'].append(fid_score)
        training_log['inception_scores'].append(inception_score)
        training_log['noise_metrics'].append(noise_metrics_epoch)
        training_log['learning_rates'].append(current_lr)
        
        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        with open(os.path.join(log_dir, 'training_log.json'), 'w') as f:
            json.dump(training_log, f, indent=2)
        
        # ä¿å­˜æœ€ä½³æŸå¤±æ¨¡å‹
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            best_model_path = os.path.join(save_dir, 'best_loss_model')
            save_model_checkpoint(
                model, 
                best_model_path, 
                epoch=epoch+1, 
                extra_info={'val_loss': best_loss, 'best_loss': True}
            )
            print(f"ğŸ‰ æ–°æœ€ä½³æŸå¤±: {best_loss:.4f}")
        
        # å®šæœŸä¿å­˜checkpoint
        if (epoch + 1) % config['training']['save_interval'] == 0:
            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}')
            save_model_checkpoint(
                model, 
                checkpoint_path, 
                epoch=epoch+1, 
                extra_info={'train_loss': avg_epoch_loss, 'val_loss': avg_val_loss}
            )
            print(f"ğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path}")
        
        # ç”Ÿæˆæ ·æœ¬å›¾åƒ
        if (epoch + 1) % config['training']['sample_interval'] == 0:
            print("  ç”Ÿæˆæ ·æœ¬å›¾åƒ...")
            save_sample_images(model, device, config, epoch + 1, sample_dir)
        
        print("-" * 80)
        
        # å¦‚æœæŸå¤±è¿ç»­å¼‚å¸¸ï¼Œæå‰åœæ­¢
        if math.isinf(avg_epoch_loss):
            print("âŒ è®­ç»ƒæŸå¤±å¼‚å¸¸ï¼Œå»ºè®®æ£€æŸ¥é…ç½®åé‡æ–°å¼€å§‹")
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(save_dir, 'final_model')
    save_model_checkpoint(
        model, 
        final_model_path, 
        epoch=config['training']['epochs'], 
        extra_info={
            'final_train_loss': avg_epoch_loss,
            'final_val_loss': avg_val_loss,
            'best_fid': best_fid if best_fid != float('inf') else None,
            'training_completed': True
        }
    )
    
    # ä¿å­˜æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š
    final_report = {
        'training_completed': True,
        'total_epochs': config['training']['epochs'],
        'best_train_loss': best_loss,
        'best_fid_score': best_fid if best_fid != float('inf') else None,
        'final_train_loss': avg_epoch_loss,
        'final_val_loss': avg_val_loss,
        'total_parameters': sum(p.numel() for p in model.unet.parameters()),
        'device': str(device),
        'config': config,
        'effective_batch_size': effective_batch_size,  # ğŸ†• è®°å½•æœ‰æ•ˆbatch size
        'gradient_accumulation_steps': accumulation_steps  # ğŸ†• è®°å½•æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    }
    
    with open(os.path.join(save_dir, 'training_report.json'), 'w') as f:
        json.dump(final_report, f, indent=2)
    
    print("=" * 80)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"  æœ€ç»ˆæ¨¡å‹: {final_model_path}")
    print(f"  æœ€ä½³æŸå¤±: {best_loss:.4f}")
    if best_fid != float('inf'):
        print(f"  æœ€ä½³FID: {best_fid:.2f}")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    print(f"  è®­ç»ƒæ—¥å¿—: {os.path.join(log_dir, 'training_log.json')}")
    print("=" * 80)

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # å¼€å§‹è®­ç»ƒ
    train_ldm() 
