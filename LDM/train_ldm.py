"""
Train LDM - åŸºäºCompVis/latent-diffusion
ä½¿ç”¨å¾®è°ƒåçš„AutoencoderKLè®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹
"""
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import yaml
import json
from typing import Dict, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# FIDè¯„ä¼°ç›¸å…³å¯¼å…¥
from torchvision import models
from scipy import linalg
from torchvision.transforms.functional import resize, to_tensor
from pathlib import Path
import shutil

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from ldm_model import create_ldm_model, LatentDiffusionModel
# ä¿®æ”¹æ•°æ®åŠ è½½å™¨å¯¼å…¥
try:
    # å°è¯•å¯¼å…¥VAEçš„æ•°æ®åŠ è½½å™¨
    sys.path.append('../VAE')
    from dataset import build_dataloader as vae_build_dataloader
    print("âœ… æˆåŠŸå¯¼å…¥VAEæ•°æ®åŠ è½½å™¨")
except ImportError:
    print("âš ï¸  VAEæ•°æ®åŠ è½½å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åˆæˆæ•°æ®")
    vae_build_dataloader = None

# äº‘æœåŠ¡å™¨ç¯å¢ƒè®¾ç½®
print("ğŸŒ LDMè®­ç»ƒç¯å¢ƒåˆå§‹åŒ–...")
print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# æ£€æŸ¥CUDA
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸš€ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    device = 'cuda'
else:
    print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    device = 'cpu'

# å†…å­˜ä¼˜åŒ–
torch.backends.cudnn.benchmark = True
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'


class FIDEvaluator:
    """FIDè¯„ä¼°å™¨ï¼Œç”¨äºè®¡ç®—ç”Ÿæˆå›¾åƒçš„FIDåˆ†æ•°"""
    
    def __init__(self, device='cuda', inception_batch_size=32):  # å‡å°‘é»˜è®¤æ‰¹æ¬¡å¤§å°
        self.device = device
        self.inception_batch_size = inception_batch_size
        
        # Inceptionæ¨¡å‹å°†åœ¨éœ€è¦æ—¶åŠ è½½ï¼Œç”¨å®Œå°±å¸è½½
        self.inception = None
        self.preprocess = None
        
        print("âœ… FIDè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼ˆå»¶è¿ŸåŠ è½½Inceptionæ¨¡å‹ï¼‰")
    
    def _load_inception_model(self):
        """å»¶è¿ŸåŠ è½½Inceptionæ¨¡å‹"""
        if self.inception is None:
            print("ğŸ”„ åŠ è½½Inception-v3æ¨¡å‹...")
            
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            
            # åŠ è½½é¢„è®­ç»ƒçš„Inception-v3æ¨¡å‹
            self.inception = models.inception_v3(pretrained=True, transform_input=False)
            self.inception.fc = nn.Identity()  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            self.inception.eval()
            self.inception.to(self.device)
            
            # å›¾åƒé¢„å¤„ç†ï¼ˆç”¨äºInception-v3ï¼‰
            self.preprocess = transforms.Compose([
                transforms.Resize((299, 299)),  # Inception-v3éœ€è¦299x299è¾“å…¥
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            print("âœ… Inception-v3æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _unload_inception_model(self):
        """å¸è½½Inceptionæ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self.inception is not None:
            print("ğŸ—‘ï¸  å¸è½½Inception-v3æ¨¡å‹...")
            self.inception = None
            self.preprocess = None
            torch.cuda.empty_cache()
            print("âœ… Inception-v3æ¨¡å‹å·²å¸è½½")
    
    def _check_memory(self, stage=""):
        """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ’¾ {stage} GPUå†…å­˜: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
    
    def extract_features(self, images):
        """
        æå–å›¾åƒç‰¹å¾
        è¾“å…¥: [N, 3, H, W] çš„RGBå›¾åƒï¼ŒèŒƒå›´[0, 1]
        """
        self._load_inception_model()
        features = []
        
        # ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜é—®é¢˜
        effective_batch_size = min(self.inception_batch_size, 16)  # æœ€å¤§16
        
        with torch.no_grad():
            for i in range(0, len(images), effective_batch_size):
                batch = images[i:i + effective_batch_size]
                
                # ç¡®ä¿æ˜¯3é€šé“RGBå›¾åƒï¼ŒèŒƒå›´[0, 1]
                if batch.size(1) != 3:
                    raise ValueError(f"æœŸæœ›3é€šé“RGBå›¾åƒï¼Œå¾—åˆ°{batch.size(1)}é€šé“")
                
                # æ£€æŸ¥æ•°å€¼èŒƒå›´
                if batch.min() < -0.1 or batch.max() > 1.1:
                    print(f"âš ï¸  å›¾åƒåƒç´ èŒƒå›´å¼‚å¸¸: [{batch.min():.3f}, {batch.max():.3f}]")
                
                # è£å‰ªåˆ°[0, 1]èŒƒå›´
                batch = torch.clamp(batch, 0.0, 1.0)
                
                # è°ƒæ•´å¤§å°å¹¶æ ‡å‡†åŒ–
                batch_processed = []
                for img in batch:
                    img_resized = resize(img, (299, 299))
                    img_normalized = self.preprocess(img_resized)
                    batch_processed.append(img_normalized)
                
                batch_processed = torch.stack(batch_processed).to(self.device)
                
                # æå–ç‰¹å¾
                feat = self.inception(batch_processed)
                features.append(feat.cpu())
                
                # åŠæ—¶æ¸…ç†ä¸­é—´ç»“æœ
                del batch_processed, feat
                
                # æ¯å¤„ç†å‡ ä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡å†…å­˜
                if (i // effective_batch_size) % 5 == 0:
                    torch.cuda.empty_cache()
        
        return torch.cat(features, dim=0)
    
    def calculate_fid(self, real_features, fake_features):
        """è®¡ç®—FIDåˆ†æ•°"""
        # è½¬æ¢ä¸ºnumpy
        real_features = real_features.numpy()
        fake_features = fake_features.numpy()
        
        # è®¡ç®—å‡å€¼å’Œåæ–¹å·®
        mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
        mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)
        
        # è®¡ç®—FID
        diff = mu1 - mu2
        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
        return fid
    
    def evaluate_fid(self, ldm_model, real_loader, vae_model, config):
        """
        è¯„ä¼°FIDåˆ†æ•°
        
        Args:
            ldm_model: æ½œåœ¨æ‰©æ•£æ¨¡å‹
            real_loader: çœŸå®å›¾åƒæ•°æ®åŠ è½½å™¨
            vae_model: VAEæ¨¡å‹ï¼ˆå®é™…ä¸Šä¸éœ€è¦ï¼Œå› ä¸ºLDM.sample()å·²ç»åŒ…å«è§£ç ï¼‰
            config: é…ç½®å­—å…¸
        """
        self._check_memory("FIDè¯„ä¼°å¼€å§‹")
        
        fid_config = config.get('evaluation', {}).get('fid_evaluation', {})
        num_samples = fid_config.get('num_samples', 500)  # å‡å°‘é»˜è®¤æ ·æœ¬æ•°
        batch_size = fid_config.get('batch_size', 16)  # å‡å°‘æ‰¹æ¬¡å¤§å°
        num_inference_steps = fid_config.get('num_inference_steps', 50)
        num_classes = config.get('unet', {}).get('num_classes', 31)
        
        # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´å‚æ•°
        available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated() / 1024**3
        free_memory = available_memory - allocated_memory
        
        if free_memory < 3:  # å°äº3GBå¯ç”¨å†…å­˜
            num_samples = min(num_samples, 200)  # è¿›ä¸€æ­¥å‡å°‘æ ·æœ¬æ•°
            batch_size = min(batch_size, 8)    # è¿›ä¸€æ­¥å‡å°‘æ‰¹æ¬¡å¤§å°
            print(f"âš ï¸  å¯ç”¨æ˜¾å­˜ä¸è¶³ ({free_memory:.1f}GB)ï¼Œè°ƒæ•´å‚æ•°ï¼šsamples={num_samples}, batch_size={batch_size}")
        
        ldm_model.eval()
        
        try:
            # æ”¶é›†çœŸå®å›¾åƒï¼ˆRGBï¼ŒèŒƒå›´[0,1]ï¼‰
            print(f"ğŸ” æ”¶é›†çœŸå®å›¾åƒ ({num_samples} å¼ )...")
            real_images = []
            for batch_idx, batch in enumerate(real_loader):
                if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                    images = batch[0]  # [B, 3, H, W], èŒƒå›´[-1, 1]
                else:
                    images = batch
                
                # è½¬æ¢åˆ°[0, 1]èŒƒå›´
                images = (images + 1.0) / 2.0
                images = torch.clamp(images, 0.0, 1.0)
                
                real_images.append(images.cpu())  # ç«‹å³ç§»åˆ°CPU
                if len(real_images) * images.size(0) >= num_samples:
                    break
            
            real_images = torch.cat(real_images, dim=0)[:num_samples]
            self._check_memory("çœŸå®å›¾åƒæ”¶é›†å®Œæˆ")
            
            # ç”Ÿæˆå‡å›¾åƒ
            print(f"ğŸ¨ ç”Ÿæˆå‡å›¾åƒ ({num_samples} å¼ )...")
            fake_images = []
            
            # ç¦ç”¨é‡‡æ ·è¿‡ç¨‹ä¸­çš„è¿›åº¦æ¡
            old_tqdm_disable = os.environ.get('TQDM_DISABLE', '0')
            os.environ['TQDM_DISABLE'] = '1'
            
            try:
                with torch.no_grad():
                    num_batches = (num_samples + batch_size - 1) // batch_size
                    
                    # ç®€åŒ–è¿›åº¦æ¡æ˜¾ç¤ºï¼Œä¸æ˜¾ç¤ºå…·ä½“æè¿°åªæ˜¾ç¤ºæ€»ä½“è¿›åº¦
                    for i in range(num_batches):
                        current_batch_size = min(batch_size, num_samples - i * batch_size)
                        
                        # éšæœºç”Ÿæˆç±»åˆ«æ ‡ç­¾
                        if num_classes > 1:
                            class_labels = torch.randint(0, num_classes, (current_batch_size,), device=self.device)
                        else:
                            class_labels = None
                        
                        # LDMç”ŸæˆRGBå›¾åƒï¼ˆå·²ç»åŒ…å«äº†æ½œå˜é‡é‡‡æ ·å’ŒVAEè§£ç ï¼‰
                        generated_images = ldm_model.sample(
                            num_samples=current_batch_size,
                            class_labels=class_labels,
                            num_inference_steps=num_inference_steps,
                            eta=0.0
                        )  # è¿”å› [B, 3, H, W]ï¼ŒèŒƒå›´[-1, 1]
                        
                        # è½¬æ¢åˆ°[0, 1]èŒƒå›´
                        generated_images = (generated_images + 1.0) / 2.0
                        generated_images = torch.clamp(generated_images, 0.0, 1.0)
                        
                        fake_images.append(generated_images.cpu())  # ç«‹å³ç§»åˆ°CPU
                        
                        # ç®€åŒ–è¿›åº¦æ˜¾ç¤º
                        if (i + 1) % max(1, num_batches // 4) == 0 or i == num_batches - 1:
                            print(f"   ğŸ“Š å·²ç”Ÿæˆ {min((i + 1) * batch_size, num_samples)}/{num_samples} å¼ å›¾åƒ")
                        
                        # æ¸…ç†æ˜¾å­˜
                        del generated_images
                        if class_labels is not None:
                            del class_labels
                        torch.cuda.empty_cache()
            finally:
                # æ¢å¤tqdmè®¾ç½®
                os.environ['TQDM_DISABLE'] = old_tqdm_disable
            
            fake_images = torch.cat(fake_images, dim=0)[:num_samples]
            self._check_memory("å‡å›¾åƒç”Ÿæˆå®Œæˆ")
            
            # æå–ç‰¹å¾
            print(f"ğŸ§  æå–çœŸå®å›¾åƒç‰¹å¾...")
            real_features = self.extract_features(real_images)
            
            # æ¸…ç†çœŸå®å›¾åƒå†…å­˜
            del real_images
            torch.cuda.empty_cache()
            self._check_memory("çœŸå®å›¾åƒç‰¹å¾æå–å®Œæˆ")
            
            print(f"ğŸ§  æå–ç”Ÿæˆå›¾åƒç‰¹å¾...")
            fake_features = self.extract_features(fake_images)
            sample_images = fake_images[:8].clone()  # åªä¿ç•™8å¼ ç”¨äºå¯è§†åŒ–
            
            # æ¸…ç†å‡å›¾åƒå†…å­˜
            del fake_images
            torch.cuda.empty_cache()
            
            # å¸è½½Inceptionæ¨¡å‹
            self._unload_inception_model()
            self._check_memory("Inceptionæ¨¡å‹å¸è½½å®Œæˆ")
            
            # è®¡ç®—FID
            fid_score = self.calculate_fid(real_features, fake_features)
            
            return fid_score, sample_images
            
        except Exception as e:
            # å‡ºé”™æ—¶ç¡®ä¿æ¸…ç†èµ„æº
            print(f"âŒ FIDè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self._unload_inception_model()
            
            # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰å¯èƒ½çš„å˜é‡
            try:
                if 'real_images' in locals():
                    del real_images
                if 'fake_images' in locals():
                    del fake_images
                if 'real_features' in locals():
                    del real_features
                if 'fake_features' in locals():
                    del fake_features
            except:
                pass
            
            torch.cuda.empty_cache()
            self._check_memory("é”™è¯¯æ¸…ç†å®Œæˆ")
            raise e


class DataLoaderWrapper:
    """æ•°æ®åŠ è½½å™¨åŒ…è£…å™¨ï¼Œå°†VAEçš„(image, label)æ ¼å¼é€‚é…ä¸ºLDMéœ€è¦çš„æ ¼å¼"""
    
    def __init__(self, original_loader):
        self.original_loader = original_loader
        self.batch_size = original_loader.batch_size
        self.dataset = original_loader.dataset
    
    def __iter__(self):
        for batch in self.original_loader:
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images, labels = batch[0], batch[1]
                yield images, labels
            else:
                raise ValueError(f"æ— æ³•å¤„ç†çš„æ‰¹æ¬¡æ ¼å¼: {type(batch)}")
    
    def __len__(self):
        return len(self.original_loader)


class LDMTrainer:
    """LDMè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        config: Dict[str, Any],
        vae_checkpoint_path: str = None,
        device: str = 'cuda'
    ):
        self.config = config
        self.device = device
        self.vae_checkpoint_path = vae_checkpoint_path
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = config.get('output_dir', './ldm_outputs')
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'checkpoints'), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'samples'), exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_optimizer()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self._init_dataloader()
        
        # åˆå§‹åŒ–FIDè¯„ä¼°å™¨
        fid_config = self.config.get('evaluation', {}).get('fid_evaluation', {})
        inception_batch_size = fid_config.get('inception_batch_size', 64)
        self.fid_evaluator = FIDEvaluator(device=self.device, inception_batch_size=inception_batch_size)
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'val_loss': [],
            'fid_score': [],
            'lr': []
        }
        
        # æœ€ä½³éªŒè¯æŸå¤±å’ŒFID
        self.best_val_loss = float('inf')
        self.best_fid_score = float('inf')
        
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("ğŸ”§ åˆå§‹åŒ–LDMæ¨¡å‹...")
        
        # è·å–é…ç½®
        unet_config = self.config.get('unet', {})
        diffusion_config = self.config.get('diffusion', {})
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_ldm_model(
            vae_checkpoint_path=self.vae_checkpoint_path,
            unet_config=unet_config,
            diffusion_config=diffusion_config,
            device=self.device
        )
        
        # ä¿å­˜VAEæ¨¡å‹å¼•ç”¨ï¼Œç”¨äºFIDè¯„ä¼°æ—¶çš„è§£ç 
        self.vae_model = self.model.vae
        
        print(f"âœ… LDMæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   è®­ç»ƒæ¯”ä¾‹: {trainable_params / total_params * 100:.1f}%")
        
    def _init_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        train_config = self.config.get('training', {})
        
        # ä¼˜åŒ–å™¨
        lr = train_config.get('learning_rate', 1e-4)
        weight_decay = train_config.get('weight_decay', 0.01)
        
        self.optimizer = self.model.configure_optimizers(lr=lr, weight_decay=weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_type = train_config.get('scheduler', 'cosine')
        if scheduler_type == 'cosine':
            # ç¡®ä¿eta_minæ˜¯æµ®ç‚¹æ•°ç±»å‹
            min_lr = train_config.get('min_lr', 1e-6)
            if isinstance(min_lr, str):
                min_lr = float(min_lr)
            
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=train_config.get('epochs', 100),
                eta_min=min_lr
            )
        elif scheduler_type == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=train_config.get('step_size', 30),
                gamma=train_config.get('gamma', 0.1)
            )
        else:
            self.scheduler = None
            
        print(f"ğŸ“ˆ ä¼˜åŒ–å™¨: AdamW (lr={lr}, weight_decay={weight_decay})")
        if self.scheduler:
            print(f"ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")
        
    def _init_dataloader(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        data_config = self.config.get('data', {})
        
        # æ•°æ®è·¯å¾„
        data_dir = data_config.get('data_dir', '/kaggle/input/dataset/dataset')
        
        # æ•°æ®å¢å¼º - ä¿æŒä¸VAEè®­ç»ƒä¸€è‡´çš„å½’ä¸€åŒ–
        train_transforms = transforms.Compose([
            transforms.Resize((256, 256)),  # åŒ¹é…VAEè®­ç»ƒæ—¶çš„å°ºå¯¸
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]
        ])
        
        val_transforms = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        # æ‰¹æ¬¡å¤§å°
        batch_size = data_config.get('batch_size', 8)
        num_workers = data_config.get('num_workers', 2)
        
        try:
            if vae_build_dataloader is not None:
                # ä½¿ç”¨VAEçš„æ•°æ®åŠ è½½å™¨
                print(f"ğŸ”„ ä½¿ç”¨VAEæ•°æ®åŠ è½½å™¨")
                train_loader, val_loader, train_size, val_size = vae_build_dataloader(
                    root_dir=data_dir,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    shuffle_train=True,
                    shuffle_val=False,
                    val_split=0.2,  # 20%éªŒè¯é›†
                    random_state=42
                )
                
                # åŒ…è£…æ•°æ®åŠ è½½å™¨ä»¥é€‚é…LDMæ ¼å¼
                self.train_loader = DataLoaderWrapper(train_loader)
                self.val_loader = DataLoaderWrapper(val_loader)
                
                print(f"âœ… VAEæ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
                print(f"   è®­ç»ƒé›†: {train_size} æ ·æœ¬, {len(self.train_loader)} æ‰¹æ¬¡")
                print(f"   éªŒè¯é›†: {val_size} æ ·æœ¬, {len(self.val_loader)} æ‰¹æ¬¡")
                print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            else:
                # å›é€€åˆ°åˆæˆæ•°æ®
                self._create_synthetic_dataloaders(batch_size, num_workers)
                
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
            print(f"ğŸ”„ å›é€€åˆ°åˆæˆæ•°æ®...")
            self._create_synthetic_dataloaders(batch_size, num_workers)
    
    def _create_synthetic_dataloaders(self, batch_size: int, num_workers: int):
        """åˆ›å»ºåˆæˆæ•°æ®åŠ è½½å™¨ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        from torch.utils.data import Dataset, DataLoader
        
        class SyntheticDataset(Dataset):
            def __init__(self, num_samples: int, num_classes: int):
                self.num_samples = num_samples
                self.num_classes = num_classes
            
            def __len__(self):
                return self.num_samples
            
            def __getitem__(self, idx):
                # ç”Ÿæˆéšæœºå›¾åƒï¼ŒèŒƒå›´[-1, 1]
                image = torch.randn(3, 256, 256)
                label = torch.randint(0, self.num_classes, (1,)).item()
                return image, label
        
        num_classes = self.config.get('unet', {}).get('num_classes', 31)
        
        train_dataset = SyntheticDataset(1000, num_classes)
        val_dataset = SyntheticDataset(200, num_classes)
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,  # åˆæˆæ•°æ®ä¸éœ€è¦å¤šè¿›ç¨‹
            pin_memory=True,
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True,
            drop_last=False
        )
        
        print(f"âœ… åˆæˆæ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, {len(self.train_loader)} æ‰¹æ¬¡")
        print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬, {len(self.val_loader)} æ‰¹æ¬¡")
        print(f"   ç±»åˆ«æ•°: {num_classes}")

    @torch.no_grad()
    def compute_vae_scaling_factor(self, num_samples: int = 1000) -> float:
        """
        è®¡ç®—å¾®è°ƒåVAEçš„æœ€ä½³ç¼©æ”¾å› å­
        
        Args:
            num_samples: ç”¨äºç»Ÿè®¡çš„æ ·æœ¬æ•°é‡
            
        Returns:
            optimal_scaling_factor: æœ€ä½³ç¼©æ”¾å› å­
        """
        print(f"ğŸ” è®¡ç®—VAEç¼©æ”¾å› å­ (ä½¿ç”¨ {num_samples} ä¸ªæ ·æœ¬)...")
        
        self.vae_model.eval()
        all_latents = []
        sample_count = 0
        
        # Get the VAE model to be used for encoding
        # It should be the one loaded by LDMTrainer, typically already on the correct device
        temp_vae = self.vae_model 
        temp_vae.eval() # Ensure it's in evaluation mode

        print(f"ğŸ” è®¡ç®—ç¼©æ”¾å› å­ï¼šä» {len(self.train_loader)} ä¸ªæ‰¹æ¬¡ä¸­å¤„ç†æœ€å¤š {num_samples} ä¸ªæ ·æœ¬...")
        with torch.no_grad(): # Crucial: no gradients needed here
            for batch_idx, batch_data in enumerate(self.train_loader):
                # Try to gracefully handle different batch structures
                if isinstance(batch_data, (list, tuple)) and len(batch_data) > 0:
                    images = batch_data[0]
                elif torch.is_tensor(batch_data):
                    images = batch_data
                else:
                    print(f"âš ï¸  è·³è¿‡æ— æ³•è§£æçš„æ‰¹æ¬¡æ•°æ®ç±»å‹: {type(batch_data)}")
                    continue
                
                images = images.to(self.device) # Move images to the correct device

                # Encode using the VAE
                # The VAE (AutoencoderKL from diffusers) encode method returns an object
                # with a `latent_dist` attribute, which is a DiagonalGaussianDistribution.
                posterior = temp_vae.encode(images).latent_dist
                
                # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¸ºäº†ä¸å®é™…ç¼–ç æ–¹å¼ä¿æŒä¸€è‡´ï¼Œè¿™é‡Œä¹Ÿä½¿ç”¨ .sample()
                # ä½†æ”¶é›†å¤šä¸ªæ ·æœ¬æ¥è·å¾—æ›´ç¨³å®šçš„ç»Ÿè®¡ä¼°è®¡
                latents = posterior.sample()  # ä½¿ç”¨ä¸encode_to_latentç›¸åŒçš„æ–¹æ³•
                
                # ğŸ“Š æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ¯”è¾ƒ mode å’Œ sample çš„å·®å¼‚
                if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ‰“å°
                    latents_mode = posterior.mode()
                    print(f"   è°ƒè¯•ä¿¡æ¯ - Mode vs Sample:")
                    print(f"     Mode:   èŒƒå›´[{latents_mode.min():.3f}, {latents_mode.max():.3f}], std={latents_mode.std():.3f}")
                    print(f"     Sample: èŒƒå›´[{latents.min():.3f}, {latents.max():.3f}], std={latents.std():.3f}")
                    
                    # æ£€æŸ¥æ–¹å·®(logvar)çš„å¤§å°ï¼Œè¿™èƒ½è§£é‡Šä¸ºä»€ä¹ˆsampleä¸modeå·®å¼‚å¾ˆå¤§
                    logvar = posterior.logvar
                    print(f"     LogVar: èŒƒå›´[{logvar.min():.3f}, {logvar.max():.3f}], mean={logvar.mean():.3f}")
                    print(f"     Var:    èŒƒå›´[{torch.exp(logvar).min():.3f}, {torch.exp(logvar).max():.3f}], mean={torch.exp(logvar).mean():.3f}")

                all_latents.append(latents.cpu()) # Move to CPU to save GPU memory
                sample_count += latents.shape[0]
                
                if sample_count >= num_samples:
                    print(f"   å·²æ”¶é›† {sample_count} ä¸ªæ½œå˜é‡æ ·æœ¬ï¼Œåœæ­¢æ”¶é›†ã€‚")
                    break
        
        if not all_latents:
            print("âš ï¸  æ— æ³•æ”¶é›†æ½œå˜é‡æ ·æœ¬ï¼Œä½¿ç”¨é»˜è®¤ç¼©æ”¾å› å­")
            return 0.18215
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬
        all_latents = torch.cat(all_latents, dim=0)[:num_samples]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean = all_latents.mean()
        std = all_latents.std()
        
        # æœ€ä½³ç¼©æ”¾å› å­ = 1 / std
        # ç›®æ ‡æ˜¯ä½¿ç¼©æ”¾åçš„æ½œå˜é‡æ ‡å‡†å·®æ¥è¿‘1
        optimal_scaling_factor = 1.0 / std.item()
        
        print(f"ğŸ“Š VAEæ½œå˜é‡ç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°é‡: {len(all_latents)}")
        print(f"   å‡å€¼: {mean:.6f}")
        print(f"   æ ‡å‡†å·®: {std:.6f}")
        print(f"   å»ºè®®ç¼©æ”¾å› å­: {optimal_scaling_factor:.6f}")
        print(f"   å½“å‰ç¼©æ”¾å› å­: {self.model.scaling_factor:.6f}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        current_factor = self.model.scaling_factor
        factor_diff = abs(optimal_scaling_factor - current_factor) / current_factor
        
        if factor_diff > 0.1:  # å·®å¼‚è¶…è¿‡10%
            print(f"âš ï¸  ç¼©æ”¾å› å­å·®å¼‚è¾ƒå¤§ ({factor_diff*100:.1f}%)ï¼Œå»ºè®®æ›´æ–°")
        else:
            print(f"âœ… ç¼©æ”¾å› å­å·®å¼‚è¾ƒå° ({factor_diff*100:.1f}%)ï¼Œå½“å‰è®¾ç½®åˆé€‚")
        
        return optimal_scaling_factor

    def train_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        # è·å–æ€»epochæ•°
        total_epochs = self.config.get('training', {}).get('epochs', 100)
        
        # ä»é…ç½®è¯»å–è¯¦ç»†ç›‘æ§è®¾ç½®
        monitoring_config = self.config.get('training', {}).get('detailed_monitoring', {})
        detailed_monitoring = (
            monitoring_config.get('enabled', True) and 
            epoch < monitoring_config.get('monitor_epochs', 10)
        )
        monitor_batches = monitoring_config.get('monitor_batches', 5)
        summary_interval = monitoring_config.get('summary_interval', 50)
        
        if detailed_monitoring:
            print(f"\nğŸ” ç¬¬ {epoch+1} è½®è¯¦ç»†ç›‘æ§:")
            latent_stats = {'min': [], 'max': [], 'mean': [], 'std': []}
            noise_stats = {'min': [], 'max': [], 'mean': [], 'std': []}
            pred_stats = {'min': [], 'max': [], 'mean': [], 'std': []}
            loss_components = []
            gradient_norms = []
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {epoch+1}/{total_epochs}",
            leave=False,
            ncols=80,  # é™åˆ¶è¿›åº¦æ¡å®½åº¦
            ascii=True  # ä½¿ç”¨ASCIIå­—ç¬¦ï¼Œé¿å…ç¼–ç é—®é¢˜
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # è§£åŒ…æ‰¹æ¬¡æ•°æ®
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images, class_labels = batch[0], batch[1]
            else:
                images = batch
                class_labels = None
            
            images = images.to(self.device)
            if class_labels is not None:
                class_labels = class_labels.to(self.device)
            
            # è¯¦ç»†ç›‘æ§ï¼šè¾“å…¥å›¾åƒç»Ÿè®¡
            if detailed_monitoring and batch_idx < monitor_batches:
                img_min, img_max = images.min().item(), images.max().item()
                img_mean, img_std = images.mean().item(), images.std().item()
                print(f"  æ‰¹æ¬¡ {batch_idx+1} - è¾“å…¥å›¾åƒ: èŒƒå›´[{img_min:.3f}, {img_max:.3f}], å‡å€¼{img_mean:.3f}, æ ‡å‡†å·®{img_std:.3f}")
            
            # å‰å‘ä¼ æ’­
            loss_dict = self.model(images, class_labels)
            loss = loss_dict['loss']
            
            # è¯¦ç»†ç›‘æ§ï¼šè·å–ä¸­é—´ç»“æœ
            if detailed_monitoring and batch_idx < monitor_batches:
                with torch.no_grad():
                    # è·å–æ½œå˜é‡
                    if hasattr(self.model, 'vae') and self.model.vae is not None:
                        try:
                            # VAEç¼–ç 
                            posterior = self.model.vae.encode(images)
                            latents = posterior.latent_dist.sample() * self.model.scaling_factor
                            
                            # ç»Ÿè®¡æ½œå˜é‡
                            lat_min, lat_max = latents.min().item(), latents.max().item()
                            lat_mean, lat_std = latents.mean().item(), latents.std().item()
                            latent_stats['min'].append(lat_min)
                            latent_stats['max'].append(lat_max)
                            latent_stats['mean'].append(lat_mean)
                            latent_stats['std'].append(lat_std)
                            
                            print(f"    æ½œå˜é‡(ç¼©æ”¾å): èŒƒå›´[{lat_min:.3f}, {lat_max:.3f}], å‡å€¼{lat_mean:.3f}, æ ‡å‡†å·®{lat_std:.3f}")
                            
                            # ç”Ÿæˆéšæœºå™ªéŸ³æ—¶é—´æ­¥
                            batch_size = latents.shape[0]
                            timesteps = torch.randint(0, self.model.diffusion.timesteps, (batch_size,), device=self.device)
                            
                            # æ·»åŠ å™ªéŸ³
                            noise = torch.randn_like(latents)
                            noisy_latents = self.model.diffusion.q_sample(latents, timesteps, noise)
                            
                            # ç»Ÿè®¡å™ªéŸ³å’ŒåŠ å™ªåçš„æ½œå˜é‡
                            noise_min, noise_max = noise.min().item(), noise.max().item()
                            noise_mean, noise_std = noise.mean().item(), noise.std().item()
                            noise_stats['min'].append(noise_min)
                            noise_stats['max'].append(noise_max)
                            noise_stats['mean'].append(noise_mean)
                            noise_stats['std'].append(noise_std)
                            
                            noisy_min, noisy_max = noisy_latents.min().item(), noisy_latents.max().item()
                            noisy_mean, noisy_std = noisy_latents.mean().item(), noisy_latents.std().item()
                            
                            print(f"    å™ªéŸ³: èŒƒå›´[{noise_min:.3f}, {noise_max:.3f}], å‡å€¼{noise_mean:.3f}, æ ‡å‡†å·®{noise_std:.3f}")
                            print(f"    åŠ å™ªæ½œå˜é‡: èŒƒå›´[{noisy_min:.3f}, {noisy_max:.3f}], å‡å€¼{noisy_mean:.3f}, æ ‡å‡†å·®{noisy_std:.3f}")
                            
                            # U-Neté¢„æµ‹
                            model_pred = self.model.unet(noisy_latents, timesteps, class_labels)
                            pred_min, pred_max = model_pred.min().item(), model_pred.max().item()
                            pred_mean, pred_std = model_pred.mean().item(), model_pred.std().item()
                            pred_stats['min'].append(pred_min)
                            pred_stats['max'].append(pred_max)
                            pred_stats['mean'].append(pred_mean)
                            pred_stats['std'].append(pred_std)
                            
                            print(f"    æ¨¡å‹é¢„æµ‹: èŒƒå›´[{pred_min:.3f}, {pred_max:.3f}], å‡å€¼{pred_mean:.3f}, æ ‡å‡†å·®{pred_std:.3f}")
                            
                        except Exception as e:
                            print(f"    âš ï¸ è¯¦ç»†ç›‘æ§å¤±è´¥: {e}")
            
            # è®°å½•æŸå¤±ç»„ä»¶
            if detailed_monitoring:
                loss_components.append(loss.item())
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # è¯¦ç»†ç›‘æ§ï¼šæ¢¯åº¦ç»Ÿè®¡
            if detailed_monitoring and batch_idx < monitor_batches:
                total_norm = 0
                param_count = 0
                for p in self.model.unet.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2)
                        total_norm += param_norm.item() ** 2
                        param_count += p.numel()
                total_norm = total_norm ** (1. / 2)
                gradient_norms.append(total_norm)
                print(f"    æ¢¯åº¦èŒƒæ•°: {total_norm:.6f}, å‚æ•°æ•°é‡: {param_count:,}")
            
            # æ¢¯åº¦è£å‰ª
            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.unet.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            avg_loss = total_loss / (batch_idx + 1)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg': f'{avg_loss:.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                'GradNorm': f'{grad_norm:.3f}' if isinstance(grad_norm, (int, float)) else 'N/A'
            })
            
            # æ¸…ç†æ˜¾å­˜
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()
            
            # è¯¦ç»†ç›‘æ§ï¼šå®šæœŸæ€»ç»“
            if detailed_monitoring and (batch_idx + 1) % summary_interval == 0:
                self._print_monitoring_summary(epoch, batch_idx, latent_stats, noise_stats, pred_stats, loss_components, gradient_norms)
        
        # è¯¦ç»†ç›‘æ§ï¼šepochç»“æŸæ€»ç»“
        if detailed_monitoring:
            self._print_epoch_monitoring_summary(epoch, latent_stats, noise_stats, pred_stats, loss_components, gradient_norms)
        
        return total_loss / num_batches
    
    def _print_monitoring_summary(self, epoch, batch_idx, latent_stats, noise_stats, pred_stats, loss_components, gradient_norms):
        """æ‰“å°ç›‘æ§æ•°æ®æ‘˜è¦"""
        print(f"\n  ğŸ“Š æ‰¹æ¬¡ 1-{batch_idx+1} ç»Ÿè®¡æ‘˜è¦:")
        
        if latent_stats['mean']:
            lat_mean_avg = np.mean(latent_stats['mean'])
            lat_std_avg = np.mean(latent_stats['std'])
            print(f"    æ½œå˜é‡: å¹³å‡å‡å€¼={lat_mean_avg:.4f}, å¹³å‡æ ‡å‡†å·®={lat_std_avg:.4f}")
        
        if noise_stats['mean']:
            noise_mean_avg = np.mean(noise_stats['mean'])
            noise_std_avg = np.mean(noise_stats['std'])
            print(f"    å™ªéŸ³: å¹³å‡å‡å€¼={noise_mean_avg:.4f}, å¹³å‡æ ‡å‡†å·®={noise_std_avg:.4f}")
        
        if pred_stats['mean']:
            pred_mean_avg = np.mean(pred_stats['mean'])
            pred_std_avg = np.mean(pred_stats['std'])
            print(f"    é¢„æµ‹: å¹³å‡å‡å€¼={pred_mean_avg:.4f}, å¹³å‡æ ‡å‡†å·®={pred_std_avg:.4f}")
        
        if loss_components:
            loss_avg = np.mean(loss_components)
            loss_std = np.std(loss_components)
            print(f"    æŸå¤±: å¹³å‡={loss_avg:.4f}, æ ‡å‡†å·®={loss_std:.4f}")
        
        if gradient_norms:
            grad_avg = np.mean(gradient_norms)
            grad_std = np.std(gradient_norms)
            print(f"    æ¢¯åº¦: å¹³å‡èŒƒæ•°={grad_avg:.6f}, æ ‡å‡†å·®={grad_std:.6f}")
    
    def _print_epoch_monitoring_summary(self, epoch, latent_stats, noise_stats, pred_stats, loss_components, gradient_norms):
        """æ‰“å°epochç›‘æ§æ€»ç»“"""
        print(f"\nğŸ¯ ç¬¬ {epoch+1} è½®å®Œæ•´ç»Ÿè®¡:")
        print(f"  ç¼©æ”¾å› å­: {self.model.scaling_factor:.6f}")
        
        if latent_stats['mean']:
            print(f"  æ½œå˜é‡ç»Ÿè®¡:")
            print(f"    å‡å€¼èŒƒå›´: [{min(latent_stats['mean']):.4f}, {max(latent_stats['mean']):.4f}]")
            print(f"    æ ‡å‡†å·®èŒƒå›´: [{min(latent_stats['std']):.4f}, {max(latent_stats['std']):.4f}]")
            print(f"    æ•°å€¼èŒƒå›´: [{min(latent_stats['min']):.4f}, {max(latent_stats['max']):.4f}]")
        
        if noise_stats['mean']:
            print(f"  å™ªéŸ³ç»Ÿè®¡:")
            print(f"    å‡å€¼èŒƒå›´: [{min(noise_stats['mean']):.4f}, {max(noise_stats['mean']):.4f}]")
            print(f"    æ ‡å‡†å·®èŒƒå›´: [{min(noise_stats['std']):.4f}, {max(noise_stats['std']):.4f}]")
        
        if pred_stats['mean']:
            print(f"  é¢„æµ‹ç»Ÿè®¡:")
            print(f"    å‡å€¼èŒƒå›´: [{min(pred_stats['mean']):.4f}, {max(pred_stats['mean']):.4f}]")
            print(f"    æ ‡å‡†å·®èŒƒå›´: [{min(pred_stats['std']):.4f}, {max(pred_stats['std']):.4f}]")
        
        if loss_components:
            print(f"  æŸå¤±ç»Ÿè®¡:")
            print(f"    æŸå¤±èŒƒå›´: [{min(loss_components):.4f}, {max(loss_components):.4f}]")
            print(f"    å¹³å‡æŸå¤±: {np.mean(loss_components):.4f}")
        
        if gradient_norms:
            print(f"  æ¢¯åº¦ç»Ÿè®¡:")
            print(f"    æ¢¯åº¦èŒƒæ•°èŒƒå›´: [{min(gradient_norms):.6f}, {max(gradient_norms):.6f}]")
            print(f"    å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(gradient_norms):.6f}")
        
        # å¥åº·æ£€æŸ¥
        print(f"  ğŸ” å¥åº·æ£€æŸ¥:")
        checks = []
        
        if latent_stats['std']:
            avg_latent_std = np.mean(latent_stats['std'])
            if 0.8 <= avg_latent_std <= 1.2:
                checks.append("âœ… æ½œå˜é‡æ ‡å‡†å·®æ­£å¸¸")
            else:
                checks.append(f"âš ï¸ æ½œå˜é‡æ ‡å‡†å·®å¼‚å¸¸: {avg_latent_std:.3f} (æœŸæœ›: ~1.0)")
        
        if noise_stats['std']:
            avg_noise_std = np.mean(noise_stats['std'])
            if 0.9 <= avg_noise_std <= 1.1:
                checks.append("âœ… å™ªéŸ³æ ‡å‡†å·®æ­£å¸¸")
            else:
                checks.append(f"âš ï¸ å™ªéŸ³æ ‡å‡†å·®å¼‚å¸¸: {avg_noise_std:.3f} (æœŸæœ›: ~1.0)")
        
        if loss_components:
            latest_loss = loss_components[-5:]  # æœ€å5ä¸ªæ‰¹æ¬¡
            avg_recent_loss = np.mean(latest_loss)
            if avg_recent_loss < 1.0:
                checks.append("âœ… æŸå¤±å€¼åˆç†")
            elif avg_recent_loss > 5.0:
                checks.append(f"âš ï¸ æŸå¤±å€¼è¿‡é«˜: {avg_recent_loss:.3f}")
            else:
                checks.append(f"ğŸ“Š æŸå¤±å€¼: {avg_recent_loss:.3f}")
        
        if gradient_norms:
            avg_grad_norm = np.mean(gradient_norms)
            if avg_grad_norm < 0.001:
                checks.append("âš ï¸ æ¢¯åº¦å¯èƒ½è¿‡å°")
            elif avg_grad_norm > 10.0:
                checks.append("âš ï¸ æ¢¯åº¦å¯èƒ½è¿‡å¤§")
            else:
                checks.append("âœ… æ¢¯åº¦èŒƒæ•°æ­£å¸¸")
        
        for check in checks:
            print(f"    {check}")
        
        print("=" * 60)
    
    @torch.no_grad()
    def validate_epoch(self, epoch: int) -> float:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        num_batches = len(self.val_loader)
        
        # è·å–æ€»epochæ•°
        total_epochs = self.config.get('training', {}).get('epochs', 100)
        
        progress_bar = tqdm(
            self.val_loader,
            desc=f"Validation {epoch+1}/{total_epochs}",
            leave=False,
            ncols=80,
            ascii=True
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # è§£åŒ…æ‰¹æ¬¡æ•°æ®
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images, class_labels = batch[0], batch[1]
            else:
                images = batch
                class_labels = None
                
            images = images.to(self.device)
            if class_labels is not None:
                class_labels = class_labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            loss_dict = self.model(images, class_labels)
            loss = loss_dict['loss']
            
            total_loss += loss.item()
            avg_loss = total_loss / (batch_idx + 1)
            
            progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})
        
        return total_loss / num_batches
    
    @torch.no_grad()
    def evaluate_fid(self, epoch: int) -> float:
        """è¯„ä¼°FIDåˆ†æ•°"""
        print(f"ğŸ¯ å¼€å§‹FIDè¯„ä¼° (Epoch {epoch + 1})...")
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        output_dir_path = Path(self.output_dir)
        try:
            total, used, free = shutil.disk_usage(output_dir_path)
            free_gb = free / (1024**3)
            print(f"ğŸ’¾ å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.1f}GB")
            
            if free_gb < 5:  # å°äº5GBå¯ç”¨ç©ºé—´
                print(f"âŒ ç£ç›˜ç©ºé—´ä¸è¶³ ({free_gb:.1f}GB)ï¼Œè·³è¿‡FIDè¯„ä¼°")
                return float('inf')
        except Exception as e:
            print(f"âš ï¸  æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}")
        
        # é¢„æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
        if torch.cuda.is_available():
            available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated() / 1024**3
            free_memory = available_memory - allocated_memory
            
            if free_memory < 1.5:  # å°äº1.5GBå¯ç”¨å†…å­˜
                print(f"âš ï¸  å¯ç”¨æ˜¾å­˜ä¸è¶³ ({free_memory:.1f}GB < 1.5GB)ï¼Œè·³è¿‡FIDè¯„ä¼°")
                return float('inf')
        
        try:
            fid_score, sample_images = self.fid_evaluator.evaluate_fid(
                ldm_model=self.model,
                real_loader=self.val_loader,
                vae_model=None,  # ä¸å†éœ€è¦ï¼Œå› ä¸ºmodel.sample()å·²ç»åŒ…å«è§£ç 
                config=self.config
            )
            
            print(f"ğŸ“Š FID Score: {fid_score:.2f}")
            
            # ä¿å­˜FIDè¯„ä¼°çš„æ ·æœ¬å›¾åƒ
            if len(sample_images) > 0:
                sample_images = torch.clamp(sample_images, 0.0, 1.0)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´
                
                fig, axes = plt.subplots(2, 4, figsize=(12, 6))
                axes = axes.flatten()
                
                for i in range(min(8, len(sample_images))):
                    img = sample_images[i].permute(1, 2, 0).cpu().numpy()
                    axes[i].imshow(img)
                    axes[i].axis('off')
                    axes[i].set_title(f'Generated')
                
                plt.tight_layout()
                fid_sample_path = os.path.join(self.output_dir, 'samples', f'fid_epoch_{epoch+1:03d}.png')
                plt.savefig(fid_sample_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                print(f"ğŸ“¸ FIDæ ·æœ¬å›¾åƒå·²ä¿å­˜: {fid_sample_path}")
            
            return fid_score
            
        except Exception as e:
            print(f"âŒ FIDè¯„ä¼°å¤±è´¥: {e}")
            # å¼ºåˆ¶æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
            return float('inf')
    
    @torch.no_grad()
    def generate_samples(self, epoch: int, num_samples: int = 8):
        """ç”Ÿæˆæ ·æœ¬å›¾åƒ"""
        self.model.eval()
        
        # ä»é…ç½®è¯»å–å‚æ•°
        sample_config = self.config.get('evaluation', {}).get('sample_generation', {})
        inference_steps = sample_config.get('inference_steps', 50)
        
        # éšæœºé€‰æ‹©ç±»åˆ«æ ‡ç­¾
        if self.model.unet.num_classes and self.model.unet.num_classes > 1:
            class_labels = torch.randint(0, self.model.unet.num_classes, (num_samples,), device=self.device)
        else:
            class_labels = None
        
        print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆ {num_samples} å¼ æ ·æœ¬å›¾åƒ...")
        
        # ä¸´æ—¶ç¦ç”¨é‡‡æ ·è¿‡ç¨‹çš„è¿›åº¦æ¡
        old_tqdm_disable = os.environ.get('TQDM_DISABLE', '0')
        os.environ['TQDM_DISABLE'] = '1'  # ç¦ç”¨æ‰€æœ‰tqdmè¿›åº¦æ¡
        
        try:
            generated_images = self.model.sample(
                num_samples=num_samples,
                class_labels=class_labels,
                num_inference_steps=inference_steps,
                eta=0.0  # DDIMç¡®å®šæ€§é‡‡æ ·
            )
        finally:
            # æ¢å¤tqdmè®¾ç½®
            os.environ['TQDM_DISABLE'] = old_tqdm_disable
        
        print(f"âœ… æ ·æœ¬ç”Ÿæˆå®Œæˆ")
        
        # è½¬æ¢åˆ°[0, 1]èŒƒå›´
        generated_images = (generated_images + 1.0) / 2.0
        generated_images = torch.clamp(generated_images, 0.0, 1.0)
        
        # ä¿å­˜å›¾åƒ - ä½¿ç”¨2x4å¸ƒå±€
        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        axes = axes.flatten()
        
        for i in range(num_samples):
            img = generated_images[i].cpu().permute(1, 2, 0).numpy()
            axes[i].imshow(img)
            axes[i].axis('off')
            if class_labels is not None:
                axes[i].set_title(f'ç±»åˆ« {class_labels[i].item()}', fontsize=10)
        
        plt.tight_layout()
        sample_path = os.path.join(self.output_dir, 'samples', f'epoch_{epoch+1:03d}.png')
        plt.savefig(sample_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“¸ æ ·æœ¬å›¾åƒå·²ä¿å­˜: {sample_path}")
    
    def save_checkpoint(self, epoch: int, is_best: bool = False, is_best_fid: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        output_dir_path = Path(self.output_dir)
        try:
            total, used, free = shutil.disk_usage(output_dir_path)
            free_gb = free / (1024**3)
            required_gb = 4.0  # é¢„ä¼°éœ€è¦4GBç©ºé—´ï¼ˆæ£€æŸ¥ç‚¹çº¦3.2GB + ç¼“å†²ï¼‰
            
            print(f"ğŸ’¾ ç£ç›˜ç©ºé—´æ£€æŸ¥: å¯ç”¨ {free_gb:.1f}GB, éœ€è¦ {required_gb:.1f}GB")
            
            if free_gb < required_gb:
                print(f"âŒ ç£ç›˜ç©ºé—´ä¸è¶³ï¼å¯ç”¨ {free_gb:.1f}GB < éœ€è¦ {required_gb:.1f}GB")
                print(f"ğŸ—‘ï¸  å°è¯•æ¸…ç†æ—§æ£€æŸ¥ç‚¹...")
                
                # ç´§æ€¥æ¸…ç†ï¼šåˆ é™¤é™¤äº†latest.pthä¹‹å¤–çš„æ‰€æœ‰æ—§æ£€æŸ¥ç‚¹
                checkpoints_dir = Path(self.output_dir) / 'checkpoints'
                if checkpoints_dir.exists():
                    deleted_files = []
                    for file in checkpoints_dir.glob("*.pth"):
                        if file.name not in ['latest.pth']:  # ä¿ç•™latest.pth
                            try:
                                file_size_gb = file.stat().st_size / (1024**3)
                                file.unlink()
                                deleted_files.append(f"{file.name} ({file_size_gb:.1f}GB)")
                            except Exception as e:
                                print(f"   âš ï¸  åˆ é™¤ {file.name} å¤±è´¥: {e}")
                    
                    if deleted_files:
                        print(f"   ğŸ—‘ï¸  å·²åˆ é™¤: {', '.join(deleted_files)}")
                        
                        # é‡æ–°æ£€æŸ¥ç©ºé—´
                        total, used, free = shutil.disk_usage(output_dir_path)
                        free_gb = free / (1024**3)
                        print(f"   ğŸ’¾ æ¸…ç†åå¯ç”¨ç©ºé—´: {free_gb:.1f}GB")
                        
                        if free_gb < required_gb:
                            print(f"   âŒ æ¸…ç†åç©ºé—´ä»ä¸è¶³ï¼Œè·³è¿‡ä¿å­˜")
                            return
                    else:
                        print(f"   âš ï¸  æ— å¯æ¸…ç†æ–‡ä»¶ï¼Œè·³è¿‡ä¿å­˜")
                        return
        except Exception as e:
            print(f"âš ï¸  ç£ç›˜ç©ºé—´æ£€æŸ¥å¤±è´¥: {e}")
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.output_dir, 'checkpoints', 'latest.pth')
        try:
            self.model.save_checkpoint(
                filepath=checkpoint_path,
                epoch=epoch,
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                metrics={'train_history': self.train_history}
            )
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return
    
    def train(self, start_epoch: int = 0):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        train_config = self.config.get('training', {})
        epochs = train_config.get('epochs', 100)
        
        print(f"ğŸš€ å¼€å§‹LDMè®­ç»ƒ...")
        print(f"   æ€»è½®æ¬¡: {epochs}")
        print(f"   èµ·å§‹è½®æ¬¡: {start_epoch + 1}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æ£€æŸ¥å¹¶è®¡ç®—VAEç¼©æ”¾å› å­
        if start_epoch == 0:  # åªåœ¨è®­ç»ƒå¼€å§‹æ—¶æ£€æŸ¥
            print("\n" + "="*60)
            print("ğŸ” VAEç¼©æ”¾å› å­æ£€æŸ¥ä¸éªŒè¯")
            print("="*60)
            
            optimal_scaling_factor = self.compute_vae_scaling_factor()
            
            current_factor = self.model.scaling_factor
            factor_diff = abs(optimal_scaling_factor - current_factor) / current_factor
            
            print(f"\nğŸ“Š ç¼©æ”¾å› å­åˆ†æ:")
            print(f"   å½“å‰ç¼©æ”¾å› å­: {current_factor:.6f}")
            print(f"   å»ºè®®ç¼©æ”¾å› å­: {optimal_scaling_factor:.6f}")
            print(f"   ç›¸å¯¹å·®å¼‚: {factor_diff*100:.1f}%")
            
            if factor_diff > 0.1:  # å·®å¼‚è¶…è¿‡10%
                print(f"\nâš ï¸  ç¼©æ”¾å› å­å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®æ›´æ–°")
                
                # è‡ªåŠ¨æ›´æ–°ç¼©æ”¾å› å­ï¼ˆä¹Ÿå¯ä»¥æ‰‹åŠ¨ç¡®è®¤ï¼‰
                auto_update = train_config.get('auto_update_scaling_factor', True)
                if auto_update:
                    print(f"ğŸ”„ è‡ªåŠ¨æ›´æ–°ç¼©æ”¾å› å­...")
                    old_factor = self.model.scaling_factor
                    self.model.scaling_factor = optimal_scaling_factor
                    print(f"âœ… ç¼©æ”¾å› å­å·²æ›´æ–°: {old_factor:.6f} â†’ {optimal_scaling_factor:.6f}")
                    
                    # éªŒè¯æ›´æ–°æ•ˆæœ
                    print(f"\nğŸ”¬ éªŒè¯æ›´æ–°æ•ˆæœ:")
                    with torch.no_grad():
                        # æµ‹è¯•ä¸€ä¸ªå°æ‰¹æ¬¡çš„ç¼–ç 
                        test_batch = next(iter(self.train_loader))
                        if isinstance(test_batch, (list, tuple)):
                            test_images = test_batch[0][:4]  # å–4å¼ å›¾ç‰‡æµ‹è¯•
                        else:
                            test_images = test_batch[:4]
                        
                        test_images = test_images.to(self.device)
                        posterior = self.vae_model.encode(test_images)
                        test_latents = posterior.latent_dist.sample() * self.model.scaling_factor
                        
                        test_mean = test_latents.mean().item()
                        test_std = test_latents.std().item()
                        print(f"   ç¼–ç æµ‹è¯•ç»“æœ: å‡å€¼={test_mean:.4f}, æ ‡å‡†å·®={test_std:.4f}")
                        
                        if 0.8 <= test_std <= 1.2:
                            print(f"   âœ… ç¼©æ”¾æ•ˆæœè‰¯å¥½ (æ ‡å‡†å·®æ¥è¿‘1.0)")
                        elif test_std < 0.8:
                            print(f"   âš ï¸  æ ‡å‡†å·®åå° ({test_std:.4f})ï¼Œå¯èƒ½å½±å“ç”Ÿæˆè´¨é‡")
                        else:
                            print(f"   âš ï¸  æ ‡å‡†å·®åå¤§ ({test_std:.4f})ï¼Œå¯èƒ½å½±å“è®­ç»ƒç¨³å®šæ€§")
                else:
                    print(f"âš ï¸  å»ºè®®æ‰‹åŠ¨æ›´æ–°é…ç½®ä¸­çš„ç¼©æ”¾å› å­")
            else:
                print(f"âœ… ç¼©æ”¾å› å­åˆé€‚ï¼Œæ— éœ€æ›´æ–°")
            
            print("="*60)
            print("âœ… ç¼©æ”¾å› å­æ£€æŸ¥å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
            print("="*60 + "\n")
        
        for epoch in range(start_epoch, epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss = self.validate_epoch(epoch)
            
            # FIDè¯„ä¼°ï¼ˆæ ¹æ®é…ç½®ï¼‰
            fid_config = self.config.get('evaluation', {}).get('fid_evaluation', {})
            sample_config = self.config.get('evaluation', {}).get('sample_generation', {})
            
            fid_score = None
            if fid_config.get('enabled', True) and (epoch + 1) % fid_config.get('eval_every_epochs', 5) == 0:
                fid_score = self.evaluate_fid(epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_history['epoch'].append(epoch + 1)
            self.train_history['train_loss'].append(train_loss)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['fid_score'].append(fid_score if fid_score is not None else None)
            self.train_history['lr'].append(self.optimizer.param_groups[0]['lr'])
            
            # æ‰“å°ç»Ÿè®¡
            print(f"ğŸ“Š Train Loss: {train_loss:.4f}")
            print(f"ğŸ“Š Val Loss: {val_loss:.4f}")
            if fid_score is not None:
                print(f"ğŸ“Š FID Score: {fid_score:.2f}")
            print(f"ğŸ“Š Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            is_best_fid = False
            
            if is_best:
                self.best_val_loss = val_loss
                print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            if fid_score is not None and fid_score < self.best_fid_score:
                self.best_fid_score = fid_score
                is_best_fid = True
                print(f"ğŸ¯ æ–°çš„æœ€ä½³FIDåˆ†æ•°: {fid_score:.2f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best, is_best_fid)
            
            # ç”Ÿæˆæ ·æœ¬ï¼ˆæ ¹æ®é…ç½®ï¼‰
            if (epoch + 1) % sample_config.get('sample_every_epochs', 5) == 0:
                num_samples = sample_config.get('num_sample_images', 8)
                self.generate_samples(epoch, num_samples)
            
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"ğŸ¯ æœ€ä½³FIDåˆ†æ•°: {self.best_fid_score:.2f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(self.output_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.train_history, f, indent=2)
        print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


def load_config(config_path: str = None) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path and os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config = yaml.safe_load(f)
            else:
                config = json.load(f)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
    else:
        # é»˜è®¤é…ç½®
        config = {
            'data': {
                'data_dir': '/kaggle/input/dataset/dataset',
                'batch_size': 8,  # ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°
                'num_workers': 2
            },
            'unet': {
                'in_channels': 4,
                'out_channels': 4,
                'model_channels': 320,
                'num_res_blocks': 2,
                'attention_resolutions': [4, 2, 1],
                'channel_mult': [1, 2, 4, 4],
                'num_classes': 31,
                'dropout': 0.1,
                'num_heads': 8,
                'use_scale_shift_norm': True,
                'resblock_updown': True
            },
            'diffusion': {
                'timesteps': 1000,
                'beta_schedule': 'cosine',
                'beta_start': 0.0001,  # 0.0001
                'beta_end': 0.02
            },
            'training': {
                'epochs': 50,
                'learning_rate': 0.0001,  # 1e-4
                'weight_decay': 0.01,
                'scheduler': 'cosine',
                'min_lr': 0.000001  # 1e-6
            },
            'output_dir': './ldm_outputs'
        }
        print("âš ï¸  ä½¿ç”¨é»˜è®¤é…ç½®")
    
    return config


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    config_path = "ldm_config.yaml"  # å¦‚æœå­˜åœ¨çš„è¯
    vae_checkpoint_path = None  # å°†æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
    
    # æŸ¥æ‰¾VAEæ£€æŸ¥ç‚¹
    possible_vae_paths = [
        "/kaggle/input/vae-model/vae_finetuned_epoch_23.pth",  # ç”¨æˆ·æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹
        "VAE/vae_model_best.pth",
        "VAE/vae_model_final.pth",
        "VAE/checkpoints/best_vae_checkpoint.pth",
        "VAE/checkpoints/vae_epoch_025.pth",
        "../VAE/vae_model_best.pth"
    ]
    
    for path in possible_vae_paths:
        if os.path.exists(path):
            vae_checkpoint_path = path
            print(f"ğŸ” æ‰¾åˆ°VAEæ£€æŸ¥ç‚¹: {path}")
            break
    
    if vae_checkpoint_path is None:
        print("âš ï¸  æœªæ‰¾åˆ°VAEæ£€æŸ¥ç‚¹ï¼Œå°†ä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusion VAE")
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LDMTrainer(
        config=config,
        vae_checkpoint_path=vae_checkpoint_path,
        device=device
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main() 
