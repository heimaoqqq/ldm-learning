"""
æ ‡å‡†VAE-LDMå®ç° - ä½¿ç”¨AutoencoderKLæ›¿æ¢VQ-VAE
åŸºäºCompVis/latent-diffusionçš„æ ‡å‡†æ¶æ„
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple
from vae_diffusion import VAEDiffusionUNet
from gaussian_diffusion import create_diffusion

class StandardVAELatentDiffusionModel(nn.Module):
    """
    æ ‡å‡†VAEæ½œåœ¨æ‰©æ•£æ¨¡å‹ - ä½¿ç”¨AutoencoderKL
    """
    def __init__(
        self,
        unet_config: Dict[str, Any],
        diffusion_config: Dict[str, Any],
        vae_model_name: str = "runwayml/stable-diffusion-v1-5",
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        super().__init__()
        
        self.device = device
        
        # 1. åŠ è½½æ ‡å‡†AutoencoderKL
        self.vae = self._load_standard_vae(vae_model_name)
        
        # 2. åˆå§‹åŒ–U-Net (é€‚é…4é€šé“æ½œåœ¨ç©ºé—´)
        unet_config['in_channels'] = 4  # AutoencoderKLçš„æ½œåœ¨é€šé“æ•°
        unet_config['out_channels'] = 4
        self.unet = VAEDiffusionUNet(**unet_config)
        
        # 3. åˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹
        self.diffusion = create_diffusion(**diffusion_config)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(device)
        
        print(f"âœ… æ ‡å‡†VAE-LDM åˆå§‹åŒ–å®Œæˆ")
        print(f"   VAEæ½œåœ¨é€šé“æ•°: {self.vae.config.latent_channels}")
        print(f"   VAEç¼©æ”¾å› å­: {self.vae.config.scaling_factor}")
        print(f"   U-Netç±»åˆ«æ•°: {self.unet.num_classes}")
        print(f"   æ‰©æ•£æ­¥æ•°: {self.diffusion.num_timesteps}")
        
    def _load_standard_vae(self, model_name: str):
        """åŠ è½½æ ‡å‡†AutoencoderKL"""
        try:
            from diffusers import AutoencoderKL
            
            vae = AutoencoderKL.from_pretrained(
                model_name,
                subfolder="vae",
                torch_dtype=torch.float32
            )
            
            # å†»ç»“VAEå‚æ•°
            for param in vae.parameters():
                param.requires_grad = False
            vae.eval()
            
            print(f"âœ… æ ‡å‡†AutoencoderKLåŠ è½½æˆåŠŸ: {model_name}")
            print(f"ğŸ”’ VAEå‚æ•°å·²å†»ç»“ï¼ˆ{sum(p.numel() for p in vae.parameters())} å‚æ•°ï¼‰")
            
            return vae
            
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½æ ‡å‡†AutoencoderKL: {e}")
            print("ğŸ’¡ è¯·å®‰è£…å¿…è¦åº“: pip install diffusers transformers accelerate")
            raise
    
    @torch.no_grad()
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨æ ‡å‡†æ–¹å¼ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ [-1, 1]
            
        Returns:
            latents: [B, 4, h, w] æ½œåœ¨è¡¨ç¤º
        """
        self.vae.eval()
        
        # æ ‡å‡†ç¼–ç æ–¹å¼
        latent_dist = self.vae.encode(images).latent_dist
        latents = latent_dist.sample() * self.vae.config.scaling_factor
        
        return latents
    
    @torch.no_grad() 
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ
        
        Args:
            latents: [B, 4, h, w] æ½œåœ¨è¡¨ç¤º
            
        Returns:
            images: [B, 3, H, W] é‡å»ºå›¾åƒ [-1, 1]
        """
        self.vae.eval()
        
        # æ ‡å‡†è§£ç æ–¹å¼
        decoded = self.vae.decode(latents / self.vae.config.scaling_factor).sample
        
        return decoded
    
    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None, current_epoch: Optional[int] = None) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒå‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾
            current_epoch: å½“å‰è®­ç»ƒçš„epochå·
            
        Returns:
            loss_dict: åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
        """
        # 1. ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            latents = self.encode_to_latent(images)
        
        # è°ƒè¯•è¾“å‡ºï¼šå‰5ä¸ªepochæ˜¾ç¤ºæ½œåœ¨è¡¨ç¤ºç»Ÿè®¡ä¿¡æ¯
        if current_epoch is not None and current_epoch < 5:
            if not hasattr(self, '_debug_batch_count'):
                self._debug_batch_count = {}
            
            if current_epoch not in self._debug_batch_count:
                self._debug_batch_count[current_epoch] = 0
            
            if self._debug_batch_count[current_epoch] < 3:  # æ¯ä¸ªepochåªæ˜¾ç¤ºå‰3ä¸ªbatch
                print(f"ğŸ“Š Epoch {current_epoch+1} Batch {self._debug_batch_count[current_epoch]+1}:")
                print(f"   æ ‡å‡†VAEè¾“å‡º: å‡å€¼={latents.mean().item():.4f}, æ ‡å‡†å·®={latents.std().item():.4f}")
                print(f"   æ½œåœ¨è¡¨ç¤ºèŒƒå›´: [{latents.min().item():.4f}, {latents.max().item():.4f}]")
                print(f"   æ½œåœ¨ç©ºé—´å½¢çŠ¶: {latents.shape}")
                
                self._debug_batch_count[current_epoch] += 1
        
        # 2. éšæœºé‡‡æ ·æ—¶é—´æ­¥
        batch_size = latents.shape[0]
        t = torch.randint(0, self.diffusion.num_timesteps, (batch_size,), device=self.device, dtype=torch.long)
        
        # 3. è®¡ç®—æ‰©æ•£æŸå¤±
        model_kwargs = {}
        if class_labels is not None:
            model_kwargs['y'] = class_labels
            
        losses = self.diffusion.training_losses(
            model=self.unet,
            x_start=latents,
            t=t,
            model_kwargs=model_kwargs
        )
        
        return losses
    
    @torch.no_grad()
    def sample(
        self,
        num_samples: int,
        class_labels: Optional[torch.Tensor] = None,
        guidance_scale: float = 1.0,
        num_inference_steps: Optional[int] = None,
        use_ddim: bool = True,
        eta: float = 0.0,
        return_intermediates: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        ç”Ÿæˆæ ·æœ¬
        """
        self.unet.eval()
        
        # è·å–æ½œåœ¨ç©ºé—´å½¢çŠ¶ (4é€šé“, 32x32 for 256x256å›¾åƒ)
        latent_shape = (4, 32, 32)  # æ ‡å‡†AutoencoderKLçš„æ½œåœ¨å½¢çŠ¶
        shape = (num_samples, *latent_shape)
        
        # æ¨¡å‹å‚æ•°
        model_kwargs = {}
        if class_labels is not None:
            assert len(class_labels) == num_samples
            model_kwargs['y'] = class_labels
        
        # é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        if use_ddim:
            latents = self.diffusion.ddim_sample_loop(
                model=self.unet,
                shape=shape,
                clip_denoised=True,
                model_kwargs=model_kwargs,
                device=self.device,
                progress=True,
                eta=eta
            )
        else:
            latents = self.diffusion.p_sample_loop(
                model=self.unet,
                shape=shape,
                clip_denoised=True,
                model_kwargs=model_kwargs,
                device=self.device,
                progress=True
            )
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        images = self.decode_from_latent(latents)
        
        if return_intermediates:
            return images, latents
        else:
            return images, None
    
    def configure_optimizers(self, lr: float = 1e-4, weight_decay: float = 0.0):
        """é…ç½®ä¼˜åŒ–å™¨ - åªä¼˜åŒ–U-Netå‚æ•°"""
        optimizer = torch.optim.AdamW(
            self.unet.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        return optimizer
    
    def save_checkpoint(self, filepath: str, epoch: int, optimizer_state: Dict = None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'unet_state_dict': self.unet.state_dict(),
            'model_config': {
                'unet_config': {
                    'image_size': self.unet.image_size,
                    'in_channels': self.unet.in_channels,
                    'model_channels': self.unet.model_channels,
                    'out_channels': self.unet.out_channels,
                    'num_res_blocks': self.unet.num_res_blocks,
                    'attention_resolutions': self.unet.attention_resolutions,
                    'dropout': self.unet.dropout,
                    'channel_mult': self.unet.channel_mult,
                    'num_classes': self.unet.num_classes,
                    'use_checkpoint': self.unet.use_checkpoint,
                    'num_heads': self.unet.num_heads,
                    'num_heads_upsample': self.unet.num_heads_upsample,
                }
            }
        }
        
        if optimizer_state is not None:
            checkpoint['optimizer_state_dict'] = optimizer_state
            
        torch.save(checkpoint, filepath)
        print(f"âœ… Checkpoint saved: {filepath}")
    
    def load_checkpoint(self, filepath: str, load_optimizer: bool = False):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # åŠ è½½U-Netæƒé‡
        self.unet.load_state_dict(checkpoint['unet_state_dict'])
        
        epoch = checkpoint.get('epoch', 0)
        optimizer_state = checkpoint.get('optimizer_state_dict', None) if load_optimizer else None
        
        print(f"âœ… Checkpoint loaded: {filepath} (epoch {epoch})")
        
        return epoch, optimizer_state

def create_standard_vae_ldm(
    image_size: int = 32,
    num_classes: int = 31,
    diffusion_steps: int = 1000,
    noise_schedule: str = "cosine",
    vae_model_name: str = "runwayml/stable-diffusion-v1-5",
    device: str = None
) -> StandardVAELatentDiffusionModel:
    """
    åˆ›å»ºæ ‡å‡†VAE-LDMæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    """
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # U-Neté…ç½® (é€‚é…4é€šé“æ½œåœ¨ç©ºé—´)
    unet_config = {
        'image_size': image_size,
        'in_channels': 4,                    # æ ‡å‡†AutoencoderKLä½¿ç”¨4é€šé“
        'model_channels': 192,
        'out_channels': 4,                   # è¾“å‡ºä¹Ÿæ˜¯4é€šé“
        'num_res_blocks': 3,
        'attention_resolutions': (4, 8, 16),
        'dropout': 0.1,
        'channel_mult': (1, 2, 3, 4),
        'num_classes': num_classes,
        'use_checkpoint': False,
        'num_heads': 4,
        'num_heads_upsample': -1,
        'use_scale_shift_norm': True
    }
    
    # æ‰©æ•£é…ç½®
    diffusion_config = {
        'timestep_respacing': "",
        'noise_schedule': noise_schedule,
        'use_kl': False,
        'sigma_small': False,
        'predict_xstart': False,
        'learn_sigma': False,
        'rescale_learned_sigmas': False,
        'diffusion_steps': diffusion_steps,
        'rescale_timesteps': True,
    }

    model = StandardVAELatentDiffusionModel(
        unet_config=unet_config,
        diffusion_config=diffusion_config,
        vae_model_name=vae_model_name,
        device=device
    )
    
    return model 
