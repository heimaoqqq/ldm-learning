"""
æµ‹è¯•è®ºæ–‡æ ‡å‡†CLDMå®ç°
éªŒè¯æ¨¡å‹ç»“æ„å’Œå‚æ•°æ˜¯å¦æ­£ç¡®
"""

import torch
import yaml
from cldm_paper_standard import PaperStandardCLDM

def test_model_structure():
    """æµ‹è¯•æ¨¡å‹ç»“æ„"""
    print("=== æµ‹è¯•è®ºæ–‡æ ‡å‡†CLDMæ¨¡å‹ç»“æ„ ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = PaperStandardCLDM(
        latent_dim=256,
        num_classes=31,
        num_timesteps=1000,
        beta_schedule="linear",
        model_channels=128,
        time_emb_dim=256,
        class_emb_dim=256,
        channel_mult=[1, 2, 4],
        num_res_blocks=2,
        attention_resolutions=[8],
        dropout=0.0
    )
    
    # æ¨¡å‹å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
    print(f"ğŸ‹ï¸ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model

def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n=== æµ‹è¯•å‰å‘ä¼ æ’­ ===")
    
    model = PaperStandardCLDM(
        latent_dim=256,
        num_classes=31,
        num_timesteps=1000
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    x_0 = torch.randn(batch_size, 256, 16, 16)
    class_labels = torch.randint(0, 31, (batch_size,))
    
    print(f"ğŸ” è¾“å…¥å½¢çŠ¶: {x_0.shape}")
    print(f"ğŸ“ ç±»åˆ«æ ‡ç­¾: {class_labels}")
    
    # å‰å‘ä¼ æ’­
    try:
        loss, pred_noise, target_noise = model(x_0, class_labels)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"ğŸ“‰ æŸå¤±å€¼: {loss.item():.6f}")
        print(f"ğŸ”Š é¢„æµ‹å™ªå£°å½¢çŠ¶: {pred_noise.shape}")
        print(f"ğŸ¯ ç›®æ ‡å™ªå£°å½¢çŠ¶: {target_noise.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False

def test_sampling():
    """æµ‹è¯•é‡‡æ ·è¿‡ç¨‹"""
    print("\n=== æµ‹è¯•DDIMé‡‡æ · ===")
    
    model = PaperStandardCLDM(
        latent_dim=256,
        num_classes=31,
        num_timesteps=1000
    )
    
    # åˆ›å»ºç±»åˆ«æ ‡ç­¾
    class_labels = torch.tensor([0, 1, 2, 3])
    device = 'cpu'
    
    print(f"ğŸ¯ é‡‡æ ·ç±»åˆ«: {class_labels.tolist()}")
    
    try:
        # é‡‡æ ·
        samples = model.sample(
            class_labels=class_labels,
            device=device,
            num_inference_steps=20,  # å¿«é€Ÿæµ‹è¯•
            eta=0.0
        )
        
        print(f"âœ… é‡‡æ ·æˆåŠŸ")
        print(f"ğŸ–¼ï¸ é‡‡æ ·ç»“æœå½¢çŠ¶: {samples.shape}")
        print(f"ğŸ“Š é‡‡æ ·å€¼èŒƒå›´: [{samples.min():.3f}, {samples.max():.3f}]")
        
        return True
    except Exception as e:
        print(f"âŒ é‡‡æ ·å¤±è´¥: {e}")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("\n=== æµ‹è¯•é…ç½®æ–‡ä»¶ ===")
    
    try:
        with open('config_paper_standard.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®é…ç½®
        cldm_config = config['cldm']
        print(f"ğŸ—ï¸ æ¨¡å‹ç±»å‹: {cldm_config['model_type']}")
        print(f"â±ï¸ æ‰©æ•£æ­¥æ•°: {cldm_config['num_timesteps']}")
        print(f"ğŸ“ æ¨¡å‹é€šé“æ•°: {cldm_config['model_channels']}")
        print(f"ğŸ¯ ç±»åˆ«æ•°: {cldm_config['num_classes']}")
        
        training_config = config['training']
        print(f"ğŸ“ å­¦ä¹ ç‡: {training_config['lr']}")
        print(f"ğŸ“¦ æ‰¹é‡å¤§å°: {config['dataset']['batch_size']}")
        
        return True
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_architecture_compliance():
    """æµ‹è¯•æ¶æ„æ˜¯å¦ç¬¦åˆè®ºæ–‡æ ‡å‡†"""
    print("\n=== éªŒè¯è®ºæ–‡æ ‡å‡†ç¬¦åˆæ€§ ===")
    
    model = PaperStandardCLDM(
        latent_dim=256,
        num_classes=31,
        num_timesteps=1000
    )
    
    # æ£€æŸ¥U-Netç»“æ„
    unet = model.unet
    
    print("ğŸ—ï¸ U-Netæ¶æ„æ£€æŸ¥:")
    print(f"   âœ… æ—¶é—´åµŒå…¥ç»´åº¦: {unet.time_emb_dim}")
    print(f"   âœ… åŸºç¡€é€šé“æ•°: {unet.model_channels}")
    print(f"   âœ… è¾“å…¥é€šé“æ•°: {unet.in_channels}")
    
    # æ£€æŸ¥æ‰©æ•£è¿‡ç¨‹
    diffusion = model.diffusion
    print(f"ğŸ”„ æ‰©æ•£è¿‡ç¨‹æ£€æŸ¥:")
    print(f"   âœ… æ—¶é—´æ­¥æ•°: {diffusion.num_timesteps}")
    print(f"   âœ… Betaå½¢çŠ¶: {model.betas.shape}")
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶
    has_attention = any('SelfAttentionBlock' in str(type(layer)) 
                       for layer in unet.middle)
    print(f"ğŸ¯ è‡ªæ³¨æ„åŠ›æœºåˆ¶: {'âœ… å­˜åœ¨' if has_attention else 'âŒ ç¼ºå¤±'}")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•è®ºæ–‡æ ‡å‡†CLDMå®ç°...")
    
    tests = [
        ("æ¨¡å‹ç»“æ„", test_model_structure),
        ("å‰å‘ä¼ æ’­", test_forward_pass),
        ("DDIMé‡‡æ ·", test_sampling),
        ("é…ç½®æ–‡ä»¶", test_config_loading),
        ("è®ºæ–‡ç¬¦åˆæ€§", test_architecture_compliance),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            if result:
                passed += 1
                print(f"âœ… {test_name} - é€šè¿‡")
            else:
                print(f"âŒ {test_name} - å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} - å¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è®ºæ–‡æ ‡å‡†å®ç°å‡†å¤‡å°±ç»ªï¼")
        print("\nğŸš€ å»ºè®®ä¸‹ä¸€æ­¥:")
        print("   1. ä¸Šä¼ åˆ°Kaggle")
        print("   2. è¿è¡Œ train_paper_standard.py")
        print("   3. ç›‘æ§è®­ç»ƒè¿›å±•å’ŒFIDæ”¹è¿›")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")

if __name__ == "__main__":
    main() 