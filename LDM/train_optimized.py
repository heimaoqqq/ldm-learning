#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæœ¬CLDMè®­ç»ƒè„šæœ¬
ä¸“é—¨ç”¨äºæå‡FIDåˆ†æ•°ï¼ŒåŸºäºä¿®å¤ç‰ˆæœ¬çš„æ¨¡å‹æ¶æ„
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.utils as vutils
import math
import shutil
import numpy as np
import yaml
from tqdm import tqdm
import argparse
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append('/kaggle/input/advanced-vqvae')
from ae_debug.adv_vq_vae import AdvVQVAE
from build_dataloader import build_dataloader

# å¯¼å…¥ä¼˜åŒ–ç‰ˆæœ¬çš„CLDM
from cldm_optimized import ImprovedCLDM

# æ£€æŸ¥ä¾èµ–
HAS_AMP = True
try:
    from torch.cuda.amp import autocast, GradScaler
except ImportError:
    HAS_AMP = False
    print("âš ï¸ æ— æ³•å¯¼å…¥AMPï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦è®­ç»ƒ")

HAS_FIDELITY = True
try:
    from torchmetrics.image.fid import FrechetInceptionDistance
    import torchmetrics
except ImportError:
    HAS_FIDELITY = False
    print("âš ï¸ æ— æ³•å¯¼å…¥torchmetricsï¼Œå°†è·³è¿‡FIDè®¡ç®—")

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸŒ± è®¾ç½®éšæœºç§å­: {seed}")

def denormalize(x):
    """åå½’ä¸€åŒ– [-1,1] -> [0,1]"""
    return (x + 1) / 2

def extract_latent_features(vqvae_model, dataloader, device, max_samples=None):
    """ä»VQ-VAEæå–æ½œåœ¨ç‰¹å¾"""
    vqvae_model.eval()
    all_latents = []
    all_labels = []
    
    sample_count = 0
    print("ğŸ“Š æå–VQ-VAEæ½œåœ¨ç‰¹å¾...")
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="æå–ç‰¹å¾"):
            if max_samples and sample_count >= max_samples:
                break
                
            images = images.to(device)
            labels = labels.to(device)
            
            # VQ-VAEç¼–ç 
            z_e = vqvae_model.encoder(images)
            z_q, _, _ = vqvae_model.quantize(z_e)
            
            all_latents.append(z_q.cpu())
            all_labels.append(labels.cpu())
            
            sample_count += images.size(0)
    
    latents = torch.cat(all_latents, dim=0)
    labels = torch.cat(all_labels, dim=0)
    
    print(f"âœ… æå–å®Œæˆ: {latents.shape[0]} ä¸ªæ½œåœ¨ç‰¹å¾ï¼Œå½¢çŠ¶: {latents.shape}")
    return latents, labels

def calculate_fid_score(cldm_model, vqvae_model, val_loader, config, device):
    """è®¡ç®—é«˜è´¨é‡FIDåˆ†æ•°"""
    if not HAS_FIDELITY:
        return float('inf')
    
    try:
        print("ğŸ§® è®¡ç®—FIDåˆ†æ•°...")
        
        # åˆå§‹åŒ–FIDåº¦é‡
        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True)
        fid_metric = fid_metric.to(device)
        
        cldm_model.eval()
        vqvae_model.eval()
        
        sample_size = config['training']['num_sample_images']
        sampling_steps = config['training']['fid_sampling_steps']
        eta = config['training']['eta']
        
        real_images = []
        generated_images = []
        sample_count = 0
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="FIDé‡‡æ ·"):
                if sample_count >= sample_size:
                    break
                    
                images = images.to(device)
                labels = labels.to(device)
                batch_size = images.size(0)
                
                # ç”Ÿæˆå¯¹åº”å›¾åƒ
                generated_z = cldm_model.sample(
                    labels, device, sampling_steps=sampling_steps, eta=eta
                )
                gen_images = vqvae_model.decoder(generated_z)
                
                # è½¬æ¢ä¸º[0,255]æ ¼å¼
                real_uint8 = ((denormalize(images) * 255).clamp(0, 255).to(torch.uint8))
                gen_uint8 = ((denormalize(gen_images) * 255).clamp(0, 255).to(torch.uint8))
                
                real_images.append(real_uint8.cpu())
                generated_images.append(gen_uint8.cpu())
                
                sample_count += batch_size
                
            # åˆå¹¶æ‰€æœ‰å›¾åƒ
            real_all = torch.cat(real_images, dim=0)[:sample_size]
            gen_all = torch.cat(generated_images, dim=0)[:sample_size]
            
            print(f"FIDè¯„ä¼°: çœŸå®å›¾åƒ {real_all.shape[0]}, ç”Ÿæˆå›¾åƒ {gen_all.shape[0]}")
            
            # è®¡ç®—FID
            fid_metric.update(real_all.to(device), real=True)
            fid_metric.update(gen_all.to(device), real=False)
            fid_score = fid_metric.compute().item()
        
        cldm_model.train()
        return fid_score
        
    except Exception as e:
        print(f"âŒ FIDè®¡ç®—å‡ºé”™: {e}")
        return float('inf')

def sample_and_save(cldm_model, vqvae_model, config, device, epoch, save_dir):
    """ç”Ÿæˆå¹¶ä¿å­˜é‡‡æ ·å›¾åƒ"""
    cldm_model.eval()
    vqvae_model.eval()
    
    num_classes = config['dataset']['num_classes']
    samples_per_class = 2
    
    print(f"ğŸ¨ ç”Ÿæˆç¬¬{epoch}è½®é‡‡æ ·å›¾åƒ...")
    
    all_samples = []
    
    with torch.no_grad():
        for class_id in range(min(16, num_classes)):  # é™åˆ¶æ˜¾ç¤ºå‰16ä¸ªç±»åˆ«
            class_labels = torch.full((samples_per_class,), class_id, device=device)
            
            # é«˜è´¨é‡é‡‡æ ·
            generated_z = cldm_model.sample(
                class_labels, 
                device, 
                sampling_steps=config['training']['sampling_steps'],
                eta=config['training']['eta']
            )
            
            # è§£ç ä¸ºå›¾åƒ
            generated_images = vqvae_model.decoder(generated_z)
            generated_images = denormalize(generated_images)
            
            all_samples.append(generated_images)
    
    # ä¿å­˜å›¾åƒç½‘æ ¼
    all_samples = torch.cat(all_samples, dim=0)
    grid = vutils.make_grid(all_samples, nrow=samples_per_class, normalize=False, padding=2)
    
    os.makedirs(os.path.join(save_dir, 'samples'), exist_ok=True)
    vutils.save_image(grid, os.path.join(save_dir, 'samples', f'epoch_{epoch:03d}.png'))
    
    cldm_model.train()

def main():
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆæœ¬CLDMè®­ç»ƒ")
    parser.add_argument('--config', default='config_optimized.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', default=None, help='ç»§ç»­è®­ç»ƒçš„checkpointè·¯å¾„')
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®ç¯å¢ƒ
    set_seed(config['system']['seed'])
    device = torch.device(config['system']['device'] if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = config['training']['save_dir']
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(os.path.join(save_dir, 'samples'), exist_ok=True)
    os.makedirs(os.path.join(save_dir, 'checkpoints'), exist_ok=True)
    
    # ä¿å­˜é…ç½®å‰¯æœ¬
    with open(os.path.join(save_dir, 'config.yaml'), 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“š åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, train_size, val_size = build_dataloader(
        config['dataset']['root_dir'],
        batch_size=config['dataset']['batch_size'],
        num_workers=config['dataset']['num_workers'],
        shuffle_train=True,
        shuffle_val=False,
        val_split=0.3
    )
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {train_size} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {val_size} æ ·æœ¬")
    print(f"  ç±»åˆ«æ•°: {config['dataset']['num_classes']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['dataset']['batch_size']}")
    
    # åŠ è½½VQ-VAEæ¨¡å‹
    print("ğŸ”§ åŠ è½½VQ-VAEæ¨¡å‹...")
    vqvae = AdvVQVAE(
        in_channels=config['vqvae']['in_channels'],
        latent_dim=config['vqvae']['latent_dim'],
        num_embeddings=config['vqvae']['num_embeddings'],
        beta=config['vqvae']['beta'],
        decay=config['vqvae']['decay'],
        groups=config['vqvae']['groups']
    ).to(device)
    
    vqvae_path = config['vqvae']['model_path']
    if os.path.exists(vqvae_path):
        vqvae.load_state_dict(torch.load(vqvae_path, map_location=device))
        print(f"âœ… VQ-VAEæƒé‡åŠ è½½æˆåŠŸ")
    else:
        print(f"âŒ VQ-VAEæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {vqvae_path}")
        return
    
    # å†»ç»“VQ-VAE
    vqvae.eval()
    for param in vqvae.parameters():
        param.requires_grad = False
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬CLDMæ¨¡å‹
    print("ğŸš€ åˆ›å»ºä¼˜åŒ–ç‰ˆCLDMæ¨¡å‹...")
    cldm = ImprovedCLDM(
        latent_dim=config['cldm']['latent_dim'],
        num_classes=config['cldm']['num_classes'],
        num_timesteps=config['cldm']['num_timesteps'],
        beta_start=config['cldm']['beta_start'],
        beta_end=config['cldm']['beta_end'],
        noise_schedule=config['cldm']['noise_schedule'],
        time_emb_dim=config['cldm']['time_emb_dim'],
        class_emb_dim=config['cldm']['class_emb_dim'],
        base_channels=config['cldm']['base_channels'],
        dropout=config['cldm']['dropout'],
        groups=config['cldm']['groups']
    ).to(device)
    
    # æ¨¡å‹ç»Ÿè®¡
    total_params = sum(p.numel() for p in cldm.parameters())
    trainable_params = sum(p.numel() for p in cldm.parameters() if p.requires_grad)
    print(f"ğŸ“ˆ æ¨¡å‹ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        cldm.parameters(),
        lr=config['training']['lr'],
        weight_decay=config['training']['weight_decay'],
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['training']['use_scheduler']:
        def lr_lambda(epoch):
            warmup_epochs = config['training']['warmup_epochs']
            total_epochs = config['training']['epochs']
            min_lr_ratio = config['training']['min_lr'] / config['training']['lr']
            
            if epoch < warmup_epochs:
                return float(epoch) / float(max(1, warmup_epochs))
            else:
                progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
                return min_lr_ratio + (1.0 - min_lr_ratio) * 0.5 * (1.0 + math.cos(math.pi * progress))
        
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        print("ğŸ“š ä½¿ç”¨Cosineå­¦ä¹ ç‡è°ƒåº¦å™¨")
    else:
        scheduler = None
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    use_amp = config['system']['mixed_precision'] and HAS_AMP
    if use_amp:
        scaler = GradScaler()
        print("âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    else:
        scaler = None
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_fid = float('inf')
    best_loss = float('inf')
    
    if args.resume and os.path.exists(args.resume):
        print(f"ğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        cldm.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch']
        best_fid = checkpoint.get('best_fid', float('inf'))
        best_loss = checkpoint.get('best_loss', float('inf'))
    
    # æå–è®­ç»ƒæ•°æ®çš„æ½œåœ¨ç‰¹å¾
    train_latents, train_labels = extract_latent_features(vqvae, train_loader, device)
    
    # åˆ›å»ºæ½œåœ¨ç‰¹å¾æ•°æ®åŠ è½½å™¨
    from torch.utils.data import TensorDataset, DataLoader
    latent_dataset = TensorDataset(train_latents, train_labels)
    latent_loader = DataLoader(
        latent_dataset,
        batch_size=config['dataset']['batch_size'],
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )
    
    # è®­ç»ƒå¾ªç¯
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒä¼˜åŒ–ç‰ˆCLDM...")
    print(f"è®­ç»ƒè½®æ•°: {config['training']['epochs']}")
    print(f"ä»ç¬¬ {start_epoch + 1} è½®å¼€å§‹")
    
    for epoch in range(start_epoch, config['training']['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        cldm.train()
        running_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(latent_loader, desc=f"Epoch {epoch+1}/{config['training']['epochs']}")
        for z_0, class_labels in pbar:
            z_0 = z_0.to(device)
            class_labels = class_labels.to(device)
            
            optimizer.zero_grad()
            
            if use_amp:
                with autocast():
                    loss = cldm(z_0, class_labels)
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(cldm.parameters(), config['training']['grad_clip_norm'])
                scaler.step(optimizer)
                scaler.update()
            else:
                loss = cldm(z_0, class_labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(cldm.parameters(), config['training']['grad_clip_norm'])
                optimizer.step()
            
            running_loss += loss.item()
            num_batches += 1
            
            pbar.set_postfix({'loss': f"{loss.item():.6f}"})
        
        avg_train_loss = running_loss / num_batches
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler:
            scheduler.step()
        
        current_lr = optimizer.param_groups[0]['lr']
        
        # éªŒè¯é˜¶æ®µ
        if (epoch + 1) % config['training']['val_interval'] == 0:
            cldm.eval()
            val_loss = 0.0
            val_batches = 0
            
            # éªŒè¯é›†æ½œåœ¨ç‰¹å¾
            val_latents, val_labels = extract_latent_features(vqvae, val_loader, device, max_samples=1000)
            val_dataset = TensorDataset(val_latents, val_labels)
            val_latent_loader = DataLoader(val_dataset, batch_size=config['dataset']['batch_size'], shuffle=False)
            
            with torch.no_grad():
                for z_0, class_labels in val_latent_loader:
                    z_0 = z_0.to(device)
                    class_labels = class_labels.to(device)
                    
                    if use_amp:
                        with autocast():
                            loss = cldm(z_0, class_labels)
                    else:
                        loss = cldm(z_0, class_labels)
                    
                    val_loss += loss.item()
                    val_batches += 1
            
            avg_val_loss = val_loss / val_batches
            
            print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
            print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
            print(f"  å­¦ä¹ ç‡: {current_lr:.8f}")
            
            # ä¿å­˜æœ€ä½³æŸå¤±æ¨¡å‹
            if avg_val_loss < best_loss:
                best_loss = avg_val_loss
                torch.save(cldm.state_dict(), os.path.join(save_dir, 'cldm_best_loss.pth'))
                print(f"  âœ… ä¿å­˜æœ€ä½³æŸå¤±æ¨¡å‹: {best_loss:.6f}")
        else:
            print(f"\nEpoch {epoch+1}: è®­ç»ƒæŸå¤± {avg_train_loss:.6f}, å­¦ä¹ ç‡ {current_lr:.8f}")
        
        # FIDè¯„ä¼°
        if (epoch + 1) % config['training']['fid_eval_freq'] == 0 and HAS_FIDELITY:
            fid_score = calculate_fid_score(cldm, vqvae, val_loader, config, device)
            print(f"  ğŸ¯ FIDåˆ†æ•°: {fid_score:.4f}")
            
            if fid_score < best_fid:
                best_fid = fid_score
                torch.save(cldm.state_dict(), os.path.join(save_dir, 'cldm_best_fid.pth'))
                print(f"  ğŸ† ä¿å­˜æœ€ä½³FIDæ¨¡å‹: {best_fid:.4f}")
        
        # ç”Ÿæˆé‡‡æ ·
        if (epoch + 1) % config['training']['sample_interval'] == 0:
            sample_and_save(cldm, vqvae, config, device, epoch + 1, save_dir)
        
        # ä¿å­˜checkpoint
        if (epoch + 1) % config['training']['save_interval'] == 0:
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': cldm.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_fid': best_fid,
                'best_loss': best_loss,
                'config': config
            }
            if scheduler:
                checkpoint['scheduler_state_dict'] = scheduler.state_dict()
            
            torch.save(checkpoint, os.path.join(save_dir, 'checkpoints', f'epoch_{epoch+1:03d}.pth'))
            print(f"  ğŸ’¾ ä¿å­˜checkpoint")
    
    # è®­ç»ƒå®Œæˆ
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")
    if HAS_FIDELITY:
        print(f"æœ€ä½³FIDåˆ†æ•°: {best_fid:.4f}")
    
    # æœ€ç»ˆé‡‡æ ·
    sample_and_save(cldm, vqvae, config, device, "final", save_dir)
    print(f"ğŸ¨ æœ€ç»ˆæ ·æœ¬å·²ä¿å­˜")

if __name__ == "__main__":
    main() 
