"""
VAE-LDMæœ¬åœ°ä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬
ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†è·¯å¾„ï¼Œè§£å†³FID=300+é—®é¢˜
"""
import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from datetime import datetime
import time

# æ·»åŠ VAEè·¯å¾„
sys.path.append('../VAE')

from vae_ldm import create_vae_ldm
from dataset_adapter import build_dataloader
from fid_evaluation import FIDEvaluator
from metrics import denormalize_for_metrics

def check_dataset_loading(data_dir):
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½"""
    print(f"ğŸ” æ£€æŸ¥æ•°æ®é›†è·¯å¾„: {data_dir}")
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {data_dir}")
        return False
    
    # æ£€æŸ¥IDæ–‡ä»¶å¤¹
    id_folders = [d for d in os.listdir(data_dir) if d.startswith('ID_') and os.path.isdir(os.path.join(data_dir, d))]
    print(f"âœ… æ‰¾åˆ° {len(id_folders)} ä¸ªIDæ–‡ä»¶å¤¹: {id_folders}")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹çš„å›¾åƒ
    if id_folders:
        first_folder = os.path.join(data_dir, id_folders[0])
        image_files = [f for f in os.listdir(first_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"âœ… {id_folders[0]} ä¸­æœ‰ {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        if image_files:
            print(f"ğŸ“· ç¤ºä¾‹æ–‡ä»¶: {image_files[0]}")
            return True
    
    return False

class LocalFixedVAELDMTrainer:
    """æœ¬åœ°ä¿®å¤ç‰ˆVAE-LDMè®­ç»ƒå™¨"""
    
    def __init__(self, config_path="config_local_fixed.yaml"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ğŸ”§ å…³é”®ï¼šæ£€æŸ¥æ•°æ®é›†
        data_dir = self.config['paths']['data_dir']
        if not check_dataset_loading(data_dir):
            raise ValueError(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {data_dir}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = self.config['paths']['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸ”§ åˆå§‹åŒ–ä¿®å¤ç‰ˆVAE-LDM...")
        self.model = self._create_fixed_model()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        print("ğŸ“Š åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
        self.train_loader, self.val_loader = self._create_dataloaders()
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨
        self._validate_dataloader()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.current_step = 0
        self.best_fid = float('inf')
        
        print("âœ… æœ¬åœ°ä¿®å¤ç‰ˆVAE-LDMè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_fixed_model(self):
        """åˆ›å»ºä¿®å¤ç‰ˆæ¨¡å‹"""
        vae_checkpoint = self.config['paths']['vae_checkpoint']
        
        if not os.path.exists(vae_checkpoint):
            print(f"âš ï¸ VAEæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {vae_checkpoint}")
            print("ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–VAE")
            vae_checkpoint = None
        else:
            print(f"âœ… æ‰¾åˆ°VAEæƒé‡: {vae_checkpoint}")
        
        model = create_vae_ldm(
            vae_checkpoint_path=vae_checkpoint,
            image_size=self.config['model']['image_size'],
            num_classes=self.config['model']['num_classes'],
            diffusion_steps=self.config['model']['diffusion_steps'],
            noise_schedule=self.config['model']['noise_schedule'],
            device=self.device
        )
        
        return model
    
    def _create_dataloaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        config = self.config['training']
        
        dataloader_config = {
            'dataset': {
                'root_dir': self.config['paths']['data_dir'],
                'batch_size': config['batch_size'],
                'num_workers': config.get('num_workers', 0),
                'val_split': 0.1,
                'shuffle_train': True
            },
            'unet': {
                'num_classes': self.config['model']['num_classes']
            }
        }
        
        return build_dataloader(dataloader_config)
    
    def _validate_dataloader(self):
        """éªŒè¯æ•°æ®åŠ è½½å™¨æ˜¯å¦æ­£ç¡®åŠ è½½çœŸå®æ•°æ®"""
        print("ğŸ” éªŒè¯æ•°æ®åŠ è½½å™¨...")
        
        try:
            sample_batch = next(iter(self.val_loader))
            images = sample_batch['image']
            labels = sample_batch['label']
            
            print(f"âœ… æˆåŠŸåŠ è½½æ‰¹æ¬¡:")
            print(f"   å›¾åƒå½¢çŠ¶: {images.shape}")
            print(f"   æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
            print(f"   å›¾åƒå€¼èŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")
            print(f"   å›¾åƒå‡å€¼: {images.mean():.3f}")
            print(f"   å›¾åƒæ ‡å‡†å·®: {images.std():.3f}")
            
            # ä¿å­˜ä¸€ä¸ªæ‰¹æ¬¡çš„çœŸå®å›¾åƒæ ·æœ¬ç”¨äºéªŒè¯
            from torchvision.utils import save_image
            display_images = denormalize_for_metrics(images[:16])
            os.makedirs('debug_output', exist_ok=True)
            save_image(
                display_images, 
                'debug_output/real_data_verification.png',
                nrow=4, 
                normalize=False
            )
            print("ğŸ“· çœŸå®æ•°æ®æ ·æœ¬å·²ä¿å­˜åˆ°: debug_output/real_data_verification.png")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å™¨éªŒè¯å¤±è´¥: {e}")
            raise
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        config = self.config['training']
        
        optimizer = torch.optim.AdamW(
            self.model.unet.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            betas=(0.9, 0.999)
        )
        
        return optimizer
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = []
        
        progress_bar = tqdm(
            self.train_loader, 
            desc=f"Epoch {self.current_epoch+1}/{self.config['training']['max_epochs']}"
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            images = batch['image'].to(self.device)
            labels = batch['label'].to(self.device, dtype=torch.long)
            
            # å‰å‘ä¼ æ’­
            losses = self.model(images, labels, current_epoch=self.current_epoch)
            loss = losses['loss'].mean()
            
            # æ¢¯åº¦ç´¯ç§¯
            accumulation_steps = self.config['training']['gradient_accumulation_steps']
            loss = loss / accumulation_steps
            loss.backward()
            
            if (batch_idx + 1) % accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.model.unet.parameters(), 
                    self.config['gpu']['gradient_clip']
                )
                
                self.optimizer.step()
                self.optimizer.zero_grad()
                self.current_step += 1
                
                # è®°å½•æ—¥å¿—
                if self.current_step % self.config['training']['log_interval'] == 0:
                    progress_bar.set_postfix({
                        'loss': f'{loss.item() * accumulation_steps:.4f}',
                        'step': self.current_step,
                        'best_fid': f'{self.best_fid:.1f}'
                    })
            
            epoch_losses.append(loss.item() * accumulation_steps)
        
        return {'train_loss': np.mean(epoch_losses)}
    
    def evaluate_fid(self):
        """è¯„ä¼°FIDåˆ†æ•°"""
        print("ğŸ“Š è¯„ä¼°FIDåˆ†æ•°...")
        self.model.eval()
        
        eval_config = self.config['evaluation']
        
        # åˆ›å»ºFIDè¯„ä¼°å™¨
        fid_evaluator = FIDEvaluator(device=self.device)
        
        # è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾
        print("   è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾...")
        fid_evaluator.compute_real_features(
            self.val_loader, 
            max_samples=eval_config.get('max_real_samples', 500)
        )
        
        # ç”Ÿæˆæ ·æœ¬
        print("   ç”Ÿæˆæ ·æœ¬...")
        num_samples = eval_config['eval_classes'] * eval_config['samples_per_class']
        
        # ç”Ÿæˆç±»åˆ«æ ‡ç­¾
        samples_per_class = eval_config['samples_per_class']
        class_labels = []
        for class_id in range(eval_config['eval_classes']):
            class_labels.extend([class_id] * samples_per_class)
        
        class_labels = torch.tensor(class_labels, device=self.device, dtype=torch.long)
        
        # æ‰¹é‡ç”Ÿæˆ
        batch_size = eval_config.get('generation_batch_size', 8)
        generated_images = []
        
        with torch.no_grad():
            for i in range(0, num_samples, batch_size):
                current_batch_size = min(batch_size, num_samples - i)
                batch_labels = class_labels[i:i+current_batch_size]
                
                batch_images, _ = self.model.sample(
                    num_samples=current_batch_size,
                    class_labels=batch_labels,
                    use_ddim=True,
                    eta=eval_config.get('eta', 0.0)
                )
                
                generated_images.append(batch_images)
        
        # åˆå¹¶å¹¶è®¡ç®—FID
        generated_images = torch.cat(generated_images, dim=0)
        fid_score = fid_evaluator.calculate_fid_score(generated_images)
        
        return fid_score
    
    def save_checkpoint(self, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'step': self.current_step,
            'unet_state_dict': self.model.unet.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_fid': self.best_fid,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.save_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_fid_checkpoint.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ æœ€ä½³FIDæ£€æŸ¥ç‚¹å·²ä¿å­˜: {best_path}")
    
    def save_sample_images(self):
        """ä¿å­˜ç”Ÿæˆæ ·æœ¬"""
        print("ğŸ¨ ç”Ÿæˆæ ·æœ¬å›¾åƒ...")
        self.model.eval()
        
        with torch.no_grad():
            num_samples = 16
            class_labels = torch.randint(0, self.config['model']['num_classes'], 
                                       (num_samples,), device=self.device)
            
            generated_images, _ = self.model.sample(
                num_samples=num_samples,
                class_labels=class_labels,
                use_ddim=True,
                eta=0.0
            )
            
            # ä¿å­˜å›¾åƒ
            from torchvision.utils import save_image
            
            display_images = denormalize_for_metrics(generated_images)
            save_path = os.path.join(self.save_dir, f'samples_epoch_{self.current_epoch}.png')
            save_image(display_images, save_path, nrow=4, normalize=False)
            print(f"ğŸ’¾ æ ·æœ¬å·²ä¿å­˜: {save_path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæœ¬åœ°ä¿®å¤ç‰ˆVAE-LDM...")
        print(f"ğŸ“Š é…ç½®: {self.config['training']['max_epochs']} epochs, "
              f"batch_size={self.config['training']['batch_size']}")
        
        start_time = time.time()
        
        for epoch in range(self.config['training']['max_epochs']):
            self.current_epoch = epoch
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()
            
            print(f"\nEpoch {epoch+1}/{self.config['training']['max_epochs']}")
            print(f"  è®­ç»ƒæŸå¤±: {train_metrics['train_loss']:.6f}")
            
            # å®šæœŸè¯„ä¼°FID
            if (epoch + 1) % self.config['training']['eval_interval'] == 0:
                try:
                    fid_score = self.evaluate_fid()
                    print(f"  ğŸ“Š FIDåˆ†æ•°: {fid_score:.2f}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³åˆ†æ•°
                    is_best = fid_score < self.best_fid
                    if is_best:
                        self.best_fid = fid_score
                        print(f"  ğŸ‰ æ–°çš„æœ€ä½³FIDåˆ†æ•°: {fid_score:.2f}")
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    self.save_checkpoint(is_best=is_best)
                    
                except Exception as e:
                    print(f"  âš ï¸ FIDè¯„ä¼°å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            # å®šæœŸä¿å­˜æ ·æœ¬
            if (epoch + 1) % self.config['debug'].get('save_interval_samples', 10) == 0:
                try:
                    self.save_sample_images()
                except Exception as e:
                    print(f"  âš ï¸ æ ·æœ¬ä¿å­˜å¤±è´¥: {e}")
        
        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ")
        print(f"ğŸ“Š æœ€ä½³FIDåˆ†æ•°: {self.best_fid:.2f}")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ åˆå§‹åŒ–æœ¬åœ°ä¿®å¤ç‰ˆVAE-LDMè®­ç»ƒ...")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_file = "config_local_fixed.yaml"
    if not os.path.exists(config_file):
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨")
        return
    
    try:
        trainer = LocalFixedVAELDMTrainer(config_file)
        trainer.train()
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
