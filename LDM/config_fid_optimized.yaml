# FIDä¼˜åŒ–ä¸“ç”¨é…ç½® - ç›®æ ‡FID < 20
# é’ˆå¯¹æ­¥æ€çƒ­åŠ›å›¾æ•°æ®çš„æ·±åº¦ä¼˜åŒ–ç‰ˆæœ¬

# VAE ç›¸å…³é…ç½® (ä½¿ç”¨é¢„è®­ç»ƒçš„VQ-VAE)
vae:
  model_path: "/kaggle/input/vae-best-fid/adv_vqvae_best_fid.pth"
  in_channels: 3
  latent_dim: 256
  num_embeddings: 512
  beta: 0.25
  vq_ema_decay: 0.99
  groups: 32
  freeze: true
  # VAEè¾“å‡ºä¿¡æ¯
  latent_height: 32
  latent_width: 32
  latent_channels: 256

# æ•°æ®é›†é…ç½® - FIDä¼˜åŒ–
dataset:
  root_dir: "/kaggle/input/dataset"
  batch_size: 16  # ğŸ¯ é™ä½batch sizeï¼Œä¸“æ³¨è´¨é‡
  num_workers: 6
  val_split: 0.2
  image_size: 256

# æ‰©æ•£æ¨¡å‹é…ç½® - é«˜è´¨é‡è®¾ç½®
diffusion:
  timesteps: 1000
  noise_schedule: "scaled_linear"
  beta_start: 0.00085
  beta_end: 0.012
  scheduler_type: "ddim"
  clip_denoised: true
  prediction_type: "epsilon"

# U-Net ç½‘ç»œç»“æ„é…ç½® - å¼ºåŒ–ç‰ˆ
unet:
  # è¾“å…¥è¾“å‡ºç»´åº¦
  in_channels: 256
  out_channels: 256
  
  # ğŸ¯ å¤§æ¨¡å‹é…ç½® - ä¸“æ³¨å›¾åƒè´¨é‡
  model_channels: 320  # ğŸš€ æ¥è¿‘å®Œæ•´SDè§„æ¨¡
  num_res_blocks: 4    # ğŸš€ å¢åŠ ResNetå—æ·±åº¦
  attention_resolutions: [16, 8, 4, 2, 1]  # ğŸš€ å…¨å°ºåº¦æ³¨æ„åŠ›
  channel_mult: [1, 2, 4, 6, 8]  # ğŸš€ æ›´æ·±çš„é€šé“å±‚æ¬¡
  
  # å¼ºåŒ–æ³¨æ„åŠ›é…ç½®
  num_head_channels: 80  # ğŸš€ å¢å¤§æ³¨æ„åŠ›å¤´
  use_spatial_transformer: true
  transformer_depth: 3   # ğŸš€ ä¸‰å±‚transformer
  
  # æ¡ä»¶åµŒå…¥
  num_classes: 31
  use_fp16: false  # ğŸ¯ ä½¿ç”¨FP32æé«˜ç²¾åº¦
  
  # æ—¶é—´åµŒå…¥
  time_embed_dim: 1280  # 320 * 4

# è®­ç»ƒé…ç½® - é«˜è´¨é‡è®­ç»ƒ
training:
  epochs: 600  # ğŸ¯ å¤§å¹…å¢åŠ è®­ç»ƒè½®æ•°
  lr: 0.00005  # ğŸ¯ æ›´å°å­¦ä¹ ç‡ï¼Œç²¾ç»†è°ƒä¼˜
  weight_decay: 0.01
  warmup_steps: 5000  # ğŸ¯ æ›´é•¿warmup
  
  # æ¢¯åº¦ç´¯ç§¯ - å¤§æ¨¡å‹è®­ç»ƒ
  gradient_accumulation_steps: 4  # ğŸ¯ ç´¯ç§¯æ¢¯åº¦ç­‰æ•ˆbatch64
  
  # ä¿å­˜è®¾ç½®
  save_dir: "fid_optimized_models"
  save_interval: 50
  sample_interval: 1  # ğŸ¯ æ›´é¢‘ç¹ç”Ÿæˆæ ·æœ¬ç›‘æ§
  log_interval: 20
  
  # æŸå¤±é…ç½® - æ··åˆæŸå¤±
  loss_type: "l2"
  
  # Classifier-Free Guidance - å¼ºåŒ–
  use_cfg: true
  cfg_dropout_prob: 0.1  # ğŸ¯ é™ä½dropoutï¼Œæé«˜æ¡ä»¶éµå¾ª
  
  # ä¼˜åŒ–å™¨é…ç½®
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  
  # æ¢¯åº¦è£å‰ª
  grad_clip_norm: 0.5  # ğŸ¯ æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
  
  # è°ƒåº¦å™¨
  use_scheduler: true
  scheduler_type: "cosine"

# FIDè¯„ä¼°é…ç½® - é«˜ç²¾åº¦è¯„ä¼°
fid_evaluation:
  enabled: true
  eval_interval: 1  # ğŸš€ é™ä½è¯„ä¼°é¢‘ç‡ï¼š25->50 epoch
  num_samples: 1000  # ğŸš€ å‡å°‘æ ·æœ¬æ•°ï¼š5000->1000
  batch_size: 32     # ğŸš€ å¢å¤§batch sizeï¼š20->32  
  max_real_samples: 1000  # ğŸš€ å‡å°‘çœŸå®æ ·æœ¬æ•°ï¼š5000->1000
  save_best_fid_model: true

# æ¨ç†é…ç½® - é«˜è´¨é‡é‡‡æ ·
inference:
  num_inference_steps: 50   # ğŸš€ å¤§å¹…å‡å°‘ï¼š250->50æ­¥
  guidance_scale: 7.5       # ğŸš€ é™ä½CFGå¼ºåº¦ï¼š12.0->7.5
  eta: 0.0  # DDIMç¡®å®šæ€§é‡‡æ ·
  
  # é‡‡æ ·é…ç½®
  sample_batch_size: 8
  num_samples_per_class: 10
  
  # è¾“å‡ºè®¾ç½®
  output_dir: "fid_optimized_samples"
  save_format: "png"

# è®¾å¤‡é…ç½®
device: "auto"
mixed_precision: false  # ğŸ¯ ç¦ç”¨æ··åˆç²¾åº¦ï¼Œè¿½æ±‚è´¨é‡
use_ema: true
ema_decay: 0.9999

# æ—¥å¿—é…ç½®
logging:
  use_wandb: false
  project_name: "SD-LDM-FID-Optimized"
  experiment_name: "fid_target_20"
  log_dir: "fid_logs" 
