"""
æ•°æ®é›†é€‚é…å™¨æ¨¡å—
å°†VAEæ–‡ä»¶å¤¹ä¸‹çš„dataset.pyé€‚é…ä¸ºLDMè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
"""

import sys
import os
from typing import Dict, Tuple, Any
import torch
from torch.utils.data import DataLoader

# æ·»åŠ VAEè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

# å°è¯•å¯¼å…¥VAEçš„æ•°æ®åŠ è½½å™¨ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å†…ç½®ç‰ˆæœ¬
try:
    from dataset import build_dataloader as vae_build_dataloader
except ImportError:
    print("âš ï¸ VAE dataset module not found, using built-in dataloader")
    vae_build_dataloader = None

def build_dataloader(config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:
    """
    é€‚é…å™¨å‡½æ•°ï¼šå°†VAEçš„build_dataloaderé€‚é…ä¸ºLDMæ ¼å¼
    
    Args:
        config: LDMé…ç½®å­—å…¸
        
    Returns:
        (train_loader, val_loader): è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    """
    # æå–æ•°æ®é›†é…ç½®
    dataset_config = config.get('dataset', {})
    
    # å°†LDMé…ç½®è½¬æ¢ä¸ºVAE build_dataloaderçš„å‚æ•°
    vae_params = {
        'root_dir': dataset_config.get('root_dir', '/kaggle/input/dataset'),
        'batch_size': dataset_config.get('batch_size', 6),
        'num_workers': dataset_config.get('num_workers', 2),
        'shuffle_train': dataset_config.get('shuffle_train', True),
        'shuffle_val': False,  # éªŒè¯é›†ä¸éœ€è¦shuffle
        'val_split': dataset_config.get('val_split', 0.1),
        'random_state': 42
    }
    
    print(f"ğŸ”„ ä½¿ç”¨VAEæ•°æ®åŠ è½½å™¨")
    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {vae_params['root_dir']}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {vae_params['batch_size']}")
    print(f"ğŸ”€ éªŒè¯é›†æ¯”ä¾‹: {vae_params['val_split']}")
    
    if vae_build_dataloader is None:
        print(f"âŒ VAEæ•°æ®åŠ è½½å™¨ä¸å¯ç”¨")
        print(f"ğŸ”„ å›é€€ä½¿ç”¨åˆæˆæ•°æ®é›†...")
        return create_synthetic_dataloaders(config)
    
    try:
        # è°ƒç”¨VAEçš„build_dataloader
        train_loader, val_loader, train_size, val_size = vae_build_dataloader(**vae_params)
        
        # åˆ›å»ºåŒ…è£…çš„æ•°æ®åŠ è½½å™¨ä»¥é€‚é…LDMçš„æ•°æ®æ ¼å¼
        wrapped_train_loader = DataLoaderWrapper(train_loader)
        wrapped_val_loader = DataLoaderWrapper(val_loader)
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ:")
        print(f"  è®­ç»ƒé›†: {train_size}ä¸ªæ ·æœ¬")
        print(f"  éªŒè¯é›†: {val_size}ä¸ªæ ·æœ¬")
        
        return wrapped_train_loader, wrapped_val_loader
        
    except Exception as e:
        print(f"âŒ ä½¿ç”¨VAEæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        print(f"ğŸ”„ å›é€€ä½¿ç”¨åˆæˆæ•°æ®é›†...")
        
        # å›é€€åˆ°åˆæˆæ•°æ®é›†
        return create_synthetic_dataloaders(config)

class DataLoaderWrapper:
    """
    æ•°æ®åŠ è½½å™¨åŒ…è£…å™¨
    å°†VAEçš„æ•°æ®æ ¼å¼(image, label)è½¬æ¢ä¸ºLDMéœ€è¦çš„æ ¼å¼{'image': ..., 'label': ...}
    """
    
    def __init__(self, original_loader: DataLoader):
        self.original_loader = original_loader
        
        # å¤åˆ¶åŸå§‹loaderçš„å±æ€§
        self.dataset = original_loader.dataset
        self.batch_size = original_loader.batch_size
        self.num_workers = original_loader.num_workers
        self.pin_memory = original_loader.pin_memory
        self.shuffle = getattr(original_loader, 'shuffle', False)
    
    def __iter__(self):
        """è¿­ä»£å™¨ï¼Œè½¬æ¢æ•°æ®æ ¼å¼"""
        for batch in self.original_loader:
            # æ£€æŸ¥batchçš„ç±»å‹å’Œæ ¼å¼
            if isinstance(batch, (tuple, list)) and len(batch) == 2:
                # VAEæ ¼å¼: (images, labels) æˆ– [images, labels]
                images, labels = batch
                # è½¬æ¢ä¸ºLDMæ ¼å¼: {'image': images, 'label': labels}
                yield {
                    'image': images,
                    'label': labels
                }
            elif isinstance(batch, dict):
                # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥è¿”å›
                yield batch
            else:
                # å¤„ç†å…¶ä»–å¯èƒ½çš„æ ¼å¼
                print(f"âš ï¸ æœªçŸ¥çš„æ‰¹æ¬¡æ ¼å¼: {type(batch)}")
                if hasattr(batch, '__len__') and len(batch) >= 2:
                    # å°è¯•å–å‰ä¸¤ä¸ªå…ƒç´ 
                    yield {
                        'image': batch[0],
                        'label': batch[1]
                    }
                else:
                    raise ValueError(f"æ— æ³•å¤„ç†çš„æ‰¹æ¬¡æ ¼å¼: {type(batch)}")
    
    def __len__(self):
        return len(self.original_loader)

def create_synthetic_dataloaders(config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:
    """
    åˆ›å»ºåˆæˆæ•°æ®é›†ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
    """
    import torch
    from torch.utils.data import Dataset, DataLoader
    
    class SyntheticDataset(Dataset):
        def __init__(self, num_samples: int, num_classes: int):
            self.num_samples = num_samples
            self.num_classes = num_classes
        
        def __len__(self):
            return self.num_samples
        
        def __getitem__(self, idx):
            image = torch.rand(3, 256, 256)
            label = torch.randint(0, self.num_classes, (1,)).item()
            return {
                'image': image,
                'label': torch.tensor(label, dtype=torch.long)
            }
    
    dataset_config = config.get('dataset', {})
    batch_size = dataset_config.get('batch_size', 6)
    num_workers = dataset_config.get('num_workers', 2)
    num_classes = config['unet']['num_classes']
    
    # åˆ›å»ºåˆæˆæ•°æ®é›†
    train_dataset = SyntheticDataset(1000, num_classes)
    val_dataset = SyntheticDataset(200, num_classes)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    print(f"âœ… åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)}ä¸ªæ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_dataset)}ä¸ªæ ·æœ¬")
    print(f"  ç±»åˆ«æ•°: {num_classes}")
    
    return train_loader, val_loader

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    test_config = {
        'dataset': {
            'root_dir': '/kaggle/input/dataset',
            'batch_size': 4,
            'num_workers': 0,
            'val_split': 0.2,
            'shuffle_train': True
        },
        'unet': {
            'num_classes': 31
        }
    }
    
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†é€‚é…å™¨...")
    
    try:
        train_loader, val_loader = build_dataloader(test_config)
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        for batch in train_loader:
            print(f"âœ… æ‰¹æ¬¡æ ¼å¼æ­£ç¡®:")
            print(f"  å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
            print(f"  æ ‡ç­¾å½¢çŠ¶: {batch['label'].shape}")
            print(f"  æ ‡ç­¾å€¼: {batch['label']}")
            break
        
        print("âœ… æ•°æ®é›†é€‚é…å™¨æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        raise 
