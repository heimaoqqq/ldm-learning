"""
VAE-LDM æ¨ç†è„šæœ¬
ç”¨äºç”Ÿæˆå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ ·æœ¬
"""
import os
import sys
import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
from typing import List, Optional

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from vae_ldm import create_vae_ldm

class VAELDMInference:
    """VAE-LDMæ¨ç†å™¨"""
    
    def __init__(
        self,
        checkpoint_path: str,
        vae_checkpoint_path: str,
        device: str = None,
    ):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸš€ åˆå§‹åŒ–VAE-LDMæ¨¡å‹...")
        self.model = create_vae_ldm(
            vae_checkpoint_path=vae_checkpoint_path,
            device=self.device
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if checkpoint_path and os.path.exists(checkpoint_path):
            print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            self.model.load_checkpoint(checkpoint_path)
        else:
            print(f"âš ï¸  æ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {checkpoint_path}")
        
        self.model.eval()
        
        print("âœ… VAE-LDMæ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ!")
    
    @torch.no_grad()
    def generate_samples(
        self,
        num_samples: int,
        class_labels: Optional[List[int]] = None,
        use_ddim: bool = True,
        num_inference_steps: Optional[int] = 50,
        eta: float = 0.0,
        guidance_scale: float = 1.0,
        seed: Optional[int] = None,
        show_progress: bool = True
    ) -> torch.Tensor:
        """
        ç”Ÿæˆæ ·æœ¬
        
        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            class_labels: ç±»åˆ«æ ‡ç­¾åˆ—è¡¨
            use_ddim: æ˜¯å¦ä½¿ç”¨DDIMé‡‡æ ·
            num_inference_steps: æ¨ç†æ­¥æ•°
            eta: DDIMéšæœºæ€§å‚æ•°
            guidance_scale: åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å¼ºåº¦
            seed: éšæœºç§å­
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            generated_images: [num_samples, 3, H, W] ç”Ÿæˆçš„å›¾åƒ
        """
        if seed is not None:
            torch.manual_seed(seed)
            np.random.seed(seed)
        
        # å¤„ç†ç±»åˆ«æ ‡ç­¾
        if class_labels is not None:
            if len(class_labels) != num_samples:
                raise ValueError(f"class_labelsé•¿åº¦ ({len(class_labels)}) ä¸ num_samples ({num_samples}) ä¸åŒ¹é…")
            class_labels = torch.tensor(class_labels, dtype=torch.long, device=self.device)
        else:
            # éšæœºé€‰æ‹©ç±»åˆ«
            class_labels = torch.randint(0, self.model.unet.num_classes, (num_samples,), device=self.device)
        
        print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬...")
        if class_labels is not None:
            print(f"   ç±»åˆ«åˆ†å¸ƒ: {torch.bincount(class_labels).tolist()}")
        print(f"   é‡‡æ ·æ–¹æ³•: {'DDIM' if use_ddim else 'DDPM'}")
        print(f"   æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"   éšæœºæ€§å‚æ•°: {eta}")
        
        # ç”Ÿæˆæ ·æœ¬
        generated_images, _ = self.model.sample(
            num_samples=num_samples,
            class_labels=class_labels,
            use_ddim=use_ddim,
            eta=eta,
            return_intermediates=False
        )
        
        print("âœ… æ ·æœ¬ç”Ÿæˆå®Œæˆ!")
        
        return generated_images
    
    def save_samples(
        self,
        images: torch.Tensor,
        save_dir: str,
        prefix: str = "generated",
        format: str = "png"
    ):
        """
        ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬
        
        Args:
            images: [N, 3, H, W] å›¾åƒå¼ é‡
            save_dir: ä¿å­˜ç›®å½•
            prefix: æ–‡ä»¶åå‰ç¼€
            format: å›¾åƒæ ¼å¼
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # è½¬æ¢åˆ°CPUå¹¶å½’ä¸€åŒ–åˆ°[0,1]
        images = images.cpu()
        images = (images + 1) / 2  # ä»[-1,1]è½¬æ¢åˆ°[0,1]
        images = torch.clamp(images, 0, 1)
        
        print(f"ğŸ’¾ ä¿å­˜ {len(images)} ä¸ªæ ·æœ¬åˆ° {save_dir}")
        
        for i, img in enumerate(images):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_np = img.permute(1, 2, 0).numpy()
            img_pil = Image.fromarray((img_np * 255).astype(np.uint8))
            
            # ä¿å­˜
            filename = f"{prefix}_{i:04d}.{format}"
            filepath = os.path.join(save_dir, filename)
            img_pil.save(filepath)
        
        print(f"âœ… æ ·æœ¬ä¿å­˜å®Œæˆ: {save_dir}")
    
    def create_class_grid(
        self,
        num_samples_per_class: int = 4,
        max_classes: int = 16,
        use_ddim: bool = True,
        eta: float = 0.0,
        seed: Optional[int] = None
    ) -> torch.Tensor:
        """
        åˆ›å»ºç±»åˆ«ç½‘æ ¼å›¾åƒ
        
        Args:
            num_samples_per_class: æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
            max_classes: æœ€å¤§ç±»åˆ«æ•°
            use_ddim: æ˜¯å¦ä½¿ç”¨DDIM
            eta: DDIMéšæœºæ€§å‚æ•°
            seed: éšæœºç§å­
            
        Returns:
            grid_image: ç½‘æ ¼å›¾åƒ
        """
        if seed is not None:
            torch.manual_seed(seed)
            np.random.seed(seed)
        
        num_classes = min(max_classes, self.model.unet.num_classes)
        total_samples = num_classes * num_samples_per_class
        
        # åˆ›å»ºç±»åˆ«æ ‡ç­¾
        class_labels = []
        for class_id in range(num_classes):
            class_labels.extend([class_id] * num_samples_per_class)
        
        # ç”Ÿæˆæ ·æœ¬
        generated_images = self.generate_samples(
            num_samples=total_samples,
            class_labels=class_labels,
            use_ddim=use_ddim,
            eta=eta,
            show_progress=True
        )
        
        # åˆ›å»ºç½‘æ ¼
        return self._make_grid(generated_images, nrow=num_samples_per_class)
    
    def _make_grid(self, images: torch.Tensor, nrow: int = 8, padding: int = 2) -> torch.Tensor:
        """åˆ›å»ºå›¾åƒç½‘æ ¼"""
        # å½’ä¸€åŒ–åˆ°[0,1]
        images = (images + 1) / 2
        images = torch.clamp(images, 0, 1)
        
        B, C, H, W = images.shape
        ncol = (B + nrow - 1) // nrow
        
        # åˆ›å»ºç½‘æ ¼ç”»å¸ƒ
        grid_h = ncol * H + (ncol + 1) * padding
        grid_w = nrow * W + (nrow + 1) * padding
        grid = torch.ones((C, grid_h, grid_w))
        
        # å¡«å……å›¾åƒ
        for idx in range(B):
            row = idx // nrow
            col = idx % nrow
            
            y_start = row * H + (row + 1) * padding
            y_end = y_start + H
            x_start = col * W + (col + 1) * padding
            x_end = x_start + W
            
            grid[:, y_start:y_end, x_start:x_end] = images[idx]
        
        return grid
    
    def interpolate_between_classes(
        self,
        class_a: int,
        class_b: int,
        num_steps: int = 8,
        use_ddim: bool = True,
        eta: float = 0.0,
        seed: Optional[int] = None
    ) -> torch.Tensor:
        """
        åœ¨ä¸¤ä¸ªç±»åˆ«ä¹‹é—´è¿›è¡Œæ’å€¼
        
        Args:
            class_a: èµ·å§‹ç±»åˆ«
            class_b: ç»“æŸç±»åˆ«
            num_steps: æ’å€¼æ­¥æ•°
            use_ddim: æ˜¯å¦ä½¿ç”¨DDIM
            eta: DDIMéšæœºæ€§å‚æ•°
            seed: éšæœºç§å­
            
        Returns:
            interpolated_images: æ’å€¼å›¾åƒåºåˆ—
        """
        if seed is not None:
            torch.manual_seed(seed)
            np.random.seed(seed)
        
        print(f"ğŸ”„ åœ¨ç±»åˆ« {class_a} å’Œ {class_b} ä¹‹é—´è¿›è¡Œæ’å€¼ ({num_steps} æ­¥)")
        
        # ç”Ÿæˆç«¯ç‚¹æ ·æœ¬
        end_samples = self.generate_samples(
            num_samples=2,
            class_labels=[class_a, class_b],
            use_ddim=use_ddim,
            eta=eta,
            show_progress=False
        )
        
        # ç®€å•çº¿æ€§æ’å€¼ï¼ˆåœ¨å›¾åƒç©ºé—´ï¼‰
        interpolated = []
        for i in range(num_steps):
            alpha = i / (num_steps - 1)
            interpolated_img = (1 - alpha) * end_samples[0] + alpha * end_samples[1]
            interpolated.append(interpolated_img)
        
        return torch.stack(interpolated)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VAE-LDMæ¨ç†')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--vae_checkpoint', type=str, required=True,
                       help='VAEæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='generated_samples',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num_samples', type=int, default=16,
                       help='ç”Ÿæˆæ ·æœ¬æ•°é‡')
    parser.add_argument('--class_labels', type=int, nargs='*', default=None,
                       help='æŒ‡å®šç±»åˆ«æ ‡ç­¾')
    parser.add_argument('--ddim_steps', type=int, default=50,
                       help='DDIMé‡‡æ ·æ­¥æ•°')
    parser.add_argument('--eta', type=float, default=0.0,
                       help='DDIMéšæœºæ€§å‚æ•°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--create_grid', action='store_true',
                       help='åˆ›å»ºç±»åˆ«ç½‘æ ¼')
    parser.add_argument('--samples_per_class', type=int, default=4,
                       help='æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ï¼ˆç½‘æ ¼æ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å™¨
    inference = VAELDMInference(
        checkpoint_path=args.checkpoint,
        vae_checkpoint_path=args.vae_checkpoint
    )
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    if args.create_grid:
        # åˆ›å»ºç±»åˆ«ç½‘æ ¼
        print("ğŸ¨ åˆ›å»ºç±»åˆ«ç½‘æ ¼...")
        grid_image = inference.create_class_grid(
            num_samples_per_class=args.samples_per_class,
            use_ddim=True,
            eta=args.eta,
            seed=args.seed
        )
        
        # ä¿å­˜ç½‘æ ¼
        grid_pil = Image.fromarray((grid_image.permute(1, 2, 0).numpy() * 255).astype(np.uint8))
        grid_path = os.path.join(args.output_dir, 'class_grid.png')
        grid_pil.save(grid_path)
        print(f"âœ… ç±»åˆ«ç½‘æ ¼ä¿å­˜è‡³: {grid_path}")
    
    else:
        # ç”Ÿæˆæ™®é€šæ ·æœ¬
        generated_images = inference.generate_samples(
            num_samples=args.num_samples,
            class_labels=args.class_labels,
            use_ddim=True,
            num_inference_steps=args.ddim_steps,
            eta=args.eta,
            seed=args.seed
        )
        
        # ä¿å­˜æ ·æœ¬
        inference.save_samples(
            images=generated_images,
            save_dir=args.output_dir,
            prefix="sample"
        )
        
        # åˆ›å»ºé¢„è§ˆç½‘æ ¼
        grid_image = inference._make_grid(generated_images, nrow=4)
        grid_pil = Image.fromarray((grid_image.permute(1, 2, 0).numpy() * 255).astype(np.uint8))
        grid_path = os.path.join(args.output_dir, 'preview_grid.png')
        grid_pil.save(grid_path)
        print(f"âœ… é¢„è§ˆç½‘æ ¼ä¿å­˜è‡³: {grid_path}")

if __name__ == "__main__":
    main() 
