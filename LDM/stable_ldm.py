"""
åŸºäºStable Diffusionçš„è½»é‡åŒ–æ½œåœ¨æ‰©æ•£æ¨¡å‹
ä¸“é—¨é€‚é…æ‚¨çš„VQ-VAEå’Œ31ä¸ªç±»åˆ«æ¡ä»¶ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from typing import Dict, Tuple, Optional, Any, Union
from tqdm import tqdm

# æ·»åŠ VAEæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from adv_vq_vae import AdvVQVAE
from simple_enhanced_unet import SimpleEnhancedUNet, create_simple_enhanced_unet  # ğŸš€ å¢å¼ºç‰ˆUNet
from scheduler import DDPMScheduler, DDIMScheduler

class EMA:
    """æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
    def __init__(self, model: nn.Module, decay: float = 0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                # ç¡®ä¿shadowå‚æ•°åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if self.shadow[name].device != param.device:
                    self.shadow[name] = self.shadow[name].to(param.device)
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                # ç¡®ä¿shadowå‚æ•°åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if self.shadow[name].device != param.device:
                    self.shadow[name] = self.shadow[name].to(param.device)
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

class StableLDM(nn.Module):
    """
    åŸºäºStable Diffusionçš„è½»é‡åŒ–æ½œåœ¨æ‰©æ•£æ¨¡å‹
    """
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–VAE (é¢„è®­ç»ƒï¼Œå†»ç»“å‚æ•°)
        self.vae = self._init_vae(config['vae'])
        
        # åˆå§‹åŒ–U-Net - ğŸš€ æ”¯æŒç®€åŒ–å¢å¼ºç‰ˆ
        unet_type = config['unet'].get('type', 'simple')  # é»˜è®¤ä½¿ç”¨ç®€å•ç‰ˆæœ¬
        
        if unet_type == 'simple_enhanced':
            print("ğŸš€ ä½¿ç”¨ç®€åŒ–å¢å¼ºç‰ˆU-Net (åŒ…å«å¤šå±‚Transformer)")
            self.unet = create_simple_enhanced_unet(config)
        else:
            print("ğŸ“‹ ä½¿ç”¨ç®€åŒ–å¢å¼ºç‰ˆU-Net (åŒ…å«å¤šå±‚Transformer)")
            # æ‰€æœ‰æƒ…å†µä¸‹éƒ½ä½¿ç”¨å¢å¼ºç‰ˆUNet
            self.unet = create_simple_enhanced_unet(config)
        
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        scheduler_config = config['diffusion']
        scheduler_type = scheduler_config.get('scheduler_type', 'ddpm').lower()
        
        if scheduler_type == 'ddim':
            self.scheduler = DDIMScheduler(
                num_train_timesteps=scheduler_config['timesteps'],
                beta_start=scheduler_config['beta_start'],
                beta_end=scheduler_config['beta_end'],
                beta_schedule=scheduler_config['noise_schedule'],
            )
            print(f"âœ… ä½¿ç”¨DDIMè°ƒåº¦å™¨")
        else:
            self.scheduler = DDPMScheduler(
                num_train_timesteps=scheduler_config['timesteps'],
                beta_start=scheduler_config['beta_start'],
                beta_end=scheduler_config['beta_end'],
                beta_schedule=scheduler_config['noise_schedule']
            )
            print(f"âœ… ä½¿ç”¨DDPMè°ƒåº¦å™¨")
        
        # CFGé…ç½®
        self.use_cfg = config['training'].get('use_cfg', False)
        self.cfg_dropout_prob = config['training'].get('cfg_dropout_prob', 0.15)
        
        # EMA
        if config.get('use_ema', False):
            self.ema = EMA(self.unet, config.get('ema_decay', 0.9999))
            print(f"âœ… å¯ç”¨EMAï¼Œè¡°å‡ç‡: {config.get('ema_decay', 0.9999)}")
        else:
            self.ema = None
        
    def _init_vae(self, vae_config: Dict[str, Any]) -> AdvVQVAE:
        """åˆå§‹åŒ–VAEå¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        vae = AdvVQVAE(
            in_channels=vae_config['in_channels'],
            latent_dim=vae_config['latent_dim'],
            num_embeddings=vae_config['num_embeddings'],
            beta=vae_config['beta'],
            decay=vae_config['vq_ema_decay'],
            groups=vae_config['groups']
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if 'model_path' in vae_config and vae_config['model_path']:
            try:
                print(f"ğŸ”„ åŠ è½½VAEé¢„è®­ç»ƒæ¨¡å‹: {vae_config['model_path']}")
                state_dict = torch.load(vae_config['model_path'], map_location='cpu')
                vae.load_state_dict(state_dict)
                print("âœ… VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # å†»ç»“VAEå‚æ•°
        if vae_config.get('freeze', True):
            for param in vae.parameters():
                param.requires_grad = False
            vae.eval()
            print("â„ï¸ VAEå‚æ•°å·²å†»ç»“")
        
        return vae
    
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´"""
        with torch.no_grad():
            # è¾“å…¥é¢„å¤„ç†ï¼šç¡®ä¿åœ¨[0,1]èŒƒå›´ï¼Œè½¬æ¢åˆ°[-1,1]
            images = torch.clamp(images, 0.0, 1.0)
            images = images * 2.0 - 1.0
            
            try:
                # VAEç¼–ç 
                encoded = self.vae.encoder(images)
                encoded = torch.clamp(encoded, -5.0, 5.0)
                
                # VQé‡åŒ–
                quantized, _, _ = self.vae.vq(encoded)
                quantized = torch.clamp(quantized, -3.0, 3.0)
                
                return quantized
                
            except Exception as e:
                print(f"âš ï¸ VAEç¼–ç å¤±è´¥: {e}")
                # è¿”å›å®‰å…¨çš„éšæœºæ½œåœ¨è¡¨ç¤º
                batch_size = images.shape[0]
                latent_shape = (batch_size, 256, 32, 32)
                return torch.randn(latent_shape, device=images.device) * 0.5
    
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ"""
        with torch.no_grad():
            # èŒƒå›´æ£€æŸ¥å’Œè£å‰ª
            latents = torch.clamp(latents, -5.0, 5.0)
            
            # VAEè§£ç 
            decoded = self.vae.decoder(latents)
            decoded = torch.clamp(decoded, -1.0, 1.0)
            
            # è½¬æ¢åˆ°[0,1]èŒƒå›´
            images = (decoded + 1.0) / 2.0
            images = torch.clamp(images, 0.0, 1.0)
            
            return images
    
    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒå‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾
            
        Returns:
            åŒ…å«æŸå¤±ä¿¡æ¯çš„å­—å…¸
        """
        batch_size = images.shape[0]
        
        # è¾“å…¥æ£€æŸ¥å’Œé¢„å¤„ç†
        images = torch.clamp(images, 0.0, 1.0)
        
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        latents = self.encode_to_latent(images)
        
        # é‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.num_train_timesteps, 
            (batch_size,), device=images.device
        )
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noise = torch.clamp(noise, -2.0, 2.0)
        
        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)
        
        # CFGè®­ç»ƒï¼šéšæœºä¸¢å¼ƒç±»åˆ«æ ‡ç­¾
        if self.use_cfg and class_labels is not None:
            cfg_mask = torch.rand(batch_size, device=images.device) < self.cfg_dropout_prob
            class_labels = class_labels.clone()
            class_labels[cfg_mask] = -1  # æ— æ¡ä»¶æ ‡ç­¾
        
        # U-Neté¢„æµ‹å™ªå£°
        predicted_noise = self.unet(noisy_latents, timesteps, class_labels)
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(predicted_noise, noise, reduction='mean')
        
        # æ£€æŸ¥æŸå¤±å¥åº·çŠ¶å†µ
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±ï¼Œä½¿ç”¨å®‰å…¨å€¼")
            loss = torch.tensor(1.0, device=images.device, requires_grad=True)
        
        # æ›´æ–°EMA
        if self.ema is not None and self.training:
            self.ema.update()
        
        return {
            'loss': loss,
            'predicted_noise': predicted_noise.detach(),
            'target_noise': noise.detach()
        }
    
    @torch.no_grad()
    def sample(
        self, 
        batch_size: int, 
        class_labels: Optional[torch.Tensor] = None,
        num_inference_steps: int = 75,
        guidance_scale: float = 8.5,
        eta: float = 0.3,
        use_ema: bool = False,
        verbose: bool = True
    ) -> torch.Tensor:
        """
        DDIM/DDPMé‡‡æ ·ç”Ÿæˆå›¾åƒ
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            class_labels: [batch_size] ç±»åˆ«æ ‡ç­¾
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: CFGå¼•å¯¼å¼ºåº¦
            eta: DDIMéšæœºæ€§å‚æ•°
            use_ema: æ˜¯å¦ä½¿ç”¨EMAæƒé‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒ [batch_size, 3, H, W]
        """
        device = self.device
        
        # ä½¿ç”¨EMAæƒé‡ï¼ˆå¦‚æœå¯ç”¨ä¸”è¯·æ±‚ï¼‰
        if use_ema and self.ema is not None:
            self.ema.apply_shadow()
        
        try:
            if verbose:
                print(f"ğŸ”„ å¼€å§‹é‡‡æ ·: æ­¥æ•°={num_inference_steps}, CFG={guidance_scale}, EMA={'âœ“' if use_ema and self.ema else 'âœ—'}")
            
            # åˆå§‹åŒ–éšæœºå™ªå£°
            latents = torch.randn(batch_size, 256, 32, 32, device=device)
            
            # è®¾ç½®æ¨ç†è°ƒåº¦å™¨
            self.scheduler.set_timesteps(num_inference_steps, device=device)
            
            # DDIMé‡‡æ ·å¾ªç¯
            for i, t in enumerate(self.scheduler.timesteps):
                t_batch = t.unsqueeze(0).repeat(batch_size).to(device)
                
                if self.use_cfg and class_labels is not None and guidance_scale > 1.0:
                    # CFG: é¢„æµ‹æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°
                    latent_model_input = torch.cat([latents] * 2)
                    t_input = torch.cat([t_batch] * 2)
                    
                    class_input = torch.cat([
                        class_labels,
                        torch.full_like(class_labels, -1)  # æ— æ¡ä»¶
                    ])
                    
                    noise_pred = self.unet(latent_model_input, t_input, class_input)
                    noise_pred_cond, noise_pred_uncond = noise_pred.chunk(2)
                    
                    # CFGå¼•å¯¼
                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
                else:
                    # æ ‡å‡†é¢„æµ‹
                    noise_pred = self.unet(latents, t_batch, class_labels)
                
                # è°ƒåº¦å™¨æ­¥éª¤
                if hasattr(self.scheduler, 'step'):
                    if isinstance(self.scheduler, DDIMScheduler):
                        result = self.scheduler.step(noise_pred, t, latents, eta=eta)
                    else:
                        result = self.scheduler.step(noise_pred, t, latents)
                    
                    latents = result.prev_sample if hasattr(result, 'prev_sample') else result[0]
                else:
                    # ç®€å•çš„DDIMæ­¥éª¤
                    alpha_t = self.scheduler.alphas_cumprod[t].to(device)
                    alpha_prev = self.scheduler.alphas_cumprod[t-1].to(device) if t > 0 else torch.tensor(1.0, device=device)
                    
                    pred_x0 = (latents - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
                    pred_x0 = torch.clamp(pred_x0, -3.0, 3.0)
                    
                    latents = torch.sqrt(alpha_prev) * pred_x0 + torch.sqrt(1 - alpha_prev) * noise_pred
            
            # è§£ç åˆ°å›¾åƒç©ºé—´
            images = self.decode_from_latent(latents)
            
            if verbose:
                final_min, final_max = images.min().item(), images.max().item()
                final_mean = images.mean().item()
                print(f"âœ… é‡‡æ ·å®Œæˆ: èŒƒå›´[{final_min:.3f}, {final_max:.3f}], å‡å€¼{final_mean:.3f}")
            
            return images
            
        finally:
            # æ¢å¤åŸå§‹æƒé‡
            if use_ema and self.ema is not None:
                self.ema.restore()
    
    def get_loss_dict(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è·å–æŸå¤±å­—å…¸ (å…¼å®¹è®­ç»ƒå¾ªç¯)"""
        images = batch['image']
        class_labels = batch.get('label', None)
        
        return self(images, class_labels)

def count_parameters(model: nn.Module) -> int:
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def create_stable_ldm(config: Dict[str, Any]) -> StableLDM:
    """åˆ›å»ºStableLDMæ¨¡å‹"""
    model = StableLDM(config)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = count_parameters(model.unet)
    print(f"ğŸ”¥ è½»é‡åŒ–U-Netå‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
    
    vae_params = count_parameters(model.vae)
    print(f"â„ï¸ VAEå‚æ•°é‡: {vae_params:,} ({vae_params/1e6:.1f}M) [å†»ç»“]")
    
    print(f"ğŸš€ æ€»è®­ç»ƒå‚æ•°: {total_params:,} ({total_params/1e6:.1f}M)")
    
    return model 
