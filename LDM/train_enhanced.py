"""
ğŸš€ å¢å¼ºç‰ˆLDMè®­ç»ƒè„šæœ¬
ä½¿ç”¨åŒ…å«çœŸæ­£Transformerçš„ç®€åŒ–å¢å¼ºç‰ˆU-Net
é›†æˆå®Œæ•´çš„æŒ‡æ ‡è¯„ä¼°ç³»ç»Ÿ
"""

import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import sys
import os
import time
from tqdm import tqdm
import numpy as np
from datetime import datetime

# æ·»åŠ VAEæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from stable_ldm import create_stable_ldm
from fid_evaluation import FIDEvaluator
from metrics import DiffusionMetrics, denormalize_for_metrics
from dataset_adapter import build_dataloader

class EnhancedTrainer:
    """å¢å¼ºç‰ˆLDMè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_stable_ldm(self.config).to(self.device)
        
        # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
        self.metrics_calculator = DiffusionMetrics(self.device)
        
        # åˆ›å»ºFIDè¯„ä¼°å™¨
        self.fid_evaluator = FIDEvaluator(self.device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.unet.parameters(),
            lr=self.config['training']['lr'],
            betas=(self.config['training']['beta1'], self.config['training']['beta2']),
            eps=self.config['training']['eps'],
            weight_decay=self.config['training']['weight_decay']
        )
        
        # æ¢¯åº¦ç´¯ç§¯è®¾ç½®
        self.gradient_accumulation_steps = self.config['training'].get('gradient_accumulation_steps', 1)
        
        # æ—©åœè®¾ç½®
        self.best_fid = float('inf')
        self.patience_counter = 0
        self.early_stopping_patience = self.config['training'].get('early_stopping_patience', 50)
        
        # æ—¥å¿—å­˜å‚¨
        self.training_logs = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = self.config['training']['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
        
    def train_epoch(self, dataloader: DataLoader, epoch: int) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = []
        epoch_metrics = {
            'noise_mse': [],
            'noise_mae': [],
            'noise_psnr': [],
            'noise_cosine_similarity': [],
            'noise_correlation': []
        }
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # å‰å‘ä¼ æ’­ - ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            images = batch['image'].to(self.device)
            labels = batch.get('label', None)
            if labels is not None:
                labels = labels.to(self.device)
            
            loss_dict = self.model(images, labels)
            
            loss = loss_dict['loss'] / self.gradient_accumulation_steps
            
            # è®¡ç®—å™ªå£°é¢„æµ‹æŒ‡æ ‡
            noise_metrics = self.metrics_calculator.calculate_all_metrics(
                loss_dict['predicted_noise'],
                loss_dict['target_noise']
            )
            
            # æ›´æ–°æŒ‡æ ‡
            for key in epoch_metrics:
                if key in noise_metrics:
                    epoch_metrics[key].append(noise_metrics[key])
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if 'grad_clip_norm' in self.config['training']:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.unet.parameters(), 
                        self.config['training']['grad_clip_norm']
                    )
                
                self.optimizer.step()
                self.optimizer.zero_grad()
                
                # æ›´æ–°EMA
                if hasattr(self.model, 'ema') and self.model.ema is not None:
                    self.model.ema.update()
            
            epoch_losses.append(loss.item() * self.gradient_accumulation_steps)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{epoch_losses[-1]:.4f}',
                'Noise MSE': f'{noise_metrics.get("noise_mse", 0):.4f}',
                'Noise PSNR': f'{noise_metrics.get("noise_psnr", 0):.2f}',
                'Cosine Sim': f'{noise_metrics.get("noise_cosine_similarity", 0):.3f}'
            })
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_metrics = {
            'epoch': epoch,
            'loss': np.mean(epoch_losses),
            'noise_mse': np.mean(epoch_metrics['noise_mse']),
            'noise_mae': np.mean(epoch_metrics['noise_mae']),
            'noise_psnr': np.mean(epoch_metrics['noise_psnr']),
            'noise_cosine_similarity': np.mean(epoch_metrics['noise_cosine_similarity']),
            'noise_correlation': np.mean(epoch_metrics['noise_correlation'])
        }
        
        return avg_metrics
    
    def evaluate_fid(self, epoch: int) -> float:
        """è¯„ä¼°FIDåˆ†æ•°"""
        try:
            self.model.eval()
            
            # ä½¿ç”¨EMAæƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self.model, 'ema') and self.model.ema is not None:
                self.model.ema.apply_shadow()
            
            fid_score = self.fid_evaluator.evaluate_model(
                self.model,
                num_samples=self.config['fid_evaluation']['num_samples'],
                batch_size=self.config['fid_evaluation']['batch_size'],
                num_classes=self.config['unet']['num_classes'],
                num_inference_steps=self.config['inference']['num_inference_steps'],
                guidance_scale=self.config['inference']['guidance_scale'],
                eta=self.config['inference']['eta']
            )
            
            # æ¢å¤åŸå§‹æƒé‡
            if hasattr(self.model, 'ema') and self.model.ema is not None:
                self.model.ema.restore()
            
            return fid_score
            
        except Exception as e:
            print(f"âš ï¸ FIDè¯„ä¼°å¤±è´¥: {e}")
            return float('inf')
    
    def generate_samples(self, epoch: int, num_samples: int = 16):
        """ç”Ÿæˆæ ·æœ¬å¹¶è®¡ç®—ISåˆ†æ•°"""
        try:
            self.model.eval()
            
            with torch.no_grad():
                # ç”Ÿæˆæ ·æœ¬
                class_labels = torch.randint(0, self.config['unet']['num_classes'], 
                                           (num_samples,), device=self.device)
                
                generated_images = self.model.sample(
                    batch_size=num_samples,
                    class_labels=class_labels,
                    num_inference_steps=self.config['inference']['num_inference_steps'],
                    guidance_scale=self.config['inference']['guidance_scale'],
                    eta=self.config['inference']['eta'],
                    use_ema=True,
                    verbose=False
                )
                
                # è®¡ç®—ISåˆ†æ•°
                is_mean, is_std = self.metrics_calculator.inception_calculator.calculate_inception_score(
                    generated_images.cpu()
                )
                
                # ä¿å­˜æ ·æœ¬
                if epoch % self.config['training']['sample_interval'] == 0:
                    import torchvision
                    sample_path = os.path.join(self.save_dir, f'samples_epoch_{epoch}.png')
                    torchvision.utils.save_image(
                        generated_images, sample_path,
                        nrow=4, normalize=True, value_range=(0, 1)
                    )
                    print(f"ğŸ“¸ æ ·æœ¬å·²ä¿å­˜: {sample_path}")
                
                return is_mean, is_std
                
        except Exception as e:
            print(f"âš ï¸ æ ·æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return None, None
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆLDMè®­ç»ƒ")
        print("=" * 80)
        print("ğŸ“Š æ¨¡å‹ç‰¹ç‚¹:")
        print("  - ç®€åŒ–å¢å¼ºç‰ˆU-Net (115.8Må‚æ•°)")
        print("  - 6ä¸ªæ³¨æ„åŠ›å±‚ Ã— 2å±‚Transformer = 12ä¸ªTransformerå—")
        print("  - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç©ºé—´ç†è§£èƒ½åŠ›")
        print("  - P100æ˜¾å­˜å‹å¥½è®¾è®¡")
        print("  - é›†æˆå®Œæ•´æŒ‡æ ‡è¯„ä¼°ï¼šISã€å™ªå£°ç²¾åº¦ã€FIDç­‰")
        print("=" * 80)
        
        # æ„å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = build_dataloader(self.config)
        
        # é¢„è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾ï¼ˆç”¨äºFIDï¼‰
        print("ğŸ”„ é¢„è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾...")
        self.fid_evaluator.compute_real_features(val_loader)
        
        start_time = time.time()
        
        for epoch in range(1, self.config['training']['epochs'] + 1):
            # è®­ç»ƒä¸€ä¸ªepoch
            epoch_metrics = self.train_epoch(train_loader, epoch)
            
            # ç”Ÿæˆæ ·æœ¬å¹¶è®¡ç®—IS
            is_mean, is_std = self.generate_samples(epoch)
            if is_mean is not None:
                epoch_metrics['inception_score_mean'] = is_mean
                epoch_metrics['inception_score_std'] = is_std
            
            # FIDè¯„ä¼°
            if epoch % self.config['fid_evaluation']['eval_interval'] == 0:
                fid_score = self.evaluate_fid(epoch)
                epoch_metrics['fid_score'] = fid_score
                
                # æ—©åœæ£€æŸ¥
                if fid_score < self.best_fid:
                    self.best_fid = fid_score
                    self.patience_counter = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_path = os.path.join(self.save_dir, 'best_fid_model.pth')
                    torch.save(self.model.state_dict(), best_model_path)
                    print(f"ğŸ† æ–°çš„æœ€ä½³FID: {fid_score:.2f}ï¼Œæ¨¡å‹å·²ä¿å­˜")
                else:
                    self.patience_counter += 1
                    
                if self.patience_counter >= self.early_stopping_patience:
                    print(f"â¹ï¸ æ—©åœè§¦å‘ï¼Œ{self.early_stopping_patience}ä¸ªepochæ— æ”¹å–„")
                    break
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if epoch % self.config['training']['save_interval'] == 0:
                checkpoint_path = os.path.join(self.save_dir, f'checkpoint_epoch_{epoch}.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_fid': self.best_fid,
                    'config': self.config
                }, checkpoint_path)
                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
            
            # è®°å½•æ—¥å¿—
            self.training_logs.append(epoch_metrics)
            
            # æ‰“å°è¯¦ç»†æŒ‡æ ‡
            print(f"\nğŸ“Š Epoch {epoch} æŒ‡æ ‡æ€»ç»“:")
            print(f"  è®­ç»ƒæŸå¤±: {epoch_metrics['loss']:.4f}")
            print(f"  å™ªå£°MSE: {epoch_metrics['noise_mse']:.4f}")
            print(f"  å™ªå£°PSNR: {epoch_metrics['noise_psnr']:.2f}dB")
            print(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {epoch_metrics['noise_cosine_similarity']:.3f}")
            print(f"  ç›¸å…³ç³»æ•°: {epoch_metrics['noise_correlation']:.3f}")
            if 'inception_score_mean' in epoch_metrics:
                print(f"  ISåˆ†æ•°: {epoch_metrics['inception_score_mean']:.2f} Â± {epoch_metrics['inception_score_std']:.2f}")
            if 'fid_score' in epoch_metrics:
                print(f"  FIDåˆ†æ•°: {epoch_metrics['fid_score']:.2f}")
            print("-" * 60)
        
        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time/3600:.2f}å°æ—¶")
        print(f"ğŸ† æœ€ä½³FIDåˆ†æ•°: {self.best_fid:.2f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ—¥å¿—
        final_model_path = os.path.join(self.save_dir, 'final_model.pth')
        torch.save(self.model.state_dict(), final_model_path)
        
        import json
        log_path = os.path.join(self.save_dir, 'training_logs.json')
        with open(log_path, 'w') as f:
            json.dump(self.training_logs, f, indent=2)
        
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}")

def main(config_path: str = "config_enhanced_unet.yaml"):
    """ä¸»å‡½æ•°"""
    
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # éªŒè¯é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if config['unet']['type'] != 'simple_enhanced':
            print("âŒ é…ç½®æ–‡ä»¶ä¸æ˜¯å¢å¼ºç‰ˆç±»å‹")
            return
            
        print(f"âœ… é…ç½®éªŒè¯æˆåŠŸ: {config['unet']['type']}")
        print(f"âœ… Transformeræ·±åº¦: {config['unet']['transformer_depth']}")
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = EnhancedTrainer(config_path)
    trainer.train()

if __name__ == "__main__":
    main() 
