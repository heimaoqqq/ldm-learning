"""
è®¡ç®—å¾®è°ƒåVAEçš„æœ€ä½³ç¼©æ”¾å› å­
å•ç‹¬è¿è¡Œæ­¤è„šæœ¬æ¥åˆ†æVAEçš„æ½œå˜é‡åˆ†å¸ƒå¹¶è®¡ç®—æœ€ä½³ç¼©æ”¾å› å­
"""
import os
import sys
import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import json
from typing import List, Tuple, Optional, Union

# è®¾ç½®è®¾å¤‡
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== VAEæ¨¡å‹å®šä¹‰ ====================
# ç›´æ¥åœ¨è„šæœ¬ä¸­å®šä¹‰VAEæ¨¡å‹ï¼Œé¿å…å¯¼å…¥é—®é¢˜

class ResnetBlock(nn.Module):
    def __init__(self, in_channels, out_channels=None, dropout=0.0, groups=32):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        
        self.norm1 = nn.GroupNorm(groups, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
        
        self.norm2 = nn.GroupNorm(groups, out_channels)
        self.dropout = nn.Dropout(dropout)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        
        if self.in_channels != self.out_channels:
            self.nin_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)
        else:
            self.nin_shortcut = None

    def forward(self, x):
        h = x
        h = self.norm1(h)
        h = torch.nn.functional.silu(h)
        h = self.conv1(h)

        h = self.norm2(h)
        h = torch.nn.functional.silu(h)
        h = self.dropout(h)
        h = self.conv2(h)

        if self.nin_shortcut is not None:
            x = self.nin_shortcut(x)

        return x + h

class Downsample(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        return self.conv(x)

class Upsample(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode='nearest')
        return self.conv(x)

class DownEncoderBlock2D(nn.Module):
    def __init__(self, in_channels, out_channels, num_layers=1, add_downsample=True):
        super().__init__()
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            in_ch = in_channels if i == 0 else out_channels
            self.resnets.append(ResnetBlock(in_ch, out_channels))
        
        self.downsamplers = None
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample(out_channels)])

    def forward(self, hidden_states):
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        
        return hidden_states

class UpDecoderBlock2D(nn.Module):
    def __init__(self, in_channels, out_channels, num_layers=1, add_upsample=True):
        super().__init__()
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            in_ch = in_channels if i == 0 else out_channels
            self.resnets.append(ResnetBlock(in_ch, out_channels))
        
        self.upsamplers = None
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample(out_channels)])

    def forward(self, hidden_states):
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        
        return hidden_states

class DiagonalGaussianDistribution:
    def __init__(self, parameters, deterministic=False):
        self.parameters = parameters
        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)
        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)
        self.deterministic = deterministic
        self.std = torch.exp(0.5 * self.logvar)
        self.var = torch.exp(self.logvar)

    def sample(self):
        if self.deterministic:
            return self.mean
        else:
            return self.mean + self.std * torch.randn_like(self.mean)

    def kl(self, other=None):
        if self.deterministic:
            return torch.Tensor([0.])
        else:
            if other is None:
                return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])
            else:
                return 0.5 * torch.sum(
                    torch.pow(self.mean - other.mean, 2) / other.var + 
                    self.var / other.var - 1.0 - self.logvar + other.logvar,
                    dim=[1, 2, 3])

    def mode(self):
        return self.mean

class Encoder(nn.Module):
    def __init__(
        self,
        in_channels=3,
        out_channels=4,
        down_block_types=["DownEncoderBlock2D"],
        block_out_channels=[128],
        layers_per_block=2,
        norm_num_groups=32,
        double_z=True,
    ):
        super().__init__()
        self.layers_per_block = layers_per_block

        self.conv_in = nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)

        self.down_blocks = nn.ModuleList([])

        # down
        output_channel = block_out_channels[0]
        for i, down_block_type in enumerate(down_block_types):
            input_channel = output_channel
            output_channel = block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1

            down_block = DownEncoderBlock2D(
                in_channels=input_channel,
                out_channels=output_channel,
                num_layers=self.layers_per_block,
                add_downsample=not is_final_block,
            )
            self.down_blocks.append(down_block)

        # mid
        self.mid_block = ResnetBlock(block_out_channels[-1], block_out_channels[-1])

        # end
        self.conv_norm_out = nn.GroupNorm(norm_num_groups, block_out_channels[-1])
        conv_out_channels = 2 * out_channels if double_z else out_channels
        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)

    def forward(self, x):
        sample = x
        sample = self.conv_in(sample)

        for down_block in self.down_blocks:
            sample = down_block(sample)

        sample = self.mid_block(sample)

        sample = self.conv_norm_out(sample)
        sample = torch.nn.functional.silu(sample)
        sample = self.conv_out(sample)

        return sample

class Decoder(nn.Module):
    def __init__(
        self,
        in_channels=4,
        out_channels=3,
        up_block_types=["UpDecoderBlock2D"],
        block_out_channels=[128],
        layers_per_block=2,
        norm_num_groups=32,
    ):
        super().__init__()
        self.layers_per_block = layers_per_block

        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)

        self.mid_block = ResnetBlock(block_out_channels[-1], block_out_channels[-1])

        self.up_blocks = nn.ModuleList([])

        reversed_block_out_channels = list(reversed(block_out_channels))
        output_channel = reversed_block_out_channels[0]
        for i, up_block_type in enumerate(up_block_types):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1

            up_block = UpDecoderBlock2D(
                in_channels=prev_output_channel,
                out_channels=output_channel,
                num_layers=self.layers_per_block + 1,
                add_upsample=not is_final_block,
            )
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel

        self.conv_norm_out = nn.GroupNorm(norm_num_groups, block_out_channels[0])
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)

    def forward(self, z):
        sample = z
        sample = self.conv_in(sample)

        sample = self.mid_block(sample)

        for up_block in self.up_blocks:
            sample = up_block(sample)

        sample = self.conv_norm_out(sample)
        sample = torch.nn.functional.silu(sample)
        sample = self.conv_out(sample)

        return sample

class AutoencoderKL(nn.Module):
    def __init__(
        self,
        in_channels=3,
        out_channels=3,
        latent_channels=4,
        down_block_types=["DownEncoderBlock2D"] * 4,
        up_block_types=["UpDecoderBlock2D"] * 4,
        block_out_channels=[128, 256, 512, 512],
        layers_per_block=2,
        norm_num_groups=32,
        sample_size=256,
    ):
        super().__init__()

        self.encoder = Encoder(
            in_channels=in_channels,
            out_channels=latent_channels,
            down_block_types=down_block_types,
            block_out_channels=block_out_channels,
            layers_per_block=layers_per_block,
            norm_num_groups=norm_num_groups,
        )

        self.decoder = Decoder(
            in_channels=latent_channels,
            out_channels=out_channels,
            up_block_types=up_block_types,
            block_out_channels=block_out_channels,
            layers_per_block=layers_per_block,
            norm_num_groups=norm_num_groups,
        )

        self.quant_conv = nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)
        self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)

    def encode(self, x):
        h = self.encoder(x)
        moments = self.quant_conv(h)
        posterior = DiagonalGaussianDistribution(moments)
        return posterior

    def decode(self, z):
        z = self.post_quant_conv(z)
        dec = self.decoder(z)
        return dec

    def forward(self, sample):
        x = sample
        posterior = self.encode(x)
        z = posterior.sample()
        dec = self.decode(z)
        return dec, posterior

# ==================== æ•°æ®åŠ è½½å™¨ ====================

class ImageDataset(Dataset):
    """ç®€å•çš„å›¾åƒæ•°æ®é›†ç±»"""
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.image_paths.append(os.path.join(root, file))
        
        print(f"æ‰¾åˆ° {len(self.image_paths)} å¼ å›¾åƒ")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image

def create_dataloader(data_dir, batch_size=16, num_workers=2):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]
    ])
    
    dataset = ImageDataset(data_dir, transform=transform)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return dataloader, len(dataset)

def compute_scaling_factor(
    vae_model,
    data_loader,
    device: str = 'cuda',
    num_samples: int = 1000,
    plot_distribution: bool = True
) -> dict:
    """
    è®¡ç®—VAEçš„æœ€ä½³ç¼©æ”¾å› å­
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œå»ºè®®ç¼©æ”¾å› å­çš„å­—å…¸
    """
    print(f"ğŸ” è®¡ç®—VAEç¼©æ”¾å› å­ (åˆ†æ {num_samples} ä¸ªæ ·æœ¬)...")
    
    vae_model.eval()
    latent_samples = []
    sample_count = 0
    
    with torch.no_grad():
        progress_bar = tqdm(data_loader, desc="æ”¶é›†æ½œå˜é‡æ ·æœ¬")
        
        for batch in progress_bar:
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images = batch[0]
            else:
                images = batch
            
            images = images.to(device)
            
            # VAEç¼–ç ï¼ˆè·å–åŸå§‹æ½œå˜é‡ï¼‰
            with torch.amp.autocast(device_type='cuda', enabled=False):
                images = images.float()
                latent_dist = vae_model.encode(images)  # ç›´æ¥è¿”å›DiagonalGaussianDistribution
                latents = latent_dist.sample()  # [B, 4, H, W]
            
            latent_samples.append(latents.cpu().flatten())  # å±•å¹³æ‰€æœ‰ç»´åº¦
            sample_count += latents.numel()
            
            progress_bar.set_postfix({'samples': sample_count})
            
            if sample_count >= num_samples * 4 * 32 * 32:  # è€ƒè™‘4é€šé“å’Œç©ºé—´ç»´åº¦
                break
    
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬
    all_latents = torch.cat(latent_samples, dim=0)[:num_samples * 4 * 32 * 32]
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean = all_latents.mean().item()
    std = all_latents.std().item()
    min_val = all_latents.min().item()
    max_val = all_latents.max().item()
    
    # è®¡ç®—ä¸åŒç™¾åˆ†ä½æ•°
    percentiles = [1, 5, 25, 50, 75, 95, 99]
    percentile_values = [np.percentile(all_latents.numpy(), p) for p in percentiles]
    
    # æœ€ä½³ç¼©æ”¾å› å­ = 1 / std
    optimal_scaling_factor = 1.0 / std
    
    # åˆ†æç»“æœ
    results = {
        'num_samples': len(all_latents),
        'mean': mean,
        'std': std,
        'min': min_val,
        'max': max_val,
        'percentiles': dict(zip(percentiles, percentile_values)),
        'optimal_scaling_factor': optimal_scaling_factor,
        'stable_diffusion_factor': 0.18215,  # å‚è€ƒå€¼
        'factor_ratio': optimal_scaling_factor / 0.18215
    }
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“Š VAEæ½œå˜é‡åˆ†å¸ƒç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°é‡: {results['num_samples']:,}")
    print(f"   å‡å€¼: {mean:.6f}")
    print(f"   æ ‡å‡†å·®: {std:.6f}")
    print(f"   æœ€å°å€¼: {min_val:.6f}")
    print(f"   æœ€å¤§å€¼: {max_val:.6f}")
    print(f"\nğŸ“ˆ ç™¾åˆ†ä½æ•°åˆ†å¸ƒ:")
    for p, v in zip(percentiles, percentile_values):
        print(f"   {p:2d}%: {v:8.4f}")
    
    print(f"\nğŸ¯ ç¼©æ”¾å› å­åˆ†æ:")
    print(f"   å»ºè®®ç¼©æ”¾å› å­: {optimal_scaling_factor:.6f}")
    print(f"   Stable Diffusionå› å­: {0.18215:.6f}")
    print(f"   ç›¸å¯¹æ¯”ä¾‹: {results['factor_ratio']:.2f}x")
    
    # è¯„ä¼°ç¼©æ”¾æ•ˆæœ
    scaled_std = std * optimal_scaling_factor
    print(f"   ç¼©æ”¾åæ ‡å‡†å·®: {scaled_std:.6f} (ç›®æ ‡: 1.0)")
    
    if abs(scaled_std - 1.0) < 0.1:
        print(f"   âœ… ç¼©æ”¾æ•ˆæœè‰¯å¥½")
    else:
        print(f"   âš ï¸  ç¼©æ”¾æ•ˆæœéœ€è¦éªŒè¯")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    if plot_distribution:
        plot_latent_distribution(all_latents, results, optimal_scaling_factor)
    
    return results

def plot_latent_distribution(latents, stats, scaling_factor):
    """ç»˜åˆ¶æ½œå˜é‡åˆ†å¸ƒå›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # åŸå§‹åˆ†å¸ƒ
    axes[0, 0].hist(latents.numpy(), bins=100, alpha=0.7, density=True)
    axes[0, 0].axvline(stats['mean'], color='red', linestyle='--', label=f'å‡å€¼: {stats["mean"]:.3f}')
    axes[0, 0].axvline(stats['mean'] + stats['std'], color='orange', linestyle='--', label=f'+1Ïƒ: {stats["mean"] + stats["std"]:.3f}')
    axes[0, 0].axvline(stats['mean'] - stats['std'], color='orange', linestyle='--', label=f'-1Ïƒ: {stats["mean"] - stats["std"]:.3f}')
    axes[0, 0].set_title('åŸå§‹æ½œå˜é‡åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('å€¼')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # ç¼©æ”¾ååˆ†å¸ƒ
    scaled_latents = latents * scaling_factor
    axes[0, 1].hist(scaled_latents.numpy(), bins=100, alpha=0.7, density=True, color='green')
    scaled_mean = scaled_latents.mean().item()
    scaled_std = scaled_latents.std().item()
    axes[0, 1].axvline(scaled_mean, color='red', linestyle='--', label=f'å‡å€¼: {scaled_mean:.3f}')
    axes[0, 1].axvline(scaled_mean + scaled_std, color='orange', linestyle='--', label=f'+1Ïƒ: {scaled_mean + scaled_std:.3f}')
    axes[0, 1].axvline(scaled_mean - scaled_std, color='orange', linestyle='--', label=f'-1Ïƒ: {scaled_mean - scaled_std:.3f}')
    axes[0, 1].set_title(f'ç¼©æ”¾ååˆ†å¸ƒ (Ã—{scaling_factor:.4f})')
    axes[0, 1].set_xlabel('å€¼')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Q-Qå›¾ï¼šæ£€éªŒæ˜¯å¦æ¥è¿‘æ­£æ€åˆ†å¸ƒ
    from scipy import stats as scipy_stats
    scipy_stats.probplot(scaled_latents.numpy(), dist="norm", plot=axes[1, 0])
    axes[1, 0].set_title('Q-Qå›¾ (ç¼©æ”¾å vs æ ‡å‡†æ­£æ€)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç™¾åˆ†ä½æ•°å¯¹æ¯”
    percentiles = list(stats['percentiles'].keys())
    values = list(stats['percentiles'].values())
    axes[1, 1].plot(percentiles, values, 'o-', label='åŸå§‹', linewidth=2)
    axes[1, 1].plot(percentiles, [v * scaling_factor for v in values], 's-', label='ç¼©æ”¾å', linewidth=2)
    axes[1, 1].set_title('ç™¾åˆ†ä½æ•°å¯¹æ¯”')
    axes[1, 1].set_xlabel('ç™¾åˆ†ä½æ•° (%)')
    axes[1, 1].set_ylabel('å€¼')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('vae_scaling_factor_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š åˆ†å¸ƒåˆ†æå›¾å·²ä¿å­˜: vae_scaling_factor_analysis.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” VAEç¼©æ”¾å› å­è®¡ç®—å·¥å…·")
    print("=" * 50)
    
    # VAEæ£€æŸ¥ç‚¹è·¯å¾„
    vae_checkpoint_paths = [
        "/kaggle/input/vae-model/vae_finetuned_epoch_23.pth",
        "VAE/vae_model_best.pth",
        "VAE/vae_model_final.pth",
        "../VAE/vae_model_best.pth"
    ]
    
    vae_checkpoint_path = None
    for path in vae_checkpoint_paths:
        if os.path.exists(path):
            vae_checkpoint_path = path
            break
    
    if vae_checkpoint_path is None:
        print("âŒ æœªæ‰¾åˆ°VAEæ£€æŸ¥ç‚¹æ–‡ä»¶")
        print("è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„æ˜¯å¦å­˜åœ¨:")
        for path in vae_checkpoint_paths:
            print(f"   - {path}")
        return
    
    print(f"âœ… æ‰¾åˆ°VAEæ£€æŸ¥ç‚¹: {vae_checkpoint_path}")
    
    # åŠ è½½VAEæ¨¡å‹
    try:
        print(f"ğŸ”§ åŠ è½½VAEæ¨¡å‹: {vae_checkpoint_path}")
        
        # åˆ›å»ºVAEæ¨¡å‹
        vae_model = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            latent_channels=4,
            down_block_types=['DownEncoderBlock2D'] * 4,
            up_block_types=['UpDecoderBlock2D'] * 4,
            block_out_channels=[128, 256, 512, 512],
            layers_per_block=2,
            norm_num_groups=32,
            sample_size=256
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(vae_checkpoint_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
        if 'vae_state_dict' in checkpoint:
            state_dict = checkpoint['vae_state_dict']
            print("âœ… æ£€æµ‹åˆ° 'vae_state_dict' æ ¼å¼")
        elif 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            print("âœ… æ£€æµ‹åˆ° 'model_state_dict' æ ¼å¼")
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            print("âœ… æ£€æµ‹åˆ° 'state_dict' æ ¼å¼")
        else:
            state_dict = checkpoint
            print("âœ… æ£€æµ‹åˆ°ç›´æ¥çŠ¶æ€å­—å…¸æ ¼å¼")
        
        # æ¸…ç†å¯èƒ½çš„é”®åå‰ç¼€
        new_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('vae.'):
                new_key = key[4:]  # ç§»é™¤ 'vae.' å‰ç¼€
            elif key.startswith('model.'):
                new_key = key[6:]  # ç§»é™¤ 'model.' å‰ç¼€
            else:
                new_key = key
            new_state_dict[new_key] = value
        
        # åŠ è½½çŠ¶æ€å­—å…¸
        missing_keys, unexpected_keys = vae_model.load_state_dict(new_state_dict, strict=False)
        
        if missing_keys:
            print(f"âš ï¸  ç¼ºå¤±çš„é”®: {missing_keys[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        if unexpected_keys:
            print(f"âš ï¸  æ„å¤–çš„é”®: {unexpected_keys[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        vae_model.eval()
        vae_model.to(device)
        
        print(f"âœ… VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        # Kaggleç¯å¢ƒçš„å¯èƒ½æ•°æ®è·¯å¾„
        possible_data_dirs = [
            "/kaggle/input/dataset/dataset",
            "/kaggle/input/dataset",
            "/kaggle/input/data/dataset", 
            "dataset",
            "data",
            "../dataset"
        ]
        
        data_dir = None
        for path in possible_data_dirs:
            if os.path.exists(path) and os.path.isdir(path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ–‡ä»¶
                has_images = False
                for root, dirs, files in os.walk(path):
                    for file in files:
                        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                            has_images = True
                            break
                    if has_images:
                        break
                
                if has_images:
                    data_dir = path
                    break
        
        if data_dir is None:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®é›†ç›®å½•")
            print("è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å›¾åƒæ–‡ä»¶:")
            for path in possible_data_dirs:
                exists = "âœ…" if os.path.exists(path) else "âŒ"
                print(f"   {exists} {path}")
            return
        
        print(f"âœ… æ‰¾åˆ°æ•°æ®é›†ç›®å½•: {data_dir}")
        
        train_loader, train_size = create_dataloader(data_dir)
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒé›†: {train_size} æ ·æœ¬")
        
        # ä½¿ç”¨è®­ç»ƒé›†è®¡ç®—ç¼©æ”¾å› å­
        data_loader = train_loader
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # è®¡ç®—ç¼©æ”¾å› å­
    try:
        results = compute_scaling_factor(
            vae_model=vae_model,
            data_loader=data_loader,
            device=device,
            num_samples=2000,  # æ›´å¤šæ ·æœ¬ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»Ÿè®¡
            plot_distribution=True
        )
        
        # ä¿å­˜ç»“æœ
        with open('vae_scaling_factor_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: vae_scaling_factor_results.json")
        
        # ç»™å‡ºä½¿ç”¨å»ºè®®
        print(f"\nğŸ“ ä½¿ç”¨å»ºè®®:")
        print(f"   1. åœ¨ ldm_config.yaml ä¸­æ›´æ–°ç¼©æ”¾å› å­")
        print(f"   2. æˆ–è€…åœ¨è®­ç»ƒæ—¶è®¾ç½® auto_update_scaling_factor: true")
        print(f"   3. å»ºè®®ç¼©æ”¾å› å­: {results['optimal_scaling_factor']:.6f}")
        
        if results['factor_ratio'] > 2.0 or results['factor_ratio'] < 0.5:
            print(f"   âš ï¸  ç¼©æ”¾å› å­ä¸æ ‡å‡†å€¼å·®å¼‚è¾ƒå¤§ï¼Œè¯·ä»”ç»†éªŒè¯")
        else:
            print(f"   âœ… ç¼©æ”¾å› å­åœ¨åˆç†èŒƒå›´å†…")
            
    except Exception as e:
        print(f"âŒ ç¼©æ”¾å› å­è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
