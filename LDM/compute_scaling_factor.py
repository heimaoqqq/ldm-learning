"""
è®¡ç®—å¾®è°ƒåVAEçš„æœ€ä½³ç¼©æ”¾å› å­
å•ç‹¬è¿è¡Œæ­¤è„šæœ¬æ¥åˆ†æVAEçš„æ½œå˜é‡åˆ†å¸ƒå¹¶è®¡ç®—æœ€ä½³ç¼©æ”¾å› å­
"""
import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader
from torchvision import transforms
import matplotlib.pyplot as plt
from tqdm import tqdm
import yaml

# å¯¼å…¥VAEæ¨¡å‹
sys.path.append('../VAE')
try:
    from vae_model import AutoencoderKL
    from dataset import build_dataloader as vae_build_dataloader
    print("âœ… æˆåŠŸå¯¼å…¥VAEç›¸å…³æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥VAEæ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

def load_vae_model(checkpoint_path: str, device: str = 'cuda') -> AutoencoderKL:
    """åŠ è½½VAEæ¨¡å‹"""
    print(f"ğŸ”§ åŠ è½½VAEæ¨¡å‹: {checkpoint_path}")
    
    # åˆ›å»ºVAEæ¨¡å‹
    vae = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        latent_channels=4,
        down_block_types=['DownEncoderBlock2D'] * 4,
        up_block_types=['UpDecoderBlock2D'] * 4,
        block_out_channels=[128, 256, 512, 512],
        layers_per_block=2,
        norm_num_groups=32,
        sample_size=256
    )
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    vae.load_state_dict(state_dict)
    vae.eval()
    vae.to(device)
    
    print(f"âœ… VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
    return vae

def compute_scaling_factor(
    vae_model,
    data_loader,
    device: str = 'cuda',
    num_samples: int = 1000,
    plot_distribution: bool = True
) -> dict:
    """
    è®¡ç®—VAEçš„æœ€ä½³ç¼©æ”¾å› å­
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œå»ºè®®ç¼©æ”¾å› å­çš„å­—å…¸
    """
    print(f"ğŸ” è®¡ç®—VAEç¼©æ”¾å› å­ (åˆ†æ {num_samples} ä¸ªæ ·æœ¬)...")
    
    vae_model.eval()
    latent_samples = []
    sample_count = 0
    
    with torch.no_grad():
        progress_bar = tqdm(data_loader, desc="æ”¶é›†æ½œå˜é‡æ ·æœ¬")
        
        for batch in progress_bar:
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                images = batch[0]
            else:
                images = batch
            
            images = images.to(device)
            
            # VAEç¼–ç ï¼ˆè·å–åŸå§‹æ½œå˜é‡ï¼‰
            with torch.amp.autocast(device_type='cuda', enabled=False):
                images = images.float()
                latent_dist = vae_model.encode(images).latent_dist
                latents = latent_dist.sample()  # [B, 4, H, W]
            
            latent_samples.append(latents.cpu().flatten())  # å±•å¹³æ‰€æœ‰ç»´åº¦
            sample_count += latents.numel()
            
            progress_bar.set_postfix({'samples': sample_count})
            
            if sample_count >= num_samples * 4 * 32 * 32:  # è€ƒè™‘4é€šé“å’Œç©ºé—´ç»´åº¦
                break
    
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬
    all_latents = torch.cat(latent_samples, dim=0)[:num_samples * 4 * 32 * 32]
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean = all_latents.mean().item()
    std = all_latents.std().item()
    min_val = all_latents.min().item()
    max_val = all_latents.max().item()
    
    # è®¡ç®—ä¸åŒç™¾åˆ†ä½æ•°
    percentiles = [1, 5, 25, 50, 75, 95, 99]
    percentile_values = [np.percentile(all_latents.numpy(), p) for p in percentiles]
    
    # æœ€ä½³ç¼©æ”¾å› å­ = 1 / std
    optimal_scaling_factor = 1.0 / std
    
    # åˆ†æç»“æœ
    results = {
        'num_samples': len(all_latents),
        'mean': mean,
        'std': std,
        'min': min_val,
        'max': max_val,
        'percentiles': dict(zip(percentiles, percentile_values)),
        'optimal_scaling_factor': optimal_scaling_factor,
        'stable_diffusion_factor': 0.18215,  # å‚è€ƒå€¼
        'factor_ratio': optimal_scaling_factor / 0.18215
    }
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“Š VAEæ½œå˜é‡åˆ†å¸ƒç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°é‡: {results['num_samples']:,}")
    print(f"   å‡å€¼: {mean:.6f}")
    print(f"   æ ‡å‡†å·®: {std:.6f}")
    print(f"   æœ€å°å€¼: {min_val:.6f}")
    print(f"   æœ€å¤§å€¼: {max_val:.6f}")
    print(f"\nğŸ“ˆ ç™¾åˆ†ä½æ•°åˆ†å¸ƒ:")
    for p, v in zip(percentiles, percentile_values):
        print(f"   {p:2d}%: {v:8.4f}")
    
    print(f"\nğŸ¯ ç¼©æ”¾å› å­åˆ†æ:")
    print(f"   å»ºè®®ç¼©æ”¾å› å­: {optimal_scaling_factor:.6f}")
    print(f"   Stable Diffusionå› å­: {0.18215:.6f}")
    print(f"   ç›¸å¯¹æ¯”ä¾‹: {results['factor_ratio']:.2f}x")
    
    # è¯„ä¼°ç¼©æ”¾æ•ˆæœ
    scaled_std = std * optimal_scaling_factor
    print(f"   ç¼©æ”¾åæ ‡å‡†å·®: {scaled_std:.6f} (ç›®æ ‡: 1.0)")
    
    if abs(scaled_std - 1.0) < 0.1:
        print(f"   âœ… ç¼©æ”¾æ•ˆæœè‰¯å¥½")
    else:
        print(f"   âš ï¸  ç¼©æ”¾æ•ˆæœéœ€è¦éªŒè¯")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    if plot_distribution:
        plot_latent_distribution(all_latents, results, optimal_scaling_factor)
    
    return results

def plot_latent_distribution(latents, stats, scaling_factor):
    """ç»˜åˆ¶æ½œå˜é‡åˆ†å¸ƒå›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # åŸå§‹åˆ†å¸ƒ
    axes[0, 0].hist(latents.numpy(), bins=100, alpha=0.7, density=True)
    axes[0, 0].axvline(stats['mean'], color='red', linestyle='--', label=f'å‡å€¼: {stats["mean"]:.3f}')
    axes[0, 0].axvline(stats['mean'] + stats['std'], color='orange', linestyle='--', label=f'+1Ïƒ: {stats["mean"] + stats["std"]:.3f}')
    axes[0, 0].axvline(stats['mean'] - stats['std'], color='orange', linestyle='--', label=f'-1Ïƒ: {stats["mean"] - stats["std"]:.3f}')
    axes[0, 0].set_title('åŸå§‹æ½œå˜é‡åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('å€¼')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # ç¼©æ”¾ååˆ†å¸ƒ
    scaled_latents = latents * scaling_factor
    axes[0, 1].hist(scaled_latents.numpy(), bins=100, alpha=0.7, density=True, color='green')
    scaled_mean = scaled_latents.mean().item()
    scaled_std = scaled_latents.std().item()
    axes[0, 1].axvline(scaled_mean, color='red', linestyle='--', label=f'å‡å€¼: {scaled_mean:.3f}')
    axes[0, 1].axvline(scaled_mean + scaled_std, color='orange', linestyle='--', label=f'+1Ïƒ: {scaled_mean + scaled_std:.3f}')
    axes[0, 1].axvline(scaled_mean - scaled_std, color='orange', linestyle='--', label=f'-1Ïƒ: {scaled_mean - scaled_std:.3f}')
    axes[0, 1].set_title(f'ç¼©æ”¾ååˆ†å¸ƒ (Ã—{scaling_factor:.4f})')
    axes[0, 1].set_xlabel('å€¼')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Q-Qå›¾ï¼šæ£€éªŒæ˜¯å¦æ¥è¿‘æ­£æ€åˆ†å¸ƒ
    from scipy import stats as scipy_stats
    scipy_stats.probplot(scaled_latents.numpy(), dist="norm", plot=axes[1, 0])
    axes[1, 0].set_title('Q-Qå›¾ (ç¼©æ”¾å vs æ ‡å‡†æ­£æ€)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç™¾åˆ†ä½æ•°å¯¹æ¯”
    percentiles = list(stats['percentiles'].keys())
    values = list(stats['percentiles'].values())
    axes[1, 1].plot(percentiles, values, 'o-', label='åŸå§‹', linewidth=2)
    axes[1, 1].plot(percentiles, [v * scaling_factor for v in values], 's-', label='ç¼©æ”¾å', linewidth=2)
    axes[1, 1].set_title('ç™¾åˆ†ä½æ•°å¯¹æ¯”')
    axes[1, 1].set_xlabel('ç™¾åˆ†ä½æ•° (%)')
    axes[1, 1].set_ylabel('å€¼')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('vae_scaling_factor_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š åˆ†å¸ƒåˆ†æå›¾å·²ä¿å­˜: vae_scaling_factor_analysis.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” VAEç¼©æ”¾å› å­è®¡ç®—å·¥å…·")
    print("=" * 50)
    
    # VAEæ£€æŸ¥ç‚¹è·¯å¾„
    vae_checkpoint_paths = [
        "/kaggle/input/vae-model/vae_finetuned_epoch_23.pth",
        "VAE/vae_model_best.pth",
        "VAE/vae_model_final.pth",
        "../VAE/vae_model_best.pth"
    ]
    
    vae_checkpoint_path = None
    for path in vae_checkpoint_paths:
        if os.path.exists(path):
            vae_checkpoint_path = path
            break
    
    if vae_checkpoint_path is None:
        print("âŒ æœªæ‰¾åˆ°VAEæ£€æŸ¥ç‚¹æ–‡ä»¶")
        print("è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„æ˜¯å¦å­˜åœ¨:")
        for path in vae_checkpoint_paths:
            print(f"   - {path}")
        return
    
    print(f"âœ… æ‰¾åˆ°VAEæ£€æŸ¥ç‚¹: {vae_checkpoint_path}")
    
    # è®¾å¤‡è®¾ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½VAEæ¨¡å‹
    try:
        vae_model = load_vae_model(vae_checkpoint_path, device)
    except Exception as e:
        print(f"âŒ VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        data_dir = "/kaggle/input/dataset/dataset"
        if not os.path.exists(data_dir):
            data_dir = "dataset"  # å¤‡ç”¨è·¯å¾„
        
        train_loader, val_loader, train_size, val_size = vae_build_dataloader(
            root_dir=data_dir,
            batch_size=16,  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
            num_workers=2,
            shuffle_train=False,  # ä¸éœ€è¦æ‰“ä¹±ï¼Œåªæ˜¯ç»Ÿè®¡
            shuffle_val=False,
            val_split=0.2,
            random_state=42
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒé›†: {train_size} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {val_size} æ ·æœ¬")
        
        # ä½¿ç”¨è®­ç»ƒé›†è®¡ç®—ç¼©æ”¾å› å­
        data_loader = train_loader
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # è®¡ç®—ç¼©æ”¾å› å­
    try:
        results = compute_scaling_factor(
            vae_model=vae_model,
            data_loader=data_loader,
            device=device,
            num_samples=2000,  # æ›´å¤šæ ·æœ¬ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»Ÿè®¡
            plot_distribution=True
        )
        
        # ä¿å­˜ç»“æœ
        import json
        with open('vae_scaling_factor_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: vae_scaling_factor_results.json")
        
        # ç»™å‡ºä½¿ç”¨å»ºè®®
        print(f"\nğŸ“ ä½¿ç”¨å»ºè®®:")
        print(f"   1. åœ¨ ldm_config.yaml ä¸­æ›´æ–°ç¼©æ”¾å› å­")
        print(f"   2. æˆ–è€…åœ¨è®­ç»ƒæ—¶è®¾ç½® auto_update_scaling_factor: true")
        print(f"   3. å»ºè®®ç¼©æ”¾å› å­: {results['optimal_scaling_factor']:.6f}")
        
        if results['factor_ratio'] > 2.0 or results['factor_ratio'] < 0.5:
            print(f"   âš ï¸  ç¼©æ”¾å› å­ä¸æ ‡å‡†å€¼å·®å¼‚è¾ƒå¤§ï¼Œè¯·ä»”ç»†éªŒè¯")
        else:
            print(f"   âœ… ç¼©æ”¾å› å­åœ¨åˆç†èŒƒå›´å†…")
            
    except Exception as e:
        print(f"âŒ ç¼©æ”¾å› å­è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
