"""
VAE-LDM å®Œæ•´æ¨¡å‹
é›†æˆVAEç¼–ç å™¨/è§£ç å™¨å’Œæ‰©æ•£æ¨¡å‹çš„å®Œæ•´æ½œåœ¨æ‰©æ•£ç³»ç»Ÿ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple
from vae_diffusion import VAEDiffusionUNet
from gaussian_diffusion import create_diffusion
import sys
import os

# æ·»åŠ VAEè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))
from adv_vq_vae import AdvVQVAE

class VAELatentDiffusionModel(nn.Module):
    """
    å®Œæ•´çš„VAEæ½œåœ¨æ‰©æ•£æ¨¡å‹
    """
    def __init__(
        self,
        vae_config: Dict[str, Any],
        unet_config: Dict[str, Any],
        diffusion_config: Dict[str, Any],
        vae_checkpoint_path: str = None,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
        scale_factor: float = 1.0,
        vae_scale_factor: float = 0.18215,
        latent_scale_factor: float = 1.0
    ):
        super().__init__()
        
        self.device = device
        
        # 1. åˆå§‹åŒ–VAEï¼ˆå†»ç»“å‚æ•°ï¼‰
        self.vae = self._load_vae(vae_config, vae_checkpoint_path)
        
        # 2. åˆå§‹åŒ–U-Net
        self.unet = VAEDiffusionUNet(**unet_config)
        
        # 3. åˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹
        self.diffusion = create_diffusion(**diffusion_config)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(device)
        
        # å­˜å‚¨é…ç½®ä¿¡æ¯ç”¨äºè¾“å‡º
        self.vae_latent_dim = vae_config.get('latent_dim', 256)
        
        self.scale_factor = scale_factor
        self.vae_scale_factor = vae_scale_factor
        self.latent_scale_factor = latent_scale_factor
        
        # æ³¨é‡Šæ‰è°ƒè¯•ç›¸å…³å˜é‡ï¼Œå› ä¸ºè°ƒè¯•æ‰“å°å·²è¢«æ³¨é‡Š
        # æ–°å¢ï¼šç”¨äºæ§åˆ¶epochçº§åˆ«è°ƒè¯•æ‰“å°çš„å±æ€§
        # self._last_printed_epoch_debug = -1
        # self._prints_this_epoch_debug = 0
        
        print(f"âœ… VAE-LDM åˆå§‹åŒ–å®Œæˆ")
        print(f"   VAEæ½œåœ¨ç»´åº¦: {self.vae_latent_dim}")
        print(f"   U-Netç±»åˆ«æ•°: {self.unet.num_classes}")
        print(f"   æ‰©æ•£æ­¥æ•°: {self.diffusion.num_timesteps}")
        
    def _load_vae(self, vae_config: Dict[str, Any], checkpoint_path: str) -> AdvVQVAE:
        """åŠ è½½é¢„è®­ç»ƒçš„VAE"""
        vae = AdvVQVAE(**vae_config)
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                print(f"ğŸ” æ£€æŸ¥ç‚¹æ–‡ä»¶åŒ…å« {len(checkpoint)} ä¸ªé”®")
                
                # å°è¯•ä¸åŒçš„é”®åæ¥åŠ è½½æ¨¡å‹çŠ¶æ€
                state_dict = None
                
                # å°è¯•å¸¸è§çš„é”®å
                possible_keys = [
                    'model_state_dict',  # æ ‡å‡†æ ¼å¼
                    'state_dict',        # ç®€åŒ–æ ¼å¼
                    'model',             # å¦ä¸€ç§æ ¼å¼
                    'vae_state_dict',    # VAEç‰¹å®šæ ¼å¼
                ]
                
                for key in possible_keys:
                    if key in checkpoint:
                        state_dict = checkpoint[key]
                        print(f"âœ… ä½¿ç”¨é”® '{key}' åŠ è½½VAEæƒé‡")
                        break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„é”®ï¼Œæ£€æŸ¥æ˜¯å¦ç›´æ¥æ˜¯çŠ¶æ€å­—å…¸
                if state_dict is None:
                    # æ£€æŸ¥æ˜¯å¦æ•´ä¸ªcheckpointå°±æ˜¯çŠ¶æ€å­—å…¸
                    if isinstance(checkpoint, dict) and any(k.startswith(('encoder', 'decoder', 'quantize', 'quant_conv', 'post_quant_conv')) for k in checkpoint.keys()):
                        state_dict = checkpoint
                        print("âœ… æ£€æŸ¥ç‚¹ç›´æ¥æ˜¯çŠ¶æ€å­—å…¸æ ¼å¼")
                    else:
                        print(f"âŒ æ— æ³•æ‰¾åˆ°VAEçŠ¶æ€å­—å…¸ï¼Œå¯ç”¨é”®: {list(checkpoint.keys())}")
                        raise KeyError(f"æ— æ³•åœ¨æ£€æŸ¥ç‚¹ä¸­æ‰¾åˆ°æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚å¯ç”¨é”®: {list(checkpoint.keys())}")
                
                # åŠ è½½çŠ¶æ€å­—å…¸
                vae.load_state_dict(state_dict)
                print(f"âœ… VAE æƒé‡åŠ è½½æˆåŠŸ: {checkpoint_path}")
                
            except Exception as e:
                print(f"âŒ VAEåŠ è½½å¤±è´¥: {e}")
                print(f"ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„VAEæƒé‡")
                print("   è­¦å‘Šï¼šè¿™å¯èƒ½å¯¼è‡´è®­ç»ƒæ€§èƒ½ä¸‹é™")
        else:
            print(f"âš ï¸  VAE checkpoint æœªæ‰¾åˆ°: {checkpoint_path}")
            print("ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„VAEæƒé‡")
            
        # å†»ç»“VAEå‚æ•°
        for param in vae.parameters():
            param.requires_grad = False
        vae.eval()
        
        print(f"ğŸ”’ VAEå‚æ•°å·²å†»ç»“ï¼ˆ{sum(p.numel() for p in vae.parameters())} å‚æ•°ï¼‰")
        
        return vae
    
    @torch.no_grad()
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """
        å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ [-1, 1]
            
        Returns:
            latents: [B, latent_dim, h, w] æ½œåœ¨è¡¨ç¤º
        """
        self.vae.eval()
        
        # å°è¯•ä½¿ç”¨VAEçš„ç¼–ç æ–¹æ³•
        try:
            # æ–¹æ³•1: ä½¿ç”¨encodeæ–¹æ³• (æœ€ç›´æ¥)
            if hasattr(self.vae, 'encode'):
                latents_raw = self.vae.encode(images)
            # æ–¹æ³•2: ä½¿ç”¨ç¼–ç å™¨ + é‡åŒ–å™¨
            elif hasattr(self.vae, 'encoder') and hasattr(self.vae, 'vq'):
                encoded = self.vae.encoder(images)
                quantized, vq_loss, perplexity = self.vae.vq(encoded)
                latents_raw = quantized
            # æ–¹æ³•3: ä½¿ç”¨forwardï¼Œè§£æè¿”å›å€¼ï¼ˆæœ€åå¤‡ç”¨ï¼‰
            else:
                vae_output = self.vae(images)
                if isinstance(vae_output, tuple) and len(vae_output) >= 3:
                    # VAE forwardé€šå¸¸è¿”å› (reconstruction, commitment_loss, codebook_loss)
                    # æˆ‘ä»¬éœ€è¦ç”¨ç¼–ç å™¨+é‡åŒ–è·å–æ½œåœ¨è¡¨ç¤º
                    encoded = self.vae.encoder(images)
                    quantized, _, _ = self.vae.vq(encoded)
                    latents_raw = quantized
                else:
                    raise ValueError("Unexpected VAE output format")
            
            # åº”ç”¨æ½œå˜é‡ç¼©æ”¾å› å­
            scaled_latents = latents_raw * self.latent_scale_factor
            return scaled_latents
                     
        except Exception as e:
            print(f"âŒ VAEç¼–ç å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªåˆé€‚å½¢çŠ¶çš„éšæœºå¼ é‡ä½œä¸ºå¤‡ç”¨
            batch_size = images.shape[0]
            latent_h, latent_w = images.shape[2] // 8, images.shape[3] // 8  # å‡è®¾8å€ä¸‹é‡‡æ ·
            backup_latents = torch.randn(batch_size, 256, latent_h, latent_w, device=images.device)
            print(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨éšæœºæ½œåœ¨è¡¨ç¤ºï¼Œå½¢çŠ¶: {backup_latents.shape}")
            return backup_latents
    
    @torch.no_grad() 
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ
        
        Args:
            latents: [B, latent_dim, h, w] æ½œåœ¨è¡¨ç¤º
            
        Returns:
            images: [B, 3, H, W] é‡å»ºå›¾åƒ [-1, 1]
        """
        self.vae.eval()
        
        # VAEè§£ç 
        decoded = self.vae.decode(latents)
        return decoded
    
    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None, current_epoch: Optional[int] = None) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒå‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾
            current_epoch: å½“å‰è®­ç»ƒçš„epochå·ï¼ˆç”¨äºæ§åˆ¶è°ƒè¯•æ‰“å°ï¼‰
            
        Returns:
            loss_dict: åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
        """
        # 1. ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            latents = self.encode_to_latent(images)
        
        # <--- æ³¨é‡Šæ‰è°ƒè¯•æ‰“å°é€»è¾‘ï¼Œä½¿æ—¥å¿—æ›´ç®€æ´ --->
        # if current_epoch is not None and current_epoch < 10: # æ£€æŸ¥æ˜¯å¦åœ¨å‰10ä¸ªepoch
        #     if self._last_printed_epoch_debug != current_epoch: # å¦‚æœæ˜¯æ–°çš„epochï¼ˆåœ¨å‰10ä¸ªä¸­ï¼‰
        #         self._last_printed_epoch_debug = current_epoch
        #         self._prints_this_epoch_debug = 0 # é‡ç½®å½“å‰epochçš„æ‰“å°è®¡æ•°å™¨
        #     
        #     if self._prints_this_epoch_debug < 5: # æ‰“å°å½“å‰epochçš„å‰5ä¸ªæ‰¹æ¬¡
        #         # ä½¿ç”¨ current_epoch + 1 ä½¿å…¶ä»1å¼€å§‹è®¡æ•°epochï¼Œ_prints_this_epoch_debug ä»0å¼€å§‹ï¼Œæ‰€ä»¥ä¹Ÿ+1
        #         print(f"Debug (Epoch {current_epoch + 1}, Batch In Epoch {self._prints_this_epoch_debug + 1}): Latents fed to U-Net - Mean: {latents.mean().item():.4f}, Std: {latents.std().item():.4f}, Min: {latents.min().item():.4f}, Max: {latents.max().item():.4f}")
        #         self._prints_this_epoch_debug += 1
        # <--- è°ƒè¯•æ‰“å°ç»“æŸ --->
        
        # 2. éšæœºé‡‡æ ·æ—¶é—´æ­¥
        batch_size = latents.shape[0]
        t = torch.randint(0, self.diffusion.num_timesteps, (batch_size,), device=self.device, dtype=torch.long)
        
        # 3. è®¡ç®—æ‰©æ•£æŸå¤±
        model_kwargs = {}
        if class_labels is not None:
            model_kwargs['y'] = class_labels
            
        losses = self.diffusion.training_losses(
            model=self.unet,
            x_start=latents,
            t=t,
            model_kwargs=model_kwargs
        )
        
        return losses
    
    @torch.no_grad()
    def sample(
        self,
        num_samples: int,
        class_labels: Optional[torch.Tensor] = None,
        guidance_scale: float = 1.0,
        num_inference_steps: Optional[int] = None,
        use_ddim: bool = True,
        eta: float = 0.0,
        return_intermediates: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        ç”Ÿæˆæ ·æœ¬
        
        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            class_labels: [num_samples] ç±»åˆ«æ ‡ç­¾
            guidance_scale: åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å¼ºåº¦
            num_inference_steps: æ¨ç†æ­¥æ•°
            use_ddim: æ˜¯å¦ä½¿ç”¨DDIMé‡‡æ ·
            eta: DDIMéšæœºæ€§å‚æ•°
            return_intermediates: æ˜¯å¦è¿”å›ä¸­é—´ç»“æœ
            
        Returns:
            images: [num_samples, 3, H, W] ç”Ÿæˆçš„å›¾åƒ
            latents: [num_samples, latent_dim, h, w] å¯é€‰çš„æ½œåœ¨è¡¨ç¤º
        """
        self.unet.eval()
        
        # è·å–æ½œåœ¨ç©ºé—´å½¢çŠ¶
        with torch.no_grad():
            # ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥å›¾åƒå°ºå¯¸ï¼š256Ã—256 (æ•°æ®é›†çš„å®é™…å°ºå¯¸)
            dummy_img = torch.randn(1, 3, 256, 256, device=self.device) 
            dummy_latent = self.encode_to_latent(dummy_img)
            latent_shape = dummy_latent.shape[1:]
        
        # é‡‡æ ·å½¢çŠ¶
        shape = (num_samples, *latent_shape)
        
        # æ¨¡å‹å‚æ•°
        model_kwargs = {}
        if class_labels is not None:
            assert len(class_labels) == num_samples
            model_kwargs['y'] = class_labels
        
        # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
        if guidance_scale > 1.0 and class_labels is not None:
            # éœ€è¦å®ç°CFGï¼Œè¿™é‡Œå…ˆç®€åŒ–
            pass
        
        # é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        if use_ddim:
            latents = self.diffusion.ddim_sample_loop(
                model=self.unet,
                shape=shape,
                clip_denoised=True,
                model_kwargs=model_kwargs,
                device=self.device,
                progress=True,
                eta=eta
            )
        else:
            latents = self.diffusion.p_sample_loop(
                model=self.unet,
                shape=shape,
                clip_denoised=True,
                model_kwargs=model_kwargs,
                device=self.device,
                progress=True
            )
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        images = self.decode_from_latent(latents)
        
        if return_intermediates:
            return images, latents
        else:
            return images, None
    
    def configure_optimizers(self, lr: float = 1e-4, weight_decay: float = 0.0):
        """é…ç½®ä¼˜åŒ–å™¨"""
        # åªä¼˜åŒ–U-Netå‚æ•°ï¼ŒVAEå‚æ•°è¢«å†»ç»“
        optimizer = torch.optim.AdamW(
            self.unet.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        return optimizer
    
    def save_checkpoint(self, filepath: str, epoch: int, optimizer_state: Dict = None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'unet_state_dict': self.unet.state_dict(),
            'model_config': {
                'unet_config': {
                    'image_size': self.unet.image_size,
                    'in_channels': self.unet.in_channels,
                    'model_channels': self.unet.model_channels,
                    'out_channels': self.unet.out_channels,
                    'num_res_blocks': self.unet.num_res_blocks,
                    'attention_resolutions': self.unet.attention_resolutions,
                    'dropout': self.unet.dropout,
                    'channel_mult': self.unet.channel_mult,
                    'num_classes': self.unet.num_classes,
                    'use_checkpoint': self.unet.use_checkpoint,
                    'num_heads': self.unet.num_heads,
                    'num_heads_upsample': self.unet.num_heads_upsample,
                },
                'diffusion_config': {
                    'timestep_respacing': "",
                    'noise_schedule': "cosine",
                    'diffusion_steps': self.diffusion.num_timesteps,
                }
            }
        }
        
        if optimizer_state is not None:
            checkpoint['optimizer_state_dict'] = optimizer_state
            
        torch.save(checkpoint, filepath)
        print(f"âœ… Checkpoint saved: {filepath}")
    
    def load_checkpoint(self, filepath: str, load_optimizer: bool = False):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # åŠ è½½U-Netæƒé‡
        self.unet.load_state_dict(checkpoint['unet_state_dict'])
        
        epoch = checkpoint.get('epoch', 0)
        optimizer_state = checkpoint.get('optimizer_state_dict', None) if load_optimizer else None
        
        print(f"âœ… Checkpoint loaded: {filepath} (epoch {epoch})")
        
        return epoch, optimizer_state

def create_vae_ldm(
    vae_checkpoint_path: str,
    image_size: int = 32,
    num_classes: int = 31,
    diffusion_steps: int = 1000,
    noise_schedule: str = "cosine",
    device: str = None
) -> VAELatentDiffusionModel:
    """
    åˆ›å»ºVAE-LDMæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        vae_checkpoint_path: VAEæ£€æŸ¥ç‚¹è·¯å¾„
        image_size: å›¾åƒå°ºå¯¸
        num_classes: ç±»åˆ«æ•°
        diffusion_steps: æ‰©æ•£æ­¥æ•°
        noise_schedule: å™ªå£°è°ƒåº¦
        device: è®¾å¤‡
        
    Returns:
        VAE-LDMæ¨¡å‹
    """
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # VAEé…ç½®ï¼ˆéœ€è¦ä¸é¢„è®­ç»ƒæ¨¡å‹åŒ¹é…ï¼‰
    vae_config = {
        'in_channels': 3,
        'latent_dim': 256,
        'num_embeddings': 512,
        'beta': 0.25,
        'groups': 1,
    }
    
    # U-Neté…ç½®
    unet_params_for_model = {
        'image_size': image_size,             # æ¥è‡ªå‡½æ•°å‚æ•°
        'in_channels': 256,                  # ç¤ºä¾‹å€¼ï¼Œå¯ä»¥é…ç½®
        'model_channels': 192,                # ç¤ºä¾‹å€¼ï¼Œå¯ä»¥é…ç½®
        'out_channels': 256,                  # ç¤ºä¾‹å€¼ï¼Œå¯ä»¥é…ç½®
        'num_res_blocks': 3,                  # ç¤ºä¾‹å€¼
        'attention_resolutions': (4, 8, 16),  # ç¤ºä¾‹å€¼ï¼ŒåŸºäº32x32æ½œåœ¨ç©ºé—´
        'dropout': 0.1,
        'channel_mult': (1, 2, 3, 4),       # ç¤ºä¾‹å€¼
        'num_classes': num_classes,           # æ¥è‡ªå‡½æ•°å‚æ•°
        'use_checkpoint': False,
        'num_heads': 4,
        'num_heads_upsample': -1,
        'use_scale_shift_norm': True  # <--- ç¡®ä¿è¿™é‡Œæ˜¯ True
    }
    
    # æ‰©æ•£é…ç½®
    diffusion_config = {
        'timestep_respacing': "",
        'noise_schedule': noise_schedule,
        'use_kl': False,
        'sigma_small': False,
        'predict_xstart': False,
        'learn_sigma': False,
        'rescale_learned_sigmas': False,
        'diffusion_steps': diffusion_steps,
        'rescale_timesteps': True,
    }
    
    # <<<< ä¿®æ”¹ï¼šæ ¹æ®è®­ç»ƒä¸­çš„è§‚å¯Ÿè°ƒæ•´ç¼©æ”¾å› å­ >>>>
    # åŸå§‹è®¡ç®— (åŸºäºå•æ‰¹æ¬¡): 0.0616 (æ¥è‡ª 1/16.2432)
    # è®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„ scaled_std â‰ˆ 0.54 æ—¶ï¼Œè¡¨ç¤º actual_raw_std â‰ˆ 0.54 / 0.0616 â‰ˆ 8.77
    # æ–°çš„ç¼©æ”¾å› å­ = 1 / actual_raw_std = 1 / 8.77 â‰ˆ 0.1140
    calculated_latent_scale_factor = 0.114 
    # <<<< ç»“æŸä¿®æ”¹ >>>>

    model = VAELatentDiffusionModel(
        vae_config=vae_config,
        unet_config=unet_params_for_model,
        diffusion_config=diffusion_config,
        vae_checkpoint_path=vae_checkpoint_path,
        device=device,
        latent_scale_factor=calculated_latent_scale_factor # <--- ä¼ é€’æ›´æ–°åçš„ç¼©æ”¾å› å­
    )
    
    return model 
