"""
VAE-LDM å®Œæ•´æ¨¡å‹
é›†æˆVAEç¼–ç å™¨/è§£ç å™¨å’Œæ‰©æ•£æ¨¡å‹çš„å®Œæ•´æ½œåœ¨æ‰©æ•£ç³»ç»Ÿ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple
from vae_diffusion import VAEDiffusionUNet
from gaussian_diffusion import create_diffusion
import sys
import os

# æ·»åŠ VAEè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))
from adv_vq_vae import AdvVQVAE

class VAELatentDiffusionModel(nn.Module):
    """
    å®Œæ•´çš„VAEæ½œåœ¨æ‰©æ•£æ¨¡å‹
    """
    def __init__(
        self,
        vae_config: Dict[str, Any],
        unet_config: Dict[str, Any],
        diffusion_config: Dict[str, Any],
        vae_checkpoint_path: str = None,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        super().__init__()
        
        self.device = device
        
        # 1. åˆå§‹åŒ–VAEï¼ˆå†»ç»“å‚æ•°ï¼‰
        self.vae = self._load_vae(vae_config, vae_checkpoint_path)
        
        # 2. åˆå§‹åŒ–U-Net
        self.unet = VAEDiffusionUNet(**unet_config)
        
        # 3. åˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹
        self.diffusion = create_diffusion(**diffusion_config)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(device)
        
        print(f"âœ… VAE-LDM åˆå§‹åŒ–å®Œæˆ")
        print(f"   VAEæ½œåœ¨ç»´åº¦: {self.vae.latent_dim}")
        print(f"   U-Netç±»åˆ«æ•°: {self.unet.num_classes}")
        print(f"   æ‰©æ•£æ­¥æ•°: {self.diffusion.num_timesteps}")
        
    def _load_vae(self, vae_config: Dict[str, Any], checkpoint_path: str) -> AdvVQVAE:
        """åŠ è½½é¢„è®­ç»ƒçš„VAE"""
        vae = AdvVQVAE(**vae_config)
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                print(f"ğŸ” æ£€æŸ¥ç‚¹æ–‡ä»¶é”®: {list(checkpoint.keys())}")
                
                # å°è¯•ä¸åŒçš„é”®åæ¥åŠ è½½æ¨¡å‹çŠ¶æ€
                state_dict = None
                
                # å°è¯•å¸¸è§çš„é”®å
                possible_keys = [
                    'model_state_dict',  # æ ‡å‡†æ ¼å¼
                    'state_dict',        # ç®€åŒ–æ ¼å¼
                    'model',             # å¦ä¸€ç§æ ¼å¼
                    'vae_state_dict',    # VAEç‰¹å®šæ ¼å¼
                ]
                
                for key in possible_keys:
                    if key in checkpoint:
                        state_dict = checkpoint[key]
                        print(f"âœ… ä½¿ç”¨é”® '{key}' åŠ è½½VAEæƒé‡")
                        break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„é”®ï¼Œæ£€æŸ¥æ˜¯å¦ç›´æ¥æ˜¯çŠ¶æ€å­—å…¸
                if state_dict is None:
                    # æ£€æŸ¥æ˜¯å¦æ•´ä¸ªcheckpointå°±æ˜¯çŠ¶æ€å­—å…¸
                    if isinstance(checkpoint, dict) and any(k.startswith(('encoder', 'decoder', 'quantize', 'quant_conv', 'post_quant_conv')) for k in checkpoint.keys()):
                        state_dict = checkpoint
                        print("âœ… æ£€æŸ¥ç‚¹ç›´æ¥æ˜¯çŠ¶æ€å­—å…¸æ ¼å¼")
                    else:
                        print(f"âŒ æ— æ³•æ‰¾åˆ°VAEçŠ¶æ€å­—å…¸ï¼Œå¯ç”¨é”®: {list(checkpoint.keys())}")
                        raise KeyError(f"æ— æ³•åœ¨æ£€æŸ¥ç‚¹ä¸­æ‰¾åˆ°æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚å¯ç”¨é”®: {list(checkpoint.keys())}")
                
                # åŠ è½½çŠ¶æ€å­—å…¸
                vae.load_state_dict(state_dict)
                print(f"âœ… VAE æƒé‡åŠ è½½æˆåŠŸ: {checkpoint_path}")
                
            except Exception as e:
                print(f"âŒ VAEåŠ è½½å¤±è´¥: {e}")
                print(f"ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„VAEæƒé‡")
                print("   è­¦å‘Šï¼šè¿™å¯èƒ½å¯¼è‡´è®­ç»ƒæ€§èƒ½ä¸‹é™")
        else:
            print(f"âš ï¸  VAE checkpoint æœªæ‰¾åˆ°: {checkpoint_path}")
            print("ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„VAEæƒé‡")
            
        # å†»ç»“VAEå‚æ•°
        for param in vae.parameters():
            param.requires_grad = False
        vae.eval()
        
        print(f"ğŸ”’ VAEå‚æ•°å·²å†»ç»“ï¼ˆ{sum(p.numel() for p in vae.parameters())} å‚æ•°ï¼‰")
        
        return vae
    
    @torch.no_grad()
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """
        å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ [-1, 1]
            
        Returns:
            latents: [B, latent_dim, h, w] æ½œåœ¨è¡¨ç¤º
        """
        self.vae.eval()
        
        # VAEç¼–ç 
        x_recon, vq_loss, perplexity, encodings, min_encoding_indices, z_q = self.vae(images)
        
        # è¿”å›é‡åŒ–åçš„æ½œåœ¨è¡¨ç¤º
        return z_q
    
    @torch.no_grad() 
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ
        
        Args:
            latents: [B, latent_dim, h, w] æ½œåœ¨è¡¨ç¤º
            
        Returns:
            images: [B, 3, H, W] é‡å»ºå›¾åƒ [-1, 1]
        """
        self.vae.eval()
        
        # VAEè§£ç 
        decoded = self.vae.decode(latents)
        return decoded
    
    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒå‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾
            
        Returns:
            loss_dict: åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
        """
        # 1. ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            latents = self.encode_to_latent(images)
        
        # 2. éšæœºé‡‡æ ·æ—¶é—´æ­¥
        batch_size = latents.shape[0]
        t = torch.randint(0, self.diffusion.num_timesteps, (batch_size,), device=self.device)
        
        # 3. è®¡ç®—æ‰©æ•£æŸå¤±
        model_kwargs = {}
        if class_labels is not None:
            model_kwargs['y'] = class_labels
            
        losses = self.diffusion.training_losses(
            model=self.unet,
            x_start=latents,
            t=t,
            model_kwargs=model_kwargs
        )
        
        return losses
    
    @torch.no_grad()
    def sample(
        self,
        num_samples: int,
        class_labels: Optional[torch.Tensor] = None,
        guidance_scale: float = 1.0,
        num_inference_steps: Optional[int] = None,
        use_ddim: bool = True,
        eta: float = 0.0,
        return_intermediates: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        ç”Ÿæˆæ ·æœ¬
        
        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            class_labels: [num_samples] ç±»åˆ«æ ‡ç­¾
            guidance_scale: åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å¼ºåº¦
            num_inference_steps: æ¨ç†æ­¥æ•°
            use_ddim: æ˜¯å¦ä½¿ç”¨DDIMé‡‡æ ·
            eta: DDIMéšæœºæ€§å‚æ•°
            return_intermediates: æ˜¯å¦è¿”å›ä¸­é—´ç»“æœ
            
        Returns:
            images: [num_samples, 3, H, W] ç”Ÿæˆçš„å›¾åƒ
            latents: [num_samples, latent_dim, h, w] å¯é€‰çš„æ½œåœ¨è¡¨ç¤º
        """
        self.unet.eval()
        
        # è·å–æ½œåœ¨ç©ºé—´å½¢çŠ¶
        with torch.no_grad():
            # ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥å›¾åƒå°ºå¯¸ï¼š256Ã—256 (æ•°æ®é›†çš„å®é™…å°ºå¯¸)
            dummy_img = torch.randn(1, 3, 256, 256, device=self.device) 
            dummy_latent = self.encode_to_latent(dummy_img)
            latent_shape = dummy_latent.shape[1:]
        
        # é‡‡æ ·å½¢çŠ¶
        shape = (num_samples, *latent_shape)
        
        # æ¨¡å‹å‚æ•°
        model_kwargs = {}
        if class_labels is not None:
            assert len(class_labels) == num_samples
            model_kwargs['y'] = class_labels
        
        # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
        if guidance_scale > 1.0 and class_labels is not None:
            # éœ€è¦å®ç°CFGï¼Œè¿™é‡Œå…ˆç®€åŒ–
            pass
        
        # é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        if use_ddim:
            latents = self.diffusion.ddim_sample_loop(
                model=self.unet,
                shape=shape,
                clip_denoised=True,
                model_kwargs=model_kwargs,
                device=self.device,
                progress=True,
                eta=eta
            )
        else:
            latents = self.diffusion.p_sample_loop(
                model=self.unet,
                shape=shape,
                clip_denoised=True,
                model_kwargs=model_kwargs,
                device=self.device,
                progress=True
            )
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        images = self.decode_from_latent(latents)
        
        if return_intermediates:
            return images, latents
        else:
            return images, None
    
    def configure_optimizers(self, lr: float = 1e-4, weight_decay: float = 0.0):
        """é…ç½®ä¼˜åŒ–å™¨"""
        # åªä¼˜åŒ–U-Netå‚æ•°ï¼ŒVAEå‚æ•°è¢«å†»ç»“
        optimizer = torch.optim.AdamW(
            self.unet.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        return optimizer
    
    def save_checkpoint(self, filepath: str, epoch: int, optimizer_state: Dict = None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'unet_state_dict': self.unet.state_dict(),
            'model_config': {
                'unet_config': {
                    'image_size': self.unet.image_size,
                    'in_channels': self.unet.in_channels,
                    'model_channels': self.unet.model_channels,
                    'out_channels': self.unet.out_channels,
                    'num_res_blocks': self.unet.num_res_blocks,
                    'attention_resolutions': self.unet.attention_resolutions,
                    'dropout': self.unet.dropout,
                    'channel_mult': self.unet.channel_mult,
                    'num_classes': self.unet.num_classes,
                    'use_checkpoint': self.unet.use_checkpoint,
                    'num_heads': self.unet.num_heads,
                    'num_heads_upsample': self.unet.num_heads_upsample,
                },
                'diffusion_config': {
                    'timestep_respacing': "",
                    'noise_schedule': "cosine",
                    'diffusion_steps': self.diffusion.num_timesteps,
                }
            }
        }
        
        if optimizer_state is not None:
            checkpoint['optimizer_state_dict'] = optimizer_state
            
        torch.save(checkpoint, filepath)
        print(f"âœ… Checkpoint saved: {filepath}")
    
    def load_checkpoint(self, filepath: str, load_optimizer: bool = False):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # åŠ è½½U-Netæƒé‡
        self.unet.load_state_dict(checkpoint['unet_state_dict'])
        
        epoch = checkpoint.get('epoch', 0)
        optimizer_state = checkpoint.get('optimizer_state_dict', None) if load_optimizer else None
        
        print(f"âœ… Checkpoint loaded: {filepath} (epoch {epoch})")
        
        return epoch, optimizer_state

def create_vae_ldm(
    vae_checkpoint_path: str,
    image_size: int = 32,
    num_classes: int = 31,
    diffusion_steps: int = 1000,
    noise_schedule: str = "cosine",
    device: str = None
) -> VAELatentDiffusionModel:
    """
    åˆ›å»ºVAE-LDMæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        vae_checkpoint_path: VAEæ£€æŸ¥ç‚¹è·¯å¾„
        image_size: å›¾åƒå°ºå¯¸
        num_classes: ç±»åˆ«æ•°
        diffusion_steps: æ‰©æ•£æ­¥æ•°
        noise_schedule: å™ªå£°è°ƒåº¦
        device: è®¾å¤‡
        
    Returns:
        VAE-LDMæ¨¡å‹
    """
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # VAEé…ç½®ï¼ˆéœ€è¦ä¸é¢„è®­ç»ƒæ¨¡å‹åŒ¹é…ï¼‰
    vae_config = {
        'in_channels': 3,
        'latent_dim': 256,
        'num_embeddings': 512,
        'beta': 0.25,
        'groups': 1,
    }
    
    # U-Neté…ç½®
    unet_config = {
        'image_size': image_size,
        'in_channels': 256,  # VAEæ½œåœ¨ç»´åº¦
        'model_channels': 128,
        'out_channels': 256,  # VAEæ½œåœ¨ç»´åº¦
        'num_res_blocks': 2,
        'attention_resolutions': (8, 16),
        'dropout': 0.1,
        'channel_mult': (1, 2, 4, 8),
        'conv_resample': True,
        'dims': 2,
        'num_classes': num_classes,
        'use_checkpoint': False,
        'num_heads': 4,
        'num_heads_upsample': -1,
        'use_scale_shift_norm': True,
    }
    
    # æ‰©æ•£é…ç½®
    diffusion_config = {
        'timestep_respacing': "",
        'noise_schedule': noise_schedule,
        'use_kl': False,
        'sigma_small': False,
        'predict_xstart': False,
        'learn_sigma': False,
        'rescale_learned_sigmas': False,
        'diffusion_steps': diffusion_steps,
        'rescale_timesteps': True,
    }
    
    model = VAELatentDiffusionModel(
        vae_config=vae_config,
        unet_config=unet_config,
        diffusion_config=diffusion_config,
        vae_checkpoint_path=vae_checkpoint_path,
        device=device
    )
    
    return model 
