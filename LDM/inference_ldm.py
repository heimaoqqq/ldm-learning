#!/usr/bin/env python3
"""
è½»é‡åŒ–StableLDMæ¨ç†è„šæœ¬
å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒ
"""

import os
import yaml
import torch
import torchvision.utils as vutils
import sys
from typing import Optional

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from stable_ldm import create_stable_ldm

def load_model(config_path: str, checkpoint_path: str, device: str = 'auto') -> torch.nn.Module:
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è®¾å¤‡é…ç½®
    if device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device)
    
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_stable_ldm(config).to(device)
    
    # åŠ è½½æƒé‡
    unet_path = os.path.join(checkpoint_path, 'unet.pth')
    if os.path.exists(unet_path):
        state_dict = torch.load(unet_path, map_location=device)
        model.unet.load_state_dict(state_dict)
        print(f"âœ… å·²åŠ è½½U-Netæƒé‡: {unet_path}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°U-Netæƒé‡æ–‡ä»¶: {unet_path}")
    
    # åŠ è½½EMAæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
    ema_path = os.path.join(checkpoint_path, 'ema.pth')
    if os.path.exists(ema_path) and hasattr(model, 'ema') and model.ema is not None:
        ema_state = torch.load(ema_path, map_location=device)
        model.ema.shadow = ema_state
        print(f"âœ… å·²åŠ è½½EMAæƒé‡: {ema_path}")
    
    model.eval()
    return model, config, device

def generate_samples(
    model,
    config,
    device,
    num_samples_per_class: int = 4,
    num_classes_to_show: int = 8,
    num_inference_steps: int = 50,
    guidance_scale: float = 7.5,
    use_ema: bool = False,
    output_dir: str = 'generated_samples'
):
    """ç”Ÿæˆæ ·æœ¬å›¾åƒ"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆæ ·æœ¬...")
    print(f"   æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"   CFGå¼ºåº¦: {guidance_scale}")
    print(f"   ä½¿ç”¨EMA: {'âœ“' if use_ema else 'âœ—'}")
    
    all_images = []
    all_labels = []
    
    total_classes = config['unet']['num_classes']
    classes_to_generate = min(num_classes_to_show, total_classes)
    
    with torch.no_grad():
        for class_id in range(classes_to_generate):
            print(f"   ç”Ÿæˆç±»åˆ« {class_id+1}/{classes_to_generate}...")
            
            class_labels = torch.tensor([class_id] * num_samples_per_class, device=device)
            
            generated_images = model.sample(
                batch_size=num_samples_per_class,
                class_labels=class_labels,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                use_ema=use_ema,
                verbose=False
            )
            
            all_images.append(generated_images)
            all_labels.extend([class_id] * num_samples_per_class)
    
    # æ‹¼æ¥æ‰€æœ‰å›¾åƒ
    if all_images:
        all_images = torch.cat(all_images, dim=0)
        
        # ä¿å­˜å›¾åƒç½‘æ ¼
        grid = vutils.make_grid(
            all_images, 
            nrow=num_samples_per_class, 
            normalize=False, 
            padding=2
        )
        
        grid_path = os.path.join(output_dir, 'sample_grid.png')
        vutils.save_image(grid, grid_path)
        print(f"âœ… æ ·æœ¬ç½‘æ ¼å·²ä¿å­˜: {grid_path}")
        
        # ä¿å­˜å•ä¸ªå›¾åƒ
        for i, (image, label) in enumerate(zip(all_images, all_labels)):
            single_path = os.path.join(output_dir, f'sample_class_{label}_idx_{i%num_samples_per_class}.png')
            vutils.save_image(image, single_path)
        
        print(f"âœ… å…±ç”Ÿæˆ {len(all_images)} å¼ å›¾åƒ")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {output_dir}")

def generate_interpolation(
    model,
    config, 
    device,
    class_a: int,
    class_b: int,
    num_steps: int = 8,
    num_inference_steps: int = 50,
    guidance_scale: float = 7.5,
    use_ema: bool = False,
    output_dir: str = 'generated_samples'
):
    """ç”Ÿæˆç±»åˆ«é—´æ’å€¼"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ”„ ç”Ÿæˆç±»åˆ«æ’å€¼: {class_a} â†’ {class_b}")
    
    # åˆ›å»ºæ’å€¼æƒé‡
    weights = torch.linspace(0, 1, num_steps)
    
    interpolated_images = []
    
    with torch.no_grad():
        for i, w in enumerate(weights):
            print(f"   æ’å€¼æ­¥éª¤ {i+1}/{num_steps} (æƒé‡: {w:.2f})")
            
            # æ··åˆç±»åˆ«åµŒå…¥ï¼ˆéœ€è¦ä¿®æ”¹æ¨¡å‹ä»¥æ”¯æŒè¿ç»­ç±»åˆ«åµŒå…¥ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºé€‰æ‹©ç¦»æ•£ç±»åˆ«
            current_class = class_a if w < 0.5 else class_b
            class_labels = torch.tensor([current_class], device=device)
            
            generated_image = model.sample(
                batch_size=1,
                class_labels=class_labels,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                use_ema=use_ema,
                verbose=False
            )
            
            interpolated_images.append(generated_image)
    
    # ä¿å­˜æ’å€¼åºåˆ—
    if interpolated_images:
        all_interp = torch.cat(interpolated_images, dim=0)
        
        # æ°´å¹³æ’åˆ—
        grid = vutils.make_grid(all_interp, nrow=num_steps, normalize=False, padding=2)
        
        interp_path = os.path.join(output_dir, f'interpolation_{class_a}_to_{class_b}.png')
        vutils.save_image(grid, interp_path)
        print(f"âœ… æ’å€¼åºåˆ—å·²ä¿å­˜: {interp_path}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='StableLDMæ¨ç†è„šæœ¬')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--output', type=str, default='generated_samples', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num_samples', type=int, default=4, help='æ¯ä¸ªç±»åˆ«ç”Ÿæˆçš„æ ·æœ¬æ•°')
    parser.add_argument('--num_classes', type=int, default=8, help='æ˜¾ç¤ºçš„ç±»åˆ«æ•°')
    parser.add_argument('--steps', type=int, default=50, help='æ¨ç†æ­¥æ•°')
    parser.add_argument('--cfg', type=float, default=7.5, help='CFGå¼•å¯¼å¼ºåº¦')
    parser.add_argument('--use_ema', action='store_true', help='ä½¿ç”¨EMAæƒé‡')
    parser.add_argument('--interpolation', action='store_true', help='ç”Ÿæˆæ’å€¼æ ·æœ¬')
    parser.add_argument('--class_a', type=int, default=0, help='æ’å€¼èµ·å§‹ç±»åˆ«')
    parser.add_argument('--class_b', type=int, default=5, help='æ’å€¼ç»“æŸç±»åˆ«')
    
    args = parser.parse_args()
    
    print("ğŸ¯ è½»é‡åŒ–StableLDMæ¨ç†")
    print("=" * 50)
    
    # åŠ è½½æ¨¡å‹
    model, config, device = load_model(args.config, args.checkpoint)
    
    if args.interpolation:
        # ç”Ÿæˆæ’å€¼
        generate_interpolation(
            model, config, device,
            class_a=args.class_a,
            class_b=args.class_b,
            num_steps=8,
            num_inference_steps=args.steps,
            guidance_scale=args.cfg,
            use_ema=args.use_ema,
            output_dir=args.output
        )
    else:
        # ç”Ÿæˆå¸¸è§„æ ·æœ¬
        generate_samples(
            model, config, device,
            num_samples_per_class=args.num_samples,
            num_classes_to_show=args.num_classes,
            num_inference_steps=args.steps,
            guidance_scale=args.cfg,
            use_ema=args.use_ema,
            output_dir=args.output
        )
    
    print("=" * 50)
    print("ğŸ‰ æ¨ç†å®Œæˆï¼")

if __name__ == "__main__":
    main() 
