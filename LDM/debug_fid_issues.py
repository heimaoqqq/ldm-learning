"""
VAE-LDM FIDè¯Šæ–­è„šæœ¬
åˆ†æFIDåˆ†æ•°é«˜çš„å…·ä½“åŸå›  - æœ¬åœ°ç‰ˆæœ¬
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision.utils import save_image, make_grid
from PIL import Image
import yaml

# æ·»åŠ VAEè·¯å¾„
sys.path.append('../VAE')

from vae_ldm import create_vae_ldm
from fid_evaluation import FIDEvaluator
from metrics import denormalize_for_metrics

# ç®€åŒ–çš„æ•°æ®é›†ç±»ï¼ˆé¿å…pickleé—®é¢˜ï¼‰
class SimpleSyntheticDataset:
    """ç®€å•çš„åˆæˆæ•°æ®é›†ï¼Œé¿å…å¤šè¿›ç¨‹pickleé—®é¢˜"""
    def __init__(self, num_samples=1000, num_classes=31):
        self.num_samples = num_samples
        self.num_classes = num_classes
        
        # ç”Ÿæˆå›ºå®šçš„åˆæˆæ•°æ®
        torch.manual_seed(42)
        self.images = torch.randn(num_samples, 3, 256, 256)
        self.labels = torch.randint(0, num_classes, (num_samples,))
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        self.images = (self.images - self.images.mean()) / self.images.std()
        self.images = torch.clamp(self.images, -3, 3) / 3  # é™åˆ¶èŒƒå›´
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return {
            'image': self.images[idx],
            'label': self.labels[idx]
        }

class FIDDiagnostics:
    """FIDé—®é¢˜è¯Šæ–­å™¨ - æœ¬åœ°ç‰ˆæœ¬"""
    
    def __init__(self, config_path="config_local_debug.yaml"):
        # åŠ è½½é…ç½®
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.config = self._create_default_config()
        
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸ”§ åŠ è½½æ¨¡å‹...")
        try:
            self.model = create_vae_ldm(
                vae_checkpoint_path=self.config['paths']['vae_checkpoint'],
                image_size=self.config['model']['image_size'],
                num_classes=self.config['model']['num_classes'],
                diffusion_steps=self.config['model']['diffusion_steps'],
                noise_schedule=self.config['model']['noise_schedule'],
                device=self.device
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹...")
            self.model = create_vae_ldm(
                vae_checkpoint_path=None,  # ä¸åŠ è½½VAE checkpoint
                image_size=32,
                num_classes=31,
                diffusion_steps=1000,
                noise_schedule="cosine",
                device=self.device
            )
        
        # åˆ›å»ºç®€å•æ•°æ®é›†ï¼ˆé¿å…æ•°æ®åŠ è½½é—®é¢˜ï¼‰
        print("ğŸ“Š åˆ›å»ºåˆæˆæ•°æ®é›†...")
        self.dataset = SimpleSyntheticDataset(num_samples=200, num_classes=31)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.debug_dir = "debug_output"
        os.makedirs(self.debug_dir, exist_ok=True)
        
    def _create_default_config(self):
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        return {
            'paths': {
                'vae_checkpoint': None,
                'data_dir': './data',
                'save_dir': './debug_checkpoints'
            },
            'model': {
                'num_classes': 31,
                'image_size': 32,
                'diffusion_steps': 1000,
                'noise_schedule': 'cosine'
            }
        }
        
    def analyze_dataset(self):
        """åˆ†ææ•°æ®é›†"""
        print("\nğŸ“Š åˆ†ææ•°æ®é›†...")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        class_counts = {}
        pixel_stats = []
        
        # é‡‡æ ·ä¸€äº›æ•°æ®è¿›è¡Œåˆ†æ
        sample_size = min(100, len(self.dataset))
        for i in range(sample_size):
            data = self.dataset[i]
            image = data['image']
            label = data['label'].item()
            
            class_counts[label] = class_counts.get(label, 0) + 1
            pixel_stats.extend(image.flatten().tolist())
        
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {len(class_counts)} ä¸ªç±»åˆ«")
        if class_counts:
            print(f"   æ ·æœ¬æ•°æœ€å¤šçš„ç±»åˆ«: {max(class_counts.values())}")
            print(f"   æ ·æœ¬æ•°æœ€å°‘çš„ç±»åˆ«: {min(class_counts.values())}")
        
        print(f"   åƒç´ å€¼èŒƒå›´: [{min(pixel_stats):.3f}, {max(pixel_stats):.3f}]")
        print(f"   åƒç´ å€¼å‡å€¼: {np.mean(pixel_stats):.3f}")
        print(f"   åƒç´ å€¼æ ‡å‡†å·®: {np.std(pixel_stats):.3f}")
        
        # ä¿å­˜æ ·æœ¬å›¾åƒ
        sample_images = torch.stack([self.dataset[i]['image'] for i in range(min(16, len(self.dataset)))])
        display_images = denormalize_for_metrics(sample_images)
        
        save_image(
            display_images, 
            os.path.join(self.debug_dir, "real_samples.png"),
            nrow=4, 
            normalize=False
        )
        print(f"   ğŸ’¾ çœŸå®å›¾åƒæ ·æœ¬å·²ä¿å­˜: {self.debug_dir}/real_samples.png")
        
        return class_counts, pixel_stats
    
    def test_vae_reconstruction(self):
        """æµ‹è¯•VAEé‡å»ºè´¨é‡"""
        print("\nğŸ”§ æµ‹è¯•VAEé‡å»º...")
        
        with torch.no_grad():
            # è·å–ä¸€äº›æ ·æœ¬å›¾åƒ
            sample_images = torch.stack([self.dataset[i]['image'] for i in range(8)])
            original_images = sample_images.to(self.device)
            
            # ç¼–ç -è§£ç 
            latents = self.model.encode_to_latent(original_images)
            reconstructed = self.model.decode_from_latent(latents)
            
            # è®¡ç®—é‡å»ºè¯¯å·®
            mse_loss = F.mse_loss(reconstructed, original_images).item()
            l1_loss = F.l1_loss(reconstructed, original_images).item()
            
            print(f"   VAEé‡å»ºMSE: {mse_loss:.6f}")
            print(f"   VAEé‡å»ºL1: {l1_loss:.6f}")
            
            # ä¿å­˜å¯¹æ¯”å›¾åƒ
            comparison = torch.cat([
                denormalize_for_metrics(original_images),
                denormalize_for_metrics(reconstructed)
            ], dim=0)
            
            save_image(
                comparison,
                os.path.join(self.debug_dir, "vae_reconstruction.png"),
                nrow=8,
                normalize=False
            )
            print(f"   ğŸ’¾ VAEé‡å»ºå¯¹æ¯”å·²ä¿å­˜: {self.debug_dir}/vae_reconstruction.png")
            
            return mse_loss, l1_loss
    
    def test_generation_quality(self):
        """æµ‹è¯•ç”Ÿæˆè´¨é‡"""
        print("\nğŸ¨ æµ‹è¯•ç”Ÿæˆè´¨é‡...")
        
        # æµ‹è¯•ä¸åŒé…ç½®
        configurations = [
            {"steps": 50, "eta": 0.0, "name": "ddim_50_deterministic"},
            {"steps": 100, "eta": 0.0, "name": "ddim_100_deterministic"},
            {"steps": 50, "eta": 0.3, "name": "ddim_50_stochastic"},
        ]
        
        results = {}
        
        for config in configurations:
            print(f"   æµ‹è¯•é…ç½®: {config['name']}")
            
            # ç”Ÿæˆå›¾åƒ
            num_samples = 8  # å‡å°‘æ ·æœ¬æ•°ç”¨äºè°ƒè¯•
            class_labels = torch.randint(0, 31, (num_samples,), device=self.device)
            
            with torch.no_grad():
                try:
                    generated_images, _ = self.model.sample(
                        num_samples=num_samples,
                        class_labels=class_labels,
                        use_ddim=True,
                        eta=config['eta']
                    )
                except Exception as e:
                    print(f"     âš ï¸ ç”Ÿæˆå¤±è´¥: {e}")
                    continue
            
            # æ£€æŸ¥ç”Ÿæˆå›¾åƒçš„ç»Ÿè®¡ä¿¡æ¯
            gen_mean = generated_images.mean().item()
            gen_std = generated_images.std().item()
            gen_min = generated_images.min().item()
            gen_max = generated_images.max().item()
            
            results[config['name']] = {
                'mean': gen_mean,
                'std': gen_std,
                'min': gen_min,
                'max': gen_max
            }
            
            print(f"     ç”Ÿæˆå›¾åƒç»Ÿè®¡: å‡å€¼={gen_mean:.3f}, æ ‡å‡†å·®={gen_std:.3f}")
            print(f"     èŒƒå›´: [{gen_min:.3f}, {gen_max:.3f}]")
            
            # ä¿å­˜ç”Ÿæˆå›¾åƒ
            display_images = denormalize_for_metrics(generated_images)
            save_image(
                display_images,
                os.path.join(self.debug_dir, f"generated_{config['name']}.png"),
                nrow=4,
                normalize=False
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if torch.isnan(generated_images).any():
                print(f"     âš ï¸ å‘ç°NaNå€¼ï¼")
            if torch.isinf(generated_images).any():
                print(f"     âš ï¸ å‘ç°æ— ç©·å¤§å€¼ï¼")
        
        return results
    
    def estimate_fid_score(self):
        """ç®€åŒ–çš„FIDä¼°ç®—"""
        print("\nğŸ“ˆ ä¼°ç®—FIDåˆ†æ•°...")
        
        try:
            # åˆ›å»ºFIDè¯„ä¼°å™¨
            fid_evaluator = FIDEvaluator(device=self.device)
            
            # å‡†å¤‡çœŸå®å›¾åƒ
            real_images = torch.stack([self.dataset[i]['image'] for i in range(min(50, len(self.dataset)))])
            real_images_norm = denormalize_for_metrics(real_images)
            
            # æå–çœŸå®å›¾åƒç‰¹å¾
            real_features = fid_evaluator.feature_extractor(real_images_norm.to(self.device))
            real_features = real_features.cpu().numpy()
            
            # ç”Ÿæˆå›¾åƒ
            with torch.no_grad():
                generated_images, _ = self.model.sample(
                    num_samples=50,
                    class_labels=torch.randint(0, 31, (50,), device=self.device),
                    use_ddim=True,
                    eta=0.0
                )
            
            gen_images_norm = denormalize_for_metrics(generated_images)
            gen_features = fid_evaluator.feature_extractor(gen_images_norm).cpu().numpy()
            
            # è®¡ç®—ç»Ÿè®¡é‡
            real_mu = real_features.mean(axis=0)
            real_sigma = np.cov(real_features, rowvar=False)
            gen_mu = gen_features.mean(axis=0)
            gen_sigma = np.cov(gen_features, rowvar=False)
            
            # è®¡ç®—FIDï¼ˆç®€åŒ–ç‰ˆï¼‰
            diff = real_mu - gen_mu
            diff_norm = np.linalg.norm(diff)
            
            # åæ–¹å·®çŸ©é˜µçš„è¿¹
            trace_real = np.trace(real_sigma)
            trace_gen = np.trace(gen_sigma)
            
            # ç®€åŒ–çš„FIDä¼°ç®—
            estimated_fid = diff_norm + abs(trace_real - trace_gen)
            
            print(f"   ç‰¹å¾å·®å¼‚èŒƒæ•°: {diff_norm:.2f}")
            print(f"   åæ–¹å·®è¿¹å·®å¼‚: {abs(trace_real - trace_gen):.2f}")
            print(f"   ä¼°ç®—FIDåˆ†æ•°: {estimated_fid:.2f}")
            
            return estimated_fid
            
        except Exception as e:
            print(f"   âš ï¸ FIDä¼°ç®—å¤±è´¥: {e}")
            return None
    
    def check_model_weights(self):
        """æ£€æŸ¥æ¨¡å‹æƒé‡"""
        print("\nğŸ” æ£€æŸ¥æ¨¡å‹æƒé‡...")
        
        total_params = sum(p.numel() for p in self.model.unet.parameters())
        trainable_params = sum(p.numel() for p in self.model.unet.parameters() if p.requires_grad)
        
        print(f"   æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # æ£€æŸ¥å…³é”®å±‚çš„æƒé‡ç»Ÿè®¡
        layer_count = 0
        abnormal_layers = []
        
        for name, param in self.model.unet.named_parameters():
            if param.requires_grad:
                layer_count += 1
                mean_val = param.data.mean().item()
                std_val = param.data.std().item()
                
                # æ£€æŸ¥å¼‚å¸¸å€¼
                if torch.isnan(param.data).any() or torch.isinf(param.data).any():
                    abnormal_layers.append(f"{name}: NaNæˆ–æ— ç©·å¤§")
                elif abs(mean_val) > 10 or std_val > 10:
                    abnormal_layers.append(f"{name}: å¼‚å¸¸åˆ†å¸ƒ (å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f})")
        
        print(f"   æ£€æŸ¥äº† {layer_count} ä¸ªå±‚")
        if abnormal_layers:
            print("   âš ï¸ å‘ç°å¼‚å¸¸å±‚:")
            for layer in abnormal_layers[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     {layer}")
        else:
            print("   âœ… æ‰€æœ‰æƒé‡çœ‹èµ·æ¥æ­£å¸¸")
    
    def run_full_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸ” å¼€å§‹VAE-LDM FIDè¯Šæ–­ï¼ˆæœ¬åœ°ç‰ˆæœ¬ï¼‰...")
        print("=" * 60)
        
        try:
            # 1. æ•°æ®é›†åˆ†æ
            class_counts, pixel_stats = self.analyze_dataset()
            
            # 2. VAEé‡å»ºæµ‹è¯•
            vae_mse, vae_l1 = self.test_vae_reconstruction()
            
            # 3. ç”Ÿæˆè´¨é‡æµ‹è¯•
            generation_results = self.test_generation_quality()
            
            # 4. FIDä¼°ç®—
            estimated_fid = self.estimate_fid_score()
            
            # 5. æ¨¡å‹æƒé‡æ£€æŸ¥
            self.check_model_weights()
            
            print("\n" + "=" * 60)
            print("ğŸ“‹ è¯Šæ–­æ€»ç»“:")
            print(f"   VAEé‡å»ºMSE: {vae_mse:.6f}")
            if estimated_fid:
                print(f"   ä¼°ç®—FIDåˆ†æ•°: {estimated_fid:.2f}")
            
            # è¯Šæ–­å»ºè®®
            print("\nğŸ’¡ é’ˆå¯¹FID=300+çš„é—®é¢˜åˆ†æ:")
            
            if vae_mse > 0.1:
                print("   âš ï¸ VAEé‡å»ºè´¨é‡å·®ï¼Œè¿™å¯èƒ½æ˜¯ä¸»è¦åŸå› ")
                print("     å»ºè®®: é‡æ–°è®­ç»ƒVAEæˆ–æ£€æŸ¥VAEæƒé‡")
            
            if estimated_fid and estimated_fid > 100:
                print("   âš ï¸ ä¼°ç®—FIDåˆ†æ•°å¾ˆé«˜ï¼Œå¯èƒ½åŸå› :")
                print("     1. æ¨¡å‹è®­ç»ƒä¸å……åˆ†ï¼ˆéœ€è¦æ›´å¤šepochï¼‰")
                print("     2. DDIMé‡‡æ ·æ­¥æ•°ä¸è¶³ï¼ˆå½“å‰50æ­¥ï¼Œå»ºè®®100+æ­¥ï¼‰")
                print("     3. ç¼ºå°‘åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰")
                print("     4. æ½œåœ¨ç©ºé—´ç¼©æ”¾å› å­ä¸åˆé€‚")
                print("     5. è®­ç»ƒæ•°æ®è´¨é‡é—®é¢˜")
            
            if abs(np.mean(pixel_stats)) > 0.1:
                print("   âš ï¸ æ•°æ®é¢„å¤„ç†å¯èƒ½æœ‰é—®é¢˜")
                print("     å½“å‰æ•°æ®å‡å€¼åç¦»0ï¼Œæ£€æŸ¥å½’ä¸€åŒ–")
            
            print(f"\nğŸ¯ è°ƒè¯•è¾“å‡ºå·²ä¿å­˜è‡³: {self.debug_dir}/")
            print("\nğŸš€ å»ºè®®çš„æ”¹è¿›æªæ–½:")
            print("   1. å¢åŠ DDIMé‡‡æ ·æ­¥æ•°åˆ°100+")
            print("   2. å¯ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆguidance_scale=2.0-7.5ï¼‰")
            print("   3. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆå½“å‰å¯èƒ½è®­ç»ƒä¸è¶³ï¼‰")
            print("   4. æ£€æŸ¥VAEé‡å»ºè´¨é‡")
            print("   5. éªŒè¯æ•°æ®é¢„å¤„ç†æµç¨‹")
            
        except Exception as e:
            print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    diagnostics = FIDDiagnostics()
    diagnostics.run_full_diagnosis() 
