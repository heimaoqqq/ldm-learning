"""
Complete LDM Model - åŸºäºCompVis/latent-diffusion
é›†æˆAutoencoderKLã€U-Netå’Œæ‰©æ•£è¿‡ç¨‹çš„å®Œæ•´æ½œåœ¨æ‰©æ•£æ¨¡å‹
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
from typing import Dict, Any, Optional, Tuple, Union
from diffusers import AutoencoderKL
from ldm_unet import create_ldm_unet, LDMUNet
from ldm_diffusion import create_ldm_diffusion, LDMDiffusion


class LatentDiffusionModel(nn.Module):
    """
    å®Œæ•´çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹
    é›†æˆVAEç¼–ç å™¨/è§£ç å™¨å’Œæ‰©æ•£U-Net
    """
    def __init__(
        self,
        vae_checkpoint_path: str = None,
        unet_config: Dict[str, Any] = None,
        diffusion_config: Dict[str, Any] = None,
        device: str = 'cuda',
        scaling_factor: float = 0.18215,  # AutoencoderKLçš„æ ‡å‡†ç¼©æ”¾å› å­
    ):
        super().__init__()
        
        self.device = device
        self.scaling_factor = scaling_factor
        
        # é»˜è®¤é…ç½®
        if unet_config is None:
            unet_config = {
                'in_channels': 4,
                'out_channels': 4,
                'model_channels': 320,
                'num_res_blocks': 2,
                'attention_resolutions': (4, 2, 1),
                'channel_mult': (1, 2, 4, 4),
                'num_classes': 31,
                'dropout': 0.0,
                'num_heads': 8,
                'use_scale_shift_norm': True,
                'resblock_updown': True,
            }
        
        if diffusion_config is None:
            diffusion_config = {
                'timesteps': 1000,
                'beta_schedule': 'cosine',
                'beta_start': 0.0001,
                'beta_end': 0.02,
                'device': device
            }
        
        # 1. åˆå§‹åŒ–VAEï¼ˆå†»ç»“å‚æ•°ï¼‰
        print("ğŸ“¦ åŠ è½½AutoencoderKL...")
        self.vae = self._load_vae(vae_checkpoint_path)
        
        # 2. åˆå§‹åŒ–U-Net
        print("ğŸ”§ åˆå§‹åŒ–U-Net...")
        self.unet = create_ldm_unet(**unet_config)
        
        # 3. åˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹
        print("ğŸŒŠ åˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹...")
        self.diffusion = create_ldm_diffusion(**diffusion_config)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(device)
        
        # å­˜å‚¨é…ç½®
        self.unet_config = unet_config
        self.diffusion_config = diffusion_config
        
        print(f"âœ… LDMåˆå§‹åŒ–å®Œæˆ")
        print(f"   VAEç¼©æ”¾å› å­: {self.scaling_factor}")
        print(f"   U-Netè¾“å…¥é€šé“: {self.unet.in_channels}")
        print(f"   æ‰©æ•£æ—¶é—´æ­¥æ•°: {self.diffusion.timesteps}")

    def _load_vae(self, checkpoint_path: str = None) -> AutoencoderKL:
        """åŠ è½½é¢„è®­ç»ƒçš„AutoencoderKL"""
        # é¦–å…ˆåŠ è½½åŸºç¡€çš„Stable Diffusion VAE
        vae = AutoencoderKL.from_pretrained(
            "runwayml/stable-diffusion-v1-5", 
            subfolder="vae"
        )
        
        # å¦‚æœæä¾›äº†å¾®è°ƒçš„æ£€æŸ¥ç‚¹ï¼ŒåŠ è½½å¾®è°ƒæƒé‡
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                print(f"ğŸ”„ åŠ è½½å¾®è°ƒVAEæƒé‡: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # å°è¯•ä¸åŒçš„é”®åæ¥åŠ è½½VAEçŠ¶æ€å­—å…¸
                vae_state_dict = None
                possible_keys = [
                    'vae_state_dict',    # æ¥è‡ªVAEå¾®è°ƒè„šæœ¬
                    'model_state_dict',  # é€šç”¨æ ¼å¼
                    'state_dict',        # ç®€åŒ–æ ¼å¼
                ]
                
                for key in possible_keys:
                    if key in checkpoint:
                        vae_state_dict = checkpoint[key]
                        print(f"âœ… ä½¿ç”¨é”® '{key}' åŠ è½½VAEæƒé‡")
                        break
                
                if vae_state_dict is None:
                    # æ£€æŸ¥æ˜¯å¦æ•´ä¸ªcheckpointå°±æ˜¯çŠ¶æ€å­—å…¸
                    if isinstance(checkpoint, dict) and any(
                        k.startswith(('encoder', 'decoder', 'quant_conv', 'post_quant_conv')) 
                        for k in checkpoint.keys()
                    ):
                        vae_state_dict = checkpoint
                        print("âœ… æ£€æŸ¥ç‚¹ç›´æ¥æ˜¯VAEçŠ¶æ€å­—å…¸æ ¼å¼")
                    else:
                        print(f"âŒ æ— æ³•æ‰¾åˆ°VAEçŠ¶æ€å­—å…¸ï¼Œå¯ç”¨é”®: {list(checkpoint.keys())}")
                        raise KeyError("æ— æ³•åœ¨æ£€æŸ¥ç‚¹ä¸­æ‰¾åˆ°VAEçŠ¶æ€å­—å…¸")
                
                # åŠ è½½çŠ¶æ€å­—å…¸
                vae.load_state_dict(vae_state_dict)
                print(f"âœ… å¾®è°ƒVAEæƒé‡åŠ è½½æˆåŠŸ")
                
                # å¦‚æœæ£€æŸ¥ç‚¹ä¸­æœ‰è®­ç»ƒå†å²ï¼Œæ‰“å°ä¸€äº›ä¿¡æ¯
                if 'train_history' in checkpoint:
                    history = checkpoint['train_history']
                    if 'epoch' in history and len(history['epoch']) > 0:
                        last_epoch = history['epoch'][-1]
                        if 'recon_loss' in history and len(history['recon_loss']) > 0:
                            last_recon_loss = history['recon_loss'][-1]
                            print(f"   å¾®è°ƒä¿¡æ¯: Epoch {last_epoch}, é‡å»ºæŸå¤±: {last_recon_loss:.4f}")
                        
            except Exception as e:
                print(f"âŒ å¾®è°ƒVAEæƒé‡åŠ è½½å¤±è´¥: {e}")
                print(f"ğŸ”„ ä½¿ç”¨é¢„è®­ç»ƒVAEæƒé‡")
        else:
            print(f"âš ï¸  æœªæä¾›VAE checkpointè·¯å¾„æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é¢„è®­ç»ƒVAEæƒé‡")
        
        # å†»ç»“VAEå‚æ•°
        for param in vae.parameters():
            param.requires_grad = False
        vae.eval()
        
        print(f"ğŸ”’ VAEå‚æ•°å·²å†»ç»“ ({sum(p.numel() for p in vae.parameters()):,} å‚æ•°)")
        
        return vae

    @torch.no_grad()
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """
        å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒï¼ŒèŒƒå›´[-1, 1]
            
        Returns:
            latents: [B, 4, h, w] æ½œåœ¨è¡¨ç¤ºï¼Œå·²ç»æŒ‰scaling_factorç¼©æ”¾
        """
        self.vae.eval()
        
        # VAEç¼–ç 
        posterior = self.vae.encode(images).latent_dist
        latents = posterior.sample()  # [B, 4, h, w]
        
        # âš ï¸ é‡è¦ä¿®å¤ï¼šdiffusers.AutoencoderKLçš„sample()æ–¹æ³•å¹¶ä¸ä¼šè‡ªåŠ¨åº”ç”¨scaling_factor
        # éœ€è¦æ‰‹åŠ¨ä¹˜ä»¥scaling_factoræ¥å°†æ½œå˜é‡ç¼©æ”¾åˆ°åˆé€‚çš„èŒƒå›´ä¾›æ‰©æ•£æ¨¡å‹ä½¿ç”¨
        latents = latents * self.scaling_factor
        
        return latents

    @torch.no_grad()
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ
        
        Args:
            latents: [B, 4, h, w] æ½œåœ¨è¡¨ç¤ºï¼ˆå·²ç»è¿‡scaling_factorç¼©æ”¾ï¼‰
            
        Returns:
            images: [B, 3, H, W] é‡å»ºå›¾åƒï¼ŒèŒƒå›´[-1, 1]
        """
        self.vae.eval()
        
        # âš ï¸ é‡è¦ä¿®å¤ï¼šVAEè§£ç å‰éœ€è¦å…ˆé™¤ä»¥scaling_factorï¼Œæ¢å¤åŸå§‹æ½œå˜é‡èŒƒå›´
        # å› ä¸ºVAEæœŸæœ›æ¥æ”¶æœªç¼©æ”¾çš„æ½œå˜é‡
        unscaled_latents = latents / self.scaling_factor
        
        # VAEè§£ç 
        images = self.vae.decode(unscaled_latents).sample
        
        # ç¡®ä¿è¾“å‡ºåœ¨æ­£ç¡®èŒƒå›´
        images = torch.clamp(images, -1.0, 1.0)
        
        return images

    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æŸå¤±å­—å…¸
        """
        batch_size = images.shape[0]
        device = images.device
        
        # 1. ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            latents = self.encode_to_latent(images)  # [B, 4, h, w]
        
        # 2. éšæœºé‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.diffusion.timesteps, (batch_size,), device=device, dtype=torch.long
        )
        
        # 3. è®¡ç®—æ‰©æ•£æŸå¤±
        losses = self.diffusion.training_losses(
            model=self.unet,
            x_start=latents,
            t=timesteps,
            class_labels=class_labels
        )
        
        return {
            'loss': losses['loss'].mean(),
            'pred_noise': losses['pred_noise'],
            'target_noise': losses['target_noise'],
            'latents': latents,
            'timesteps': timesteps
        }

    @torch.no_grad()
    def sample(
        self,
        num_samples: int,
        class_labels: Optional[torch.Tensor] = None,
        num_inference_steps: int = 50,
        eta: float = 0.0,
        guidance_scale: float = 1.0,
        generator: Optional[torch.Generator] = None,
        return_latents: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ä»å™ªéŸ³é‡‡æ ·ç”Ÿæˆå›¾åƒ
        
        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            class_labels: [num_samples] ç±»åˆ«æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            num_inference_steps: æ¨ç†æ­¥æ•°
            eta: DDIMçš„éšæœºæ€§å‚æ•°ï¼ˆ0=ç¡®å®šæ€§ï¼Œ1=éšæœºï¼‰
            guidance_scale: åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å¼ºåº¦
            generator: éšæœºæ•°ç”Ÿæˆå™¨
            return_latents: æ˜¯å¦è¿”å›æ½œåœ¨è¡¨ç¤º
            
        Returns:
            images: [num_samples, 3, H, W] ç”Ÿæˆçš„å›¾åƒ
            latents: [num_samples, 4, h, w] æ½œåœ¨è¡¨ç¤ºï¼ˆå¦‚æœreturn_latents=Trueï¼‰
        """
        device = next(self.parameters()).device
        
        # è®¡ç®—æ½œåœ¨ç©ºé—´çš„å½¢çŠ¶
        # å‡è®¾è¾“å…¥å›¾åƒæ˜¯256x256ï¼ŒVAEä¸‹é‡‡æ ·8å€ï¼Œå¾—åˆ°32x32
        latent_shape = (num_samples, 4, 32, 32)
        
        # å¦‚æœä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
        if guidance_scale > 1.0 and class_labels is not None:
            # ä¿®å¤ï¼šä½¿ç”¨num_classesä½œä¸ºæ— æ¡ä»¶æ ‡è®°ï¼Œè€Œä¸æ˜¯-1
            # å› ä¸ºEmbeddingå±‚æ— æ³•å¤„ç†è´Ÿæ•°ç´¢å¼•
            unconditional_label = self.unet.num_classes  # ä½¿ç”¨ç±»åˆ«æ•°ä½œä¸ºæ— æ¡ä»¶æ ‡è®°
            
            # å®šä¹‰å¼•å¯¼ç‰ˆæœ¬çš„æ¨¡å‹
            def guided_unet(x, t, class_labels):
                batch_size = x.shape[0]
                
                # åˆ›å»ºæ— æ¡ä»¶æ ‡ç­¾
                unconditional_labels = torch.full(
                    (batch_size,), unconditional_label, 
                    device=device, dtype=class_labels.dtype
                )
                
                # æ¡ä»¶é¢„æµ‹
                cond_pred = self.unet(x, t, class_labels)
                
                # æ— æ¡ä»¶é¢„æµ‹
                uncond_pred = self.unet(x, t, unconditional_labels)
                
                # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
                guided_pred = uncond_pred + guidance_scale * (cond_pred - uncond_pred)
                
                return guided_pred
            
            model_fn = guided_unet
        else:
            model_fn = self.unet
        
        # é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        latents = self.diffusion.sample(
            model=model_fn,
            shape=latent_shape,
            class_labels=class_labels,
            num_inference_steps=num_inference_steps,
            eta=eta,
            generator=generator
        )
        
        # è§£ç åˆ°å›¾åƒ
        images = self.decode_from_latent(latents)
        
        if return_latents:
            return images, latents
        else:
            return images

    def configure_optimizers(self, lr: float = 1e-4, weight_decay: float = 0.01):
        """é…ç½®ä¼˜åŒ–å™¨"""
        # åªä¼˜åŒ–U-Netå‚æ•°ï¼ŒVAEä¿æŒå†»ç»“
        optimizer = torch.optim.AdamW(
            self.unet.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        return optimizer

    def save_checkpoint(
        self,
        filepath: str,
        epoch: int,
        optimizer_state: Optional[Dict] = None,
        scheduler_state: Optional[Dict] = None,
        metrics: Optional[Dict] = None
    ):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'unet_state_dict': self.unet.state_dict(),
            'unet_config': self.unet_config,
            'diffusion_config': self.diffusion_config,
            'scaling_factor': self.scaling_factor,
        }
        
        if optimizer_state:
            checkpoint['optimizer_state_dict'] = optimizer_state
        if scheduler_state:
            checkpoint['scheduler_state_dict'] = scheduler_state
        if metrics:
            checkpoint['metrics'] = metrics
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")

    def load_checkpoint(self, filepath: str, load_optimizer: bool = False):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # åŠ è½½U-Netæƒé‡
        self.unet.load_state_dict(checkpoint['unet_state_dict'])
        
        print(f"âœ… LDMæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {filepath}")
        print(f"   è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'Unknown')}")
        
        if load_optimizer:
            return checkpoint.get('optimizer_state_dict'), checkpoint.get('scheduler_state_dict')
        
        return None, None


def create_ldm_model(
    vae_checkpoint_path: str = None,
    unet_config: Dict[str, Any] = None,
    diffusion_config: Dict[str, Any] = None,
    device: str = 'cuda'
) -> LatentDiffusionModel:
    """
    åˆ›å»ºå®Œæ•´LDMæ¨¡å‹çš„å·¥å‚å‡½æ•°
    """
    return LatentDiffusionModel(
        vae_checkpoint_path=vae_checkpoint_path,
        unet_config=unet_config,
        diffusion_config=diffusion_config,
        device=device
    )


if __name__ == "__main__":
    # æµ‹è¯•LDMæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("ğŸ§ª æµ‹è¯•LDMæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_ldm_model(device=device)
    
    # æµ‹è¯•ç¼–ç -è§£ç 
    print("\nğŸ“Š æµ‹è¯•VAEç¼–ç -è§£ç ...")
    batch_size = 2
    images = torch.randn(batch_size, 3, 256, 256, device=device)  # éšæœºå›¾åƒ
    
    with torch.no_grad():
        # ç¼–ç 
        latents = model.encode_to_latent(images)
        print(f"åŸå§‹å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {latents.shape}")
        
        # è§£ç 
        reconstructed = model.decode_from_latent(latents)
        print(f"é‡å»ºå›¾åƒå½¢çŠ¶: {reconstructed.shape}")
        
        # è®¡ç®—é‡å»ºè¯¯å·®
        mse = F.mse_loss(images, reconstructed)
        print(f"é‡å»ºMSE: {mse.item():.4f}")
    
    # æµ‹è¯•è®­ç»ƒå‰å‘ä¼ æ’­
    print("\nğŸ¯ æµ‹è¯•è®­ç»ƒå‰å‘ä¼ æ’­...")
    class_labels = torch.randint(0, 31, (batch_size,), device=device)
    
    loss_dict = model(images, class_labels)
    print(f"è®­ç»ƒæŸå¤±: {loss_dict['loss'].item():.4f}")
    print(f"é¢„æµ‹å™ªéŸ³å½¢çŠ¶: {loss_dict['pred_noise'].shape}")
    
    # æµ‹è¯•é‡‡æ ·ï¼ˆå°‘é‡æ­¥æ•°ï¼‰
    print("\nğŸ¨ æµ‹è¯•å›¾åƒç”Ÿæˆ...")
    with torch.no_grad():
        generated_images = model.sample(
            num_samples=1,
            class_labels=torch.tensor([0], device=device),
            num_inference_steps=10  # å¿«é€Ÿæµ‹è¯•
        )
        print(f"ç”Ÿæˆå›¾åƒå½¢çŠ¶: {generated_images.shape}")
    
    # ç»Ÿè®¡æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nğŸ“ˆ æ¨¡å‹ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
    print(f"   VAEå‚æ•°æ•°: {sum(p.numel() for p in model.vae.parameters()):,} (å†»ç»“)")
    print(f"   U-Netå‚æ•°æ•°: {sum(p.numel() for p in model.unet.parameters()):,} (å¯è®­ç»ƒ)")
    
    print("\nâœ… LDMæ¨¡å‹æµ‹è¯•æˆåŠŸ!") 
