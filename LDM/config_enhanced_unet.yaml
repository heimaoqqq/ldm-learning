# 增强版U-Net配置 - 完整Transformer实现
# 目标：将FID降到20以内，使用真正的多层Transformer

device: 'auto'  # 'cuda', 'cpu', or 'auto'

# VAE配置 (预训练，冻结)
vae:
  in_channels: 3
  latent_dim: 256
  num_embeddings: 512
  beta: 0.25
  vq_ema_decay: 0.99
  groups: 1
  model_path: '../VAE/vae_ckpt/checkpoint_epoch_100.pth'
  freeze: true

# 🚀 简化增强版U-Net - 包含真正的多层Transformer
unet:
  type: 'simple_enhanced'    # 🚀 使用简化增强版 (115.8M参数)
  in_channels: 256
  out_channels: 256
  model_channels: 192        # 显存友好的模型大小
  num_classes: 31
  time_embed_dim: 768        # 合理的嵌入维度
  
  # 🚀 真正的Transformer配置
  transformer_depth: 3       # 每个注意力层3层Transformer
  num_heads: 8               # 8个注意力头
  use_attention_layers: [1, 2]  # 在第1层和第2层使用多层注意力
  
  # 说明：
  # - 6个注意力层 × 3层Transformer = 18个Transformer块
  # - 比简单版多50.3M参数，但增加了强大的空间注意力能力
  # - 预期能显著改善FID值

# 扩散过程配置
diffusion:
  timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  noise_schedule: 'linear'
  scheduler_type: 'ddim'  # 🚀 使用改进的DDIM调度器

# 推理配置 - 高质量采样
inference:
  num_inference_steps: 75   # 🚀 75步采样，平衡质量和速度
  guidance_scale: 8.5       # 🚀 CFG强度
  eta: 0.3                  # 🚀 DDIM随机性参数
  
  # 采样配置 - 🚀 资源优化
  sample_batch_size: 16     # 🚀 从8增加到16（充分利用显存）
  num_samples_per_class: 10
  
  # 输出设置
  output_dir: "enhanced_samples"
  save_format: "png"

# 训练配置 - 针对Transformer优化
training:
  epochs: 600  # 🎯 足够的训练轮数
  lr: 0.00008   # 🚀 从0.0001轻微降低，适配batch size调整
  weight_decay: 0.02  # 🚀 正则化防止过拟合
  warmup_steps: 3000  # 🚀 从5000调整到3000，适配样本数
  
  # 梯度累积 - 🚀 针对4587样本优化
  gradient_accumulation_steps: 3  # 🚀 调整为3，有效batch_size = 8×3 = 24
  
  # 早停策略
  early_stopping_patience: 45  # 🚀 基于FID值早停
  early_stopping_metric: "fid"
  
  # 保存设置
  save_dir: "enhanced_models"
  save_interval: 50  # 🚀 频繁保存
  sample_interval: 15  # 🚀 频繁生成样本监控
  log_interval: 15
  
  # 损失配置
  loss_type: "l2"
  
  # Classifier-Free Guidance
  use_cfg: true
  cfg_dropout_prob: 0.15
  
  # 优化器配置
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.999
  eps: 0.00000001
  
  # 梯度裁剪
  grad_clip_norm: 0.5
  
  # 调度器
  use_scheduler: true
  scheduler_type: "cosine"

# FID评估配置 - 高精度评估
fid_evaluation:
  enabled: true
  eval_interval: 15        # 🚀 从20降到15，适配较小数据集
  num_samples: 1500        # 🚀 从2000降到1500，适配验证集规模
  batch_size: 64           # 🚀 保持64用于FID评估
  max_real_samples: 510    # 🚀 使用全部验证集样本
  save_best_fid_model: true
  
  # 🚀 加速参数
  eval_classes: 3          # 🚀 只评估3个随机类别
  num_inference_steps: 50  # 🚀 评估时使用50步
  parallel_generation: true  # 🚀 启用并行生成

# 数据集配置 - 🚀 针对样本数优化
dataset:
  root_dir: '/kaggle/input/dataset'  # 🔧 正确的Kaggle数据集路径
  batch_size: 8            # 🚀 针对4587样本优化，8×3累积=24有效batch
  num_workers: 8           # 🚀 充分利用CPU
  val_split: 0.1           # 🚀 10%验证集，适配当前数据划分
  shuffle_train: true
  pin_memory: true         # 🚀 加速数据传输
  persistent_workers: true # 🚀 保持worker进程
  
  # 🚫 数据增强已移除 - 对微多普勒时频图有害
  # 原因：水平翻转、旋转、颜色抖动会破坏时频图的物理意义
  # 时频图的时间轴和频率轴具有明确的物理含义，不应随意变换
  augmentation:
    enabled: false  # 🚫 完全禁用数据增强

# EMA配置
use_ema: true
ema_decay: 0.99

# 混合精度训练
mixed_precision: false  # 🎯 追求质量，关闭混合精度

# 日志配置
logging:
  log_dir: "enhanced_logs"
  tensorboard: false
  wandb: false 

# 🚀 性能优化配置 - 保守优化保证训练质量
performance:
  # 显存优化
  empty_cache_interval: 30  # 🚀 从50降到30，更频繁清理
  
  # CPU优化
  torch_num_threads: 8      # 设置PyTorch线程数
  
  # 编译优化
  compile_model: false      # 🚀 关闭编译，确保训练稳定性
  
  # 预取优化
  prefetch_factor: 4        # 数据预取倍数

# 🚀 训练质量监控
quality_monitoring:
  # 指标改进检测
  improvement_threshold: 0.02  # FID改进阈值
  loss_improvement_threshold: 0.001  # 损失改进阈值
  
  # 训练稳定性监控
  gradient_norm_threshold: 10.0  # 梯度爆炸检测
  loss_spike_threshold: 2.0      # 损失突增检测 
