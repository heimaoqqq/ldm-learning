#!/usr/bin/env python3
"""
æ½œåœ¨æ‰©æ•£æ¨¡å‹ (Latent Diffusion Model) ä¸»è¦å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
import math
from typing import Dict, Tuple, Optional, Any

# æ·»åŠ VAEæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from adv_vq_vae import AdvVQVAE
from unet import UNetModel  # ä½¿ç”¨ä¿®å¤åçš„åŸå§‹U-Net
from scheduler import DDPMScheduler, DDIMScheduler  # ğŸ†• å¯¼å…¥DDIMè°ƒåº¦å™¨

class LatentDiffusionModel(nn.Module):
    """
    æ½œåœ¨æ‰©æ•£æ¨¡å‹
    """
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–VAE (é¢„è®­ç»ƒï¼Œå†»ç»“å‚æ•°)
        self.vae = self._init_vae(config['vae'])
        
        # åˆå§‹åŒ–æ‰©æ•£è°ƒåº¦å™¨ - ğŸ†• æ”¯æŒDDIMå’Œæ”¹è¿›eta
        if config['diffusion'].get('scheduler_type', 'ddpm') == 'ddim':
            self.scheduler = DDIMScheduler(
                num_train_timesteps=config['diffusion']['timesteps'],
                beta_start=config['diffusion']['beta_start'],
                beta_end=config['diffusion']['beta_end'],
                beta_schedule=config['diffusion']['noise_schedule'],
                eta=config['inference'].get('eta', 0.0),
                adaptive_eta=config['inference'].get('adaptive_eta', False)
            )
        else:
            self.scheduler = DDPMScheduler(
                num_train_timesteps=config['diffusion']['timesteps'],
                beta_start=config['diffusion']['beta_start'],
                beta_end=config['diffusion']['beta_end'],
                beta_schedule=config['diffusion']['noise_schedule']
            )
        
        # åˆå§‹åŒ–U-Net
        self.unet = UNetModel(**config['unet'])
        
        # å…¶ä»–é…ç½®
        self.use_cfg = config['training'].get('use_cfg', False)
        self.cfg_dropout_prob = config['training'].get('cfg_dropout_prob', 0.1)
        
    def _init_vae(self, vae_config: Dict[str, Any]) -> AdvVQVAE:
        """åˆå§‹åŒ–VAEå¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        vae = AdvVQVAE(
            in_channels=vae_config['in_channels'],
            latent_dim=vae_config['latent_dim'],
            num_embeddings=vae_config['num_embeddings'],
            beta=vae_config['beta'],
            decay=vae_config['vq_ema_decay'],
            groups=vae_config['groups']
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if 'model_path' in vae_config and vae_config['model_path']:
            try:
                print(f"ğŸ”„ åŠ è½½VAEé¢„è®­ç»ƒæ¨¡å‹: {vae_config['model_path']}")
                state_dict = torch.load(vae_config['model_path'], map_location='cpu')
                vae.load_state_dict(state_dict)
                print("âœ… VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # å†»ç»“VAEå‚æ•°
        if vae_config.get('freeze', True):
            for param in vae.parameters():
                param.requires_grad = False
            vae.eval()
            print("â„ï¸ VAEå‚æ•°å·²å†»ç»“")
        
        return vae
    
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´"""
        with torch.no_grad():
            # VAEç¼–ç 
            encoded = self.vae.encoder(images)
            # VQé‡åŒ–
            quantized, _, _ = self.vae.vq(encoded)
            return quantized
    
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ"""
        with torch.no_grad():
            return self.vae.decoder(latents)
    
    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾
            
        Returns:
            åŒ…å«æŸå¤±ä¿¡æ¯çš„å­—å…¸
        """
        batch_size = images.shape[0]
        
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        latents = self.encode_to_latent(images)  # [B, C, H', W']
        
        # é‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.num_train_timesteps, 
            (batch_size,), device=images.device
        )
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)
        
        # CFGè®­ç»ƒ (éšæœºä¸¢å¼ƒç±»åˆ«æ ‡ç­¾)
        if self.use_cfg and class_labels is not None:
            # éšæœºmaskæ‰ä¸€äº›ç±»åˆ«æ ‡ç­¾ç”¨äºCFGè®­ç»ƒ
            cfg_mask = torch.rand(batch_size, device=images.device) < self.cfg_dropout_prob
            class_labels = class_labels.clone()
            class_labels[cfg_mask] = -1  # ä½¿ç”¨-1è¡¨ç¤ºæ— æ¡ä»¶
        
        # U-Neté¢„æµ‹å™ªå£°
        predicted_noise = self.unet(noisy_latents, timesteps, class_labels)
        
        # è®¡ç®—æŸå¤±
        noise_loss = F.mse_loss(predicted_noise, noise, reduction='mean')
        
        return {
            'noise_loss': noise_loss,
            'total_loss': noise_loss,
            'predicted_noise': predicted_noise,
            'target_noise': noise
        }
    
    @torch.no_grad()
    def sample(
        self, 
        batch_size: int, 
        class_labels: Optional[torch.Tensor] = None,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        eta: float = 0.0
    ) -> torch.Tensor:
        """
        DDIMé‡‡æ ·ç”Ÿæˆå›¾åƒ
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            class_labels: [batch_size] ç±»åˆ«æ ‡ç­¾
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: CFGå¼•å¯¼å¼ºåº¦
            eta: DDIMå‚æ•° (0ä¸ºç¡®å®šæ€§é‡‡æ ·)
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒ [batch_size, 3, H, W]
        """
        device = self.device
        
        # è·å–æ½œåœ¨ç©ºé—´å°ºå¯¸
        # å‡è®¾è¾“å…¥å›¾åƒä¸º256x256ï¼ŒVAEä¸‹é‡‡æ ·8å€ï¼Œå¾—åˆ°32x32
        latent_height = 32
        latent_width = 32
        latent_channels = self.config['unet']['in_channels']
        
        # åˆå§‹åŒ–éšæœºå™ªå£°
        latents = torch.randn(
            batch_size, latent_channels, latent_height, latent_width,
            device=device
        )
        
        # è®¾ç½®æ¨ç†è°ƒåº¦å™¨
        self.scheduler.set_timesteps(num_inference_steps)
        
        for t in self.scheduler.timesteps:
            # æ‰©å±•æ—¶é—´æ­¥
            t_batch = t.unsqueeze(0).repeat(batch_size).to(device)
            
            if self.use_cfg and class_labels is not None and guidance_scale > 1.0:
                # CFG: åŒæ—¶é¢„æµ‹æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°
                latent_model_input = torch.cat([latents] * 2)
                t_input = torch.cat([t_batch] * 2)
                
                # åˆ›å»ºæ¡ä»¶æ ‡ç­¾ (æœ‰æ¡ä»¶ + æ— æ¡ä»¶)
                class_input = torch.cat([
                    class_labels,
                    torch.full_like(class_labels, -1)  # æ— æ¡ä»¶æ ‡ç­¾
                ])
                
                # U-Neté¢„æµ‹
                noise_pred = self.unet(latent_model_input, t_input, class_input)
                
                # åˆ†ç¦»æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
                noise_pred_cond, noise_pred_uncond = noise_pred.chunk(2)
                
                # CFGå¼•å¯¼
                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
            else:
                # æ— CFGçš„æ ‡å‡†é¢„æµ‹
                noise_pred = self.unet(latents, t_batch, class_labels)
            
            # DDIMæ­¥éª¤
            result = self.scheduler.step(noise_pred, t, latents)
            latents = result.prev_sample if hasattr(result, 'prev_sample') else result[0]
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        images = self.decode_from_latent(latents)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        images = (images + 1.0) / 2.0
        images = torch.clamp(images, 0.0, 1.0)
        
        return images
    
    def get_loss_dict(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è·å–æŸå¤±å­—å…¸ (å…¼å®¹è®­ç»ƒå¾ªç¯)"""
        images = batch['image']
        class_labels = batch.get('label', None)
        
        return self(images, class_labels)


class ClassifierFreeGuidance:
    """
    æ— åˆ†ç±»å™¨å¼•å¯¼ (Classifier-Free Guidance) å·¥å…·ç±»
    """
    def __init__(self, guidance_scale: float = 7.5, unconditional_token: int = -1):
        self.guidance_scale = guidance_scale
        self.unconditional_token = unconditional_token
    
    def apply_cfg(
        self,
        noise_pred_uncond: torch.Tensor,
        noise_pred_cond: torch.Tensor,
        guidance_scale: Optional[float] = None,
    ) -> torch.Tensor:
        """
        åº”ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼
        Args:
            noise_pred_uncond: æ— æ¡ä»¶å™ªå£°é¢„æµ‹
            noise_pred_cond: æœ‰æ¡ä»¶å™ªå£°é¢„æµ‹
            guidance_scale: å¼•å¯¼å¼ºåº¦
        Returns:
            å¼•å¯¼åçš„å™ªå£°é¢„æµ‹
        """
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
        
        return noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
    
    def prepare_labels_for_cfg(
        self, 
        labels: torch.Tensor,
        dropout_prob: float = 0.1,
        training: bool = True,
    ) -> torch.Tensor:
        """
        ä¸ºCFGè®­ç»ƒå‡†å¤‡æ ‡ç­¾
        Args:
            labels: åŸå§‹æ ‡ç­¾
            dropout_prob: æ¡ä»¶ä¸¢å¼ƒæ¦‚ç‡
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        Returns:
            å¤„ç†åçš„æ ‡ç­¾
        """
        if training and dropout_prob > 0:
            dropout_mask = torch.rand(labels.shape[0], device=labels.device) < dropout_prob
            unconditional_labels = torch.full_like(labels, self.unconditional_token)
            return torch.where(dropout_mask, unconditional_labels, labels)
        return labels 
