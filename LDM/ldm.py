#!/usr/bin/env python3
"""
æ½œåœ¨æ‰©æ•£æ¨¡å‹ (Latent Diffusion Model) ä¸»è¦å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
import math
from typing import Dict, Tuple, Optional, Any

# æ·»åŠ VAEæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from adv_vq_vae import AdvVQVAE
from unet import UNetModel  # ä½¿ç”¨ä¿®å¤åçš„åŸå§‹U-Net
from scheduler import DDPMScheduler, DDIMScheduler  # ğŸ†• å¯¼å…¥DDIMè°ƒåº¦å™¨

class LatentDiffusionModel(nn.Module):
    """
    æ½œåœ¨æ‰©æ•£æ¨¡å‹
    """
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–VAE (é¢„è®­ç»ƒï¼Œå†»ç»“å‚æ•°)
        self.vae = self._init_vae(config['vae'])
        
        # åˆå§‹åŒ–æ‰©æ•£è°ƒåº¦å™¨ - ğŸ†• æ”¯æŒDDIMå’Œæ”¹è¿›eta
        scheduler_config = config['diffusion']
        scheduler_type = scheduler_config.get('scheduler_type', 'ddpm').lower()
        
        if scheduler_type == 'ddim':
            # ğŸ”§ ä»æ¨ç†é…ç½®è·å–etaå‚æ•°ï¼Œç¡®ä¿ä¸€è‡´æ€§
            inference_config = config.get('inference', {})
            default_eta = inference_config.get('eta', 0.0)
            adaptive_eta = inference_config.get('adaptive_eta', False)
            
            self.scheduler = DDIMScheduler(
                num_train_timesteps=scheduler_config['timesteps'],
                beta_start=scheduler_config['beta_start'],
                beta_end=scheduler_config['beta_end'],
                beta_schedule=scheduler_config['noise_schedule'],
                eta=default_eta,
                adaptive_eta=adaptive_eta
            )
            print(f"âœ… ä½¿ç”¨DDIMè°ƒåº¦å™¨ï¼Œé»˜è®¤eta={default_eta}, è‡ªé€‚åº”eta={adaptive_eta}")
        else:
            # æ ‡å‡†DDPMè°ƒåº¦å™¨
            self.scheduler = DDPMScheduler(
                num_train_timesteps=scheduler_config['timesteps'],
                beta_start=scheduler_config['beta_start'],
                beta_end=scheduler_config['beta_end'],
                beta_schedule=scheduler_config['noise_schedule']
            )
            print(f"âœ… ä½¿ç”¨DDPMè°ƒåº¦å™¨")
        
        # åˆå§‹åŒ–U-Net
        self.unet = UNetModel(**config['unet'])
        
        # å…¶ä»–é…ç½®
        self.use_cfg = config['training'].get('use_cfg', False)
        self.cfg_dropout_prob = config['training'].get('cfg_dropout_prob', 0.1)
        
    def _init_vae(self, vae_config: Dict[str, Any]) -> AdvVQVAE:
        """åˆå§‹åŒ–VAEå¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        vae = AdvVQVAE(
            in_channels=vae_config['in_channels'],
            latent_dim=vae_config['latent_dim'],
            num_embeddings=vae_config['num_embeddings'],
            beta=vae_config['beta'],
            decay=vae_config['vq_ema_decay'],
            groups=vae_config['groups']
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if 'model_path' in vae_config and vae_config['model_path']:
            try:
                print(f"ğŸ”„ åŠ è½½VAEé¢„è®­ç»ƒæ¨¡å‹: {vae_config['model_path']}")
                state_dict = torch.load(vae_config['model_path'], map_location='cpu')
                vae.load_state_dict(state_dict)
                print("âœ… VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # å†»ç»“VAEå‚æ•°
        if vae_config.get('freeze', True):
            for param in vae.parameters():
                param.requires_grad = False
            vae.eval()
            print("â„ï¸ VAEå‚æ•°å·²å†»ç»“")
        
        return vae
    
    def encode_to_latent(self, images: torch.Tensor) -> torch.Tensor:
        """å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´"""
        with torch.no_grad():
            # VAEç¼–ç 
            encoded = self.vae.encoder(images)
            # VQé‡åŒ–
            quantized, _, _ = self.vae.vq(encoded)
            return quantized
    
    def decode_from_latent(self, latents: torch.Tensor) -> torch.Tensor:
        """ä»æ½œåœ¨ç©ºé—´è§£ç åˆ°å›¾åƒ"""
        with torch.no_grad():
            # ğŸ”§ æ·»åŠ æ½œåœ¨è¡¨ç¤ºèŒƒå›´æ£€æŸ¥å’Œè£å‰ª
            # ç›‘æ§è¾“å…¥èŒƒå›´
            input_min, input_max = latents.min().item(), latents.max().item()
            input_std = latents.std().item()
            
            # å¦‚æœèŒƒå›´è¶…å‡ºåˆç†èŒƒå›´ï¼Œè¿›è¡Œè£å‰ª
            if abs(input_min) > 10 or abs(input_max) > 10:
                print(f"âš ï¸ æ½œåœ¨è¡¨ç¤ºèŒƒå›´å¼‚å¸¸: [{input_min:.3f}, {input_max:.3f}], è¿›è¡Œè£å‰ª")
                latents = torch.clamp(latents, min=-5.0, max=5.0)
            
            # å¦‚æœæ ‡å‡†å·®è¿‡å¤§ï¼Œè¿›è¡Œç¼©æ”¾
            if input_std > 3.0:
                print(f"âš ï¸ æ½œåœ¨è¡¨ç¤ºæ ‡å‡†å·®è¿‡å¤§: {input_std:.3f}, è¿›è¡Œç¼©æ”¾")
                latents = latents / (input_std / 2.0)  # ç¼©æ”¾åˆ°åˆç†èŒƒå›´
            
            # VAEè§£ç 
            decoded = self.vae.decoder(latents)
            
            # ğŸ”§ æ£€æŸ¥è§£ç è¾“å‡ºèŒƒå›´
            output_min, output_max = decoded.min().item(), decoded.max().item()
            if output_min < -1.1 or output_max > 1.1:
                print(f"âš ï¸ VAEè§£ç è¾“å‡ºè¶…å‡ºtanhèŒƒå›´: [{output_min:.3f}, {output_max:.3f}]")
                decoded = torch.clamp(decoded, min=-1.0, max=1.0)
            
            return decoded
    
    def forward(self, images: torch.Tensor, class_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W] è¾“å…¥å›¾åƒ
            class_labels: [B] ç±»åˆ«æ ‡ç­¾
            
        Returns:
            åŒ…å«æŸå¤±ä¿¡æ¯çš„å­—å…¸
        """
        batch_size = images.shape[0]
        
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        latents = self.encode_to_latent(images)  # [B, C, H', W']
        
        # é‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.num_train_timesteps, 
            (batch_size,), device=images.device
        )
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)
        
        # CFGè®­ç»ƒ (éšæœºä¸¢å¼ƒç±»åˆ«æ ‡ç­¾)
        if self.use_cfg and class_labels is not None:
            # éšæœºmaskæ‰ä¸€äº›ç±»åˆ«æ ‡ç­¾ç”¨äºCFGè®­ç»ƒ
            cfg_mask = torch.rand(batch_size, device=images.device) < self.cfg_dropout_prob
            class_labels = class_labels.clone()
            class_labels[cfg_mask] = -1  # ä½¿ç”¨-1è¡¨ç¤ºæ— æ¡ä»¶
        
        # U-Neté¢„æµ‹å™ªå£°
        predicted_noise = self.unet(noisy_latents, timesteps, class_labels)
        
        # è®¡ç®—æŸå¤±
        noise_loss = F.mse_loss(predicted_noise, noise, reduction='mean')
        
        return {
            'noise_loss': noise_loss,
            'total_loss': noise_loss,
            'predicted_noise': predicted_noise,
            'target_noise': noise
        }
    
    @torch.no_grad()
    def sample(
        self, 
        batch_size: int, 
        class_labels: Optional[torch.Tensor] = None,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        eta: Optional[float] = None  # ğŸ”§ æ”¹ä¸ºå¯é€‰å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨è°ƒåº¦å™¨é…ç½®
    ) -> torch.Tensor:
        """
        DDIM/DDPMé‡‡æ ·ç”Ÿæˆå›¾åƒ
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            class_labels: [batch_size] ç±»åˆ«æ ‡ç­¾
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: CFGå¼•å¯¼å¼ºåº¦
            eta: DDIMå‚æ•° (None=ä½¿ç”¨è°ƒåº¦å™¨é»˜è®¤å€¼, 0=ç¡®å®šæ€§é‡‡æ ·)
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒ [batch_size, 3, H, W]
        """
        device = self.device
        
        # ğŸ”§ ç¡®å®šå®é™…ä½¿ç”¨çš„etaå€¼
        if eta is None:
            # ä½¿ç”¨è°ƒåº¦å™¨çš„é»˜è®¤etaï¼ˆå¦‚æœæ˜¯DDIMï¼‰
            actual_eta = getattr(self.scheduler, 'eta', 0.0)
        else:
            actual_eta = eta
        
        # ğŸ”§ æ£€æŸ¥è°ƒåº¦å™¨ç±»å‹å’Œetaå…¼å®¹æ€§
        is_ddim = isinstance(self.scheduler, DDIMScheduler)
        if not is_ddim and actual_eta != 0.0:
            print(f"âš ï¸ å½“å‰ä½¿ç”¨DDPMè°ƒåº¦å™¨ï¼Œetaå‚æ•°({actual_eta})å°†è¢«å¿½ç•¥")
            actual_eta = 0.0
        
        print(f"ğŸ”„ å¼€å§‹é‡‡æ ·: è°ƒåº¦å™¨={'DDIM' if is_ddim else 'DDPM'}, æ­¥æ•°={num_inference_steps}, eta={actual_eta}, CFG={guidance_scale}")
        
        # è·å–æ½œåœ¨ç©ºé—´å°ºå¯¸
        # å‡è®¾è¾“å…¥å›¾åƒä¸º256x256ï¼ŒVAEä¸‹é‡‡æ ·8å€ï¼Œå¾—åˆ°32x32
        latent_height = 32
        latent_width = 32
        latent_channels = self.config['unet']['in_channels']
        
        # åˆå§‹åŒ–éšæœºå™ªå£°
        latents = torch.randn(
            batch_size, latent_channels, latent_height, latent_width,
            device=device
        )
        
        # è®¾ç½®æ¨ç†è°ƒåº¦å™¨
        self.scheduler.set_timesteps(num_inference_steps, device=device)
        
        for i, t in enumerate(self.scheduler.timesteps):
            # æ‰©å±•æ—¶é—´æ­¥
            t_batch = t.unsqueeze(0).repeat(batch_size).to(device)
            
            if self.use_cfg and class_labels is not None and guidance_scale > 1.0:
                # CFG: åŒæ—¶é¢„æµ‹æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°
                latent_model_input = torch.cat([latents] * 2)
                t_input = torch.cat([t_batch] * 2)
                
                # åˆ›å»ºæ¡ä»¶æ ‡ç­¾ (æœ‰æ¡ä»¶ + æ— æ¡ä»¶)
                class_input = torch.cat([
                    class_labels,
                    torch.full_like(class_labels, -1)  # æ— æ¡ä»¶æ ‡ç­¾
                ])
                
                # U-Neté¢„æµ‹
                noise_pred = self.unet(latent_model_input, t_input, class_input)
                
                # åˆ†ç¦»æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
                noise_pred_cond, noise_pred_uncond = noise_pred.chunk(2)
                
                # CFGå¼•å¯¼
                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
            else:
                # æ— CFGçš„æ ‡å‡†é¢„æµ‹
                noise_pred = self.unet(latents, t_batch, class_labels)
            
            # ğŸ”§ ç»Ÿä¸€çš„è°ƒåº¦å™¨æ­¥éª¤è°ƒç”¨
            try:
                if is_ddim:
                    # DDIMè°ƒåº¦å™¨ - ä¼ é€’etaå‚æ•°
                    result = self.scheduler.step(
                        model_output=noise_pred, 
                        timestep=t, 
                        sample=latents, 
                        eta=actual_eta
                    )
                else:
                    # DDPMè°ƒåº¦å™¨ - ä¼ é€’etaä»¥ä¿æŒå…¼å®¹æ€§ï¼ˆä½†ä¼šè¢«å¿½ç•¥ï¼‰
                    result = self.scheduler.step(
                        model_output=noise_pred, 
                        timestep=t, 
                        sample=latents,
                        eta=actual_eta  # DDPMä¼šå¿½ç•¥è¿™ä¸ªå‚æ•°
                    )
                
                latents = result.prev_sample if hasattr(result, 'prev_sample') else result[0]
                
                # ğŸ”§ å®šæœŸæ‰“å°è¿›åº¦
                if i % (len(self.scheduler.timesteps) // 4) == 0:
                    print(f"  ğŸ“Š é‡‡æ ·è¿›åº¦: {i+1}/{len(self.scheduler.timesteps)}")
                    
            except Exception as e:
                print(f"âš ï¸ é‡‡æ ·æ­¥éª¤ {i} å‡ºé”™: {e}")
                # ç»§ç»­é‡‡æ ·ï¼Œä½†ä½¿ç”¨å‰ä¸€ä¸ªlatents
                continue
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        try:
            images = self.decode_from_latent(latents)
            
            # ğŸ”§ æ”¹è¿›çš„å½’ä¸€åŒ–å¤„ç†
            # è®°å½•è§£ç åçš„åŸå§‹èŒƒå›´
            raw_min, raw_max = images.min().item(), images.max().item()
            
            # å½’ä¸€åŒ–åˆ°[0,1]ï¼Œæ›´é²æ£’çš„å¤„ç†
            if raw_min >= -1.1 and raw_max <= 1.1:
                # æ ‡å‡†æƒ…å†µï¼šVAEè¾“å‡ºåœ¨[-1,1]èŒƒå›´å†…
                images = (images + 1.0) / 2.0
            else:
                # å¼‚å¸¸æƒ…å†µï¼šä½¿ç”¨min-maxå½’ä¸€åŒ–
                print(f"âš ï¸ VAEè¾“å‡ºèŒƒå›´å¼‚å¸¸: [{raw_min:.3f}, {raw_max:.3f}], ä½¿ç”¨è‡ªé€‚åº”å½’ä¸€åŒ–")
                images = (images - raw_min) / (raw_max - raw_min + 1e-8)
            
            # æœ€ç»ˆè£å‰ªåˆ°[0,1]
            images = torch.clamp(images, 0.0, 1.0)
            
            # ğŸ”§ è´¨é‡æ£€æŸ¥
            final_min, final_max = images.min().item(), images.max().item()
            final_mean = images.mean().item()
            
            if final_mean < 0.1 or final_mean > 0.9:
                print(f"âš ï¸ ç”Ÿæˆå›¾åƒäº®åº¦å¼‚å¸¸: å‡å€¼={final_mean:.3f}")
            
            print(f"âœ… é‡‡æ ·å®Œæˆï¼Œç”Ÿæˆ {batch_size} å¼ å›¾åƒ")
            print(f"   ğŸ“Š æœ€ç»ˆå›¾åƒèŒƒå›´: [{final_min:.3f}, {final_max:.3f}], å‡å€¼: {final_mean:.3f}")
            return images
            
        except Exception as e:
            print(f"âš ï¸ å›¾åƒè§£ç å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„å›¾åƒå¼ é‡
            return torch.zeros(batch_size, 3, 256, 256, device=device)
    
    def get_loss_dict(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è·å–æŸå¤±å­—å…¸ (å…¼å®¹è®­ç»ƒå¾ªç¯)"""
        images = batch['image']
        class_labels = batch.get('label', None)
        
        return self(images, class_labels)


class ClassifierFreeGuidance:
    """
    æ— åˆ†ç±»å™¨å¼•å¯¼ (Classifier-Free Guidance) å·¥å…·ç±»
    """
    def __init__(self, guidance_scale: float = 7.5, unconditional_token: int = -1):
        self.guidance_scale = guidance_scale
        self.unconditional_token = unconditional_token
    
    def apply_cfg(
        self,
        noise_pred_uncond: torch.Tensor,
        noise_pred_cond: torch.Tensor,
        guidance_scale: Optional[float] = None,
    ) -> torch.Tensor:
        """
        åº”ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼
        Args:
            noise_pred_uncond: æ— æ¡ä»¶å™ªå£°é¢„æµ‹
            noise_pred_cond: æœ‰æ¡ä»¶å™ªå£°é¢„æµ‹
            guidance_scale: å¼•å¯¼å¼ºåº¦
        Returns:
            å¼•å¯¼åçš„å™ªå£°é¢„æµ‹
        """
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
        
        return noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
    
    def prepare_labels_for_cfg(
        self, 
        labels: torch.Tensor,
        dropout_prob: float = 0.1,
        training: bool = True,
    ) -> torch.Tensor:
        """
        ä¸ºCFGè®­ç»ƒå‡†å¤‡æ ‡ç­¾
        Args:
            labels: åŸå§‹æ ‡ç­¾
            dropout_prob: æ¡ä»¶ä¸¢å¼ƒæ¦‚ç‡
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        Returns:
            å¤„ç†åçš„æ ‡ç­¾
        """
        if training and dropout_prob > 0:
            dropout_mask = torch.rand(labels.shape[0], device=labels.device) < dropout_prob
            unconditional_labels = torch.full_like(labels, self.unconditional_token)
            return torch.where(dropout_mask, unconditional_labels, labels)
        return labels 
