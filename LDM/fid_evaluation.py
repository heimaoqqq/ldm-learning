import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader
from torchvision import models, transforms
from scipy import linalg
import sys
import os
from typing import Tuple, Optional, List
from tqdm import tqdm

# åœ¨Kaggleç¯å¢ƒä¸­ï¼ŒVAEæ¨¡å—å·²å¤åˆ¶åˆ°å½“å‰ç›®å½•
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

def denormalize(x):
    """åå½’ä¸€åŒ–å›¾åƒï¼Œä»[-1,1]è½¬æ¢åˆ°[0,1]"""
    return (x * 0.5 + 0.5).clamp(0, 1)

class InceptionV3Features(nn.Module):
    """
    ä½¿ç”¨InceptionV3æå–ç‰¹å¾ç”¨äºFIDè®¡ç®—
    """
    def __init__(self, device='cuda'):
        super().__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„InceptionV3
        self.inception = models.inception_v3(pretrained=True, transform_input=False)
        self.inception.eval()
        
        # å†»ç»“å‚æ•°
        for param in self.inception.parameters():
            param.requires_grad = False
            
        self.device = device
        self.to(device)
        
        # é¢„å¤„ç†ï¼šInceptionV3æœŸæœ›è¾“å…¥ä¸º[0,1]èŒƒå›´ï¼Œ299x299å°ºå¯¸
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œè¿›è¡Œresizeï¼Œå› ä¸ºtransforms.ResizeæœŸæœ›PILå›¾åƒ
        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                           std=[0.229, 0.224, 0.225])
    
    def forward(self, x):
        """
        æå–ç‰¹å¾
        Args:
            x: [batch_size, 3, H, W] å›¾åƒå¼ é‡ï¼ŒèŒƒå›´[0,1]
        Returns:
            features: [batch_size, 2048] ç‰¹å¾å‘é‡
        """
        # è°ƒæ•´å›¾åƒå°ºå¯¸åˆ°299x299
        if x.size(-1) != 299 or x.size(-2) != 299:
            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)
        
        # åº”ç”¨ImageNetæ ‡å‡†åŒ–
        x = self.normalize(x)
        
        # è‡ªå®šä¹‰å‰å‘ä¼ æ’­ï¼Œè·³è¿‡è¾…åŠ©åˆ†ç±»å™¨
        # N x 3 x 299 x 299
        x = self.inception.Conv2d_1a_3x3(x)
        # N x 32 x 149 x 149
        x = self.inception.Conv2d_2a_3x3(x)
        # N x 32 x 147 x 147
        x = self.inception.Conv2d_2b_3x3(x)
        # N x 64 x 147 x 147
        x = self.inception.maxpool1(x)
        # N x 64 x 73 x 73
        x = self.inception.Conv2d_3b_1x1(x)
        # N x 80 x 73 x 73
        x = self.inception.Conv2d_4a_3x3(x)
        # N x 192 x 71 x 71
        x = self.inception.maxpool2(x)
        # N x 192 x 35 x 35
        x = self.inception.Mixed_5b(x)
        # N x 256 x 35 x 35
        x = self.inception.Mixed_5c(x)
        # N x 288 x 35 x 35
        x = self.inception.Mixed_5d(x)
        # N x 288 x 35 x 35
        x = self.inception.Mixed_6a(x)
        # N x 768 x 17 x 17
        x = self.inception.Mixed_6b(x)
        # N x 768 x 17 x 17
        x = self.inception.Mixed_6c(x)
        # N x 768 x 17 x 17
        x = self.inception.Mixed_6d(x)
        # N x 768 x 17 x 17
        x = self.inception.Mixed_6e(x)
        # N x 768 x 17 x 17
        # è·³è¿‡ AuxLogits
        x = self.inception.Mixed_7a(x)
        # N x 1280 x 8 x 8
        x = self.inception.Mixed_7b(x)
        # N x 2048 x 8 x 8
        x = self.inception.Mixed_7c(x)
        # N x 2048 x 8 x 8
        # Adaptive average pooling
        x = self.inception.avgpool(x)
        # N x 2048 x 1 x 1
        
        # æ‰å¹³åŒ–ç‰¹å¾
        features = x.view(x.size(0), -1)
        # N x 2048
        
        return features

def calculate_frechet_distance(mu1: np.ndarray, sigma1: np.ndarray, 
                             mu2: np.ndarray, sigma2: np.ndarray, 
                             eps: float = 1e-6) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå¤šå˜é‡é«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»
    
    Args:
        mu1, mu2: å‡å€¼å‘é‡
        sigma1, sigma2: åæ–¹å·®çŸ©é˜µ
        eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
    
    Returns:
        FIDåˆ†æ•°
    """
    mu1 = np.atleast_1d(mu1)
    mu2 = np.atleast_1d(mu2)
    
    sigma1 = np.atleast_2d(sigma1)
    sigma2 = np.atleast_2d(sigma2)
    
    assert mu1.shape == mu2.shape, "å‡å€¼å‘é‡å½¢çŠ¶ä¸åŒ¹é…"
    assert sigma1.shape == sigma2.shape, "åæ–¹å·®çŸ©é˜µå½¢çŠ¶ä¸åŒ¹é…"
    
    diff = mu1 - mu2
    
    # è®¡ç®—åæ–¹å·®çŸ©é˜µçš„å¹³æ–¹æ ¹
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    
    # å¤„ç†æ•°å€¼è¯¯å·®
    if not np.isfinite(covmean).all():
        msg = ("fid calculation produces singular product; "
               "adding %s to diagonal of cov estimates") % eps
        print(msg)
        offset = np.eye(sigma1.shape[0]) * eps
        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))
    
    # æ•°å€¼ç¨³å®šæ€§
    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.absolute(covmean.imag))
            raise ValueError(f"è™šéƒ¨è¿‡å¤§: {m}")
        covmean = covmean.real
    
    tr_covmean = np.trace(covmean)
    
    return (diff.dot(diff) + np.trace(sigma1) + 
            np.trace(sigma2) - 2 * tr_covmean)

def extract_features_from_dataloader(feature_extractor: InceptionV3Features,
                                   dataloader: DataLoader,
                                   max_samples: Optional[int] = None,
                                   description: str = "æå–ç‰¹å¾") -> np.ndarray:
    """
    ä»æ•°æ®åŠ è½½å™¨ä¸­æå–ç‰¹å¾
    
    Args:
        feature_extractor: ç‰¹å¾æå–å™¨
        dataloader: æ•°æ®åŠ è½½å™¨
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        description: è¿›åº¦æ¡æè¿°
    
    Returns:
        features: [N, 2048] ç‰¹å¾æ•°ç»„
    """
    features_list = []
    sample_count = 0
    
    feature_extractor.eval()
    with torch.no_grad():
        pbar = tqdm(dataloader, desc=description)
        for batch_idx, (images, _) in enumerate(pbar):
            if max_samples and sample_count >= max_samples:
                break
                
            images = images.to(feature_extractor.device)
            
            # å¦‚æœè¾“å…¥æ˜¯[-1,1]èŒƒå›´ï¼Œéœ€è¦åå½’ä¸€åŒ–åˆ°[0,1]
            if images.min() < 0:
                images = denormalize(images)
            
            batch_features = feature_extractor(images)
            features_list.append(batch_features.cpu().numpy())
            
            sample_count += images.size(0)
            pbar.set_postfix({'samples': sample_count})
            
            if max_samples and sample_count >= max_samples:
                break
    
    features = np.concatenate(features_list, axis=0)
    if max_samples:
        features = features[:max_samples]
    
    return features

def extract_features_from_images(feature_extractor: InceptionV3Features,
                                images: torch.Tensor) -> np.ndarray:
    """
    ä»å›¾åƒå¼ é‡ä¸­æå–ç‰¹å¾
    
    Args:
        feature_extractor: ç‰¹å¾æå–å™¨
        images: [N, 3, H, W] å›¾åƒå¼ é‡
    
    Returns:
        features: [N, 2048] ç‰¹å¾æ•°ç»„
    """
    feature_extractor.eval()
    with torch.no_grad():
        # å¦‚æœè¾“å…¥æ˜¯[-1,1]èŒƒå›´ï¼Œéœ€è¦åå½’ä¸€åŒ–åˆ°[0,1]
        if images.min() < 0:
            images = denormalize(images)
        
        # åˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜
        batch_size = 32
        features_list = []
        
        for i in range(0, images.size(0), batch_size):
            batch = images[i:i+batch_size].to(feature_extractor.device)
            batch_features = feature_extractor(batch)
            features_list.append(batch_features.cpu().numpy())
        
        features = np.concatenate(features_list, axis=0)
    
    return features

def calculate_statistics(features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    è®¡ç®—ç‰¹å¾çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µ
    
    Args:
        features: [N, D] ç‰¹å¾æ•°ç»„
    
    Returns:
        mu: å‡å€¼å‘é‡
        sigma: åæ–¹å·®çŸ©é˜µ
    """
    mu = np.mean(features, axis=0)
    sigma = np.cov(features, rowvar=False)
    return mu, sigma

class FIDEvaluator:
    """
    FIDè¯„ä¼°å™¨
    """
    def __init__(self, device='cuda'):
        self.device = device
        self.feature_extractor = InceptionV3Features(device)
        self.real_features_cache = None
        self.real_mu = None
        self.real_sigma = None
    
    def compute_real_features(self, real_dataloader: DataLoader, 
                            max_samples: Optional[int] = None,
                            cache: bool = True):
        """
        è®¡ç®—çœŸå®æ•°æ®çš„ç‰¹å¾ç»Ÿè®¡
        
        Args:
            real_dataloader: çœŸå®æ•°æ®çš„æ•°æ®åŠ è½½å™¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            cache: æ˜¯å¦ç¼“å­˜ç»“æœ
        """
        print("è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾...")
        real_features = extract_features_from_dataloader(
            self.feature_extractor, real_dataloader, max_samples, "æå–çœŸå®æ•°æ®ç‰¹å¾"
        )
        
        self.real_mu, self.real_sigma = calculate_statistics(real_features)
        
        if cache:
            self.real_features_cache = real_features
        
        print(f"çœŸå®æ•°æ®ç‰¹å¾è®¡ç®—å®Œæˆ: {real_features.shape[0]} ä¸ªæ ·æœ¬")
    
    def calculate_fid_score(self, generated_images: torch.Tensor) -> float:
        """
        è®¡ç®—ç”Ÿæˆå›¾åƒçš„FIDåˆ†æ•°
        
        Args:
            generated_images: [N, 3, H, W] ç”Ÿæˆçš„å›¾åƒå¼ é‡
        
        Returns:
            FIDåˆ†æ•°
        """
        if self.real_mu is None or self.real_sigma is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ compute_real_features() è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾")
        
        # æå–ç”Ÿæˆå›¾åƒç‰¹å¾
        gen_features = extract_features_from_images(self.feature_extractor, generated_images)
        gen_mu, gen_sigma = calculate_statistics(gen_features)
        
        # è®¡ç®—FID
        fid_score = calculate_frechet_distance(self.real_mu, self.real_sigma, 
                                             gen_mu, gen_sigma)
        
        return fid_score
    
    def evaluate_model(self, model, num_samples: int = 1000, 
                      batch_size: int = 8, num_classes: int = 31,
                      num_inference_steps: int = 250, guidance_scale: float = 7.5) -> float:
        """
        è¯„ä¼°æ¨¡å‹çš„FIDåˆ†æ•°
        
        Args:
            model: LDMæ¨¡å‹
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°
            batch_size: æ‰¹é‡å¤§å°
            num_classes: ç±»åˆ«æ•°
            num_inference_steps: DDIMæ¨ç†æ­¥æ•°
            guidance_scale: CFGå¼•å¯¼å¼ºåº¦
        
        Returns:
            FIDåˆ†æ•°
        """
        if self.real_mu is None or self.real_sigma is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ compute_real_features() è®¡ç®—çœŸå®æ•°æ®ç‰¹å¾")
        
        model.eval()
        generated_images = []
        
        # ğŸ”§ è®¡ç®—æ€»æ‰¹æ¬¡æ•°å’Œåˆ›å»ºè¿›åº¦æ¡
        total_batches = (num_samples + batch_size - 1) // batch_size
        progress_bar = tqdm(
            total=total_batches, 
            desc=f"  FIDç”Ÿæˆ({num_inference_steps}æ­¥)", 
            unit="batch",
            ncols=80,  # æ§åˆ¶è¿›åº¦æ¡å®½åº¦
            leave=False  # å®Œæˆåæ¸…é™¤è¿›åº¦æ¡
        )
        
        # ç”Ÿæˆæ ·æœ¬
        with torch.no_grad():
            samples_per_class = num_samples // num_classes
            remaining_samples = num_samples % num_classes
            
            for class_id in range(num_classes):
                # æ¯ä¸ªç±»åˆ«ç”Ÿæˆç›¸åº”æ•°é‡çš„æ ·æœ¬
                current_samples = samples_per_class
                if class_id < remaining_samples:
                    current_samples += 1
                
                if current_samples == 0:
                    continue
                
                # åˆ†æ‰¹ç”Ÿæˆ
                for i in range(0, current_samples, batch_size):
                    current_batch_size = min(batch_size, current_samples - i)
                    class_labels = torch.tensor([class_id] * current_batch_size, 
                                               device=self.device)
                    
                    images = model.sample(
                        batch_size=current_batch_size,
                        class_labels=class_labels,
                        num_inference_steps=num_inference_steps,  # ğŸ”§ ä½¿ç”¨ä¼ å…¥çš„æ¨ç†æ­¥æ•°
                        guidance_scale=guidance_scale,  # ğŸ”§ ä½¿ç”¨ä¼ å…¥çš„å¼•å¯¼å¼ºåº¦
                        verbose=False  # ğŸ”§ å…³é—­sampleæ–¹æ³•å†…éƒ¨çš„è¯¦ç»†è¾“å‡º
                    )
                    
                    generated_images.append(images.cpu())
                    
                    # ğŸ”§ æ›´æ–°è¿›åº¦æ¡
                    progress_bar.update(1)
                    progress_bar.set_postfix({
                        'class': f'{class_id+1}/{num_classes}',
                        'samples': len(generated_images) * batch_size
                    })
        
        progress_bar.close()  # å…³é—­è¿›åº¦æ¡
        
        # åˆå¹¶æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒ
        all_generated = torch.cat(generated_images, dim=0)
        print(f"  è®¡ç®—FIDåˆ†æ•°...")
        
        # è®¡ç®—FID
        fid_score = self.calculate_fid_score(all_generated)
        
        return fid_score 
