"""
ğŸ”® å¢å¼ºç‰ˆLDMæ¨ç†è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒ
æ”¯æŒæ‰¹é‡ç”Ÿæˆã€è´¨é‡è¯„ä¼°å’Œå¯è§†åŒ–
"""

import yaml
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import sys
from tqdm import tqdm
import torchvision.transforms as transforms
from typing import List, Dict, Optional, Tuple
import argparse
from datetime import datetime

# æ·»åŠ VAEæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'VAE'))

from stable_ldm import create_stable_ldm
from metrics import DiffusionMetrics, denormalize_for_metrics
from dataset_adapter import build_dataloader  # ğŸ”§ ä½¿ç”¨é€‚é…å™¨

class EnhancedInference:
    """å¢å¼ºç‰ˆLDMæ¨ç†å™¨"""
    
    def __init__(self, config_path: str, model_path: str):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”® æ¨ç†è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_stable_ldm(self.config).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        self.load_model(model_path)
        
        # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
        self.metrics_calculator = DiffusionMetrics(self.device)
        
        # ç±»åˆ«æ ‡ç­¾æ˜ å°„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        self.class_names = self._get_class_names()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ”¯æŒ{self.config['unet']['num_classes']}ä¸ªç±»åˆ«")
    
    def load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
            
            self.model.load_state_dict(state_dict)
            self.model.eval()
            
            print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ: {model_path}")
            
            # æ‰“å°é¢å¤–ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'epoch' in checkpoint:
                print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
            if 'best_fid' in checkpoint:
                print(f"ğŸ† æœ€ä½³FID: {checkpoint['best_fid']:.2f}")
                
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def _get_class_names(self) -> List[str]:
        """è·å–ç±»åˆ«åç§°"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®æ•°æ®é›†å®šä¹‰ç±»åˆ«åç§°
        # æš‚æ—¶ä½¿ç”¨é€šç”¨åç§°
        return [f"Class_{i}" for i in range(self.config['unet']['num_classes'])]
    
    def generate_single_class(
        self, 
        class_id: int, 
        num_samples: int = 16,
        **sampling_kwargs
    ) -> torch.Tensor:
        """ä¸ºå•ä¸ªç±»åˆ«ç”Ÿæˆå›¾åƒ"""
        
        if class_id < 0 or class_id >= self.config['unet']['num_classes']:
            raise ValueError(f"ç±»åˆ«ID {class_id} è¶…å‡ºèŒƒå›´ [0, {self.config['unet']['num_classes']-1}]")
        
        class_labels = torch.full((num_samples,), class_id, dtype=torch.long, device=self.device)
        
        with torch.no_grad():
            images = self.model.sample(
                batch_size=num_samples,
                class_labels=class_labels,
                num_inference_steps=sampling_kwargs.get('num_inference_steps', 
                                                       self.config['inference']['num_inference_steps']),
                guidance_scale=sampling_kwargs.get('guidance_scale', 
                                                 self.config['inference']['guidance_scale']),
                eta=sampling_kwargs.get('eta', self.config['inference']['eta']),
                use_ema=sampling_kwargs.get('use_ema', True),
                verbose=sampling_kwargs.get('verbose', True)
            )
        
        return images
    
    def generate_mixed_classes(
        self, 
        num_samples: int = 16,
        class_distribution: Optional[List[int]] = None,
        **sampling_kwargs
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ··åˆç±»åˆ«çš„å›¾åƒ"""
        
        if class_distribution is None:
            # å‡åŒ€åˆ†å¸ƒ
            class_labels = torch.randint(0, self.config['unet']['num_classes'], 
                                       (num_samples,), device=self.device)
        else:
            # æŒ‰æŒ‡å®šåˆ†å¸ƒç”Ÿæˆ
            if len(class_distribution) != self.config['unet']['num_classes']:
                raise ValueError("class_distributioné•¿åº¦å¿…é¡»ç­‰äºç±»åˆ«æ•°")
            
            # æŒ‰æƒé‡éšæœºé‡‡æ ·
            weights = torch.tensor(class_distribution, dtype=torch.float)
            class_labels = torch.multinomial(weights, num_samples, replacement=True).to(self.device)
        
        with torch.no_grad():
            images = self.model.sample(
                batch_size=num_samples,
                class_labels=class_labels,
                num_inference_steps=sampling_kwargs.get('num_inference_steps', 
                                                       self.config['inference']['num_inference_steps']),
                guidance_scale=sampling_kwargs.get('guidance_scale', 
                                                 self.config['inference']['guidance_scale']),
                eta=sampling_kwargs.get('eta', self.config['inference']['eta']),
                use_ema=sampling_kwargs.get('use_ema', True),
                verbose=sampling_kwargs.get('verbose', True)
            )
        
        return images, class_labels
    
    def generate_unconditional(self, num_samples: int = 16, **sampling_kwargs) -> torch.Tensor:
        """ç”Ÿæˆæ— æ¡ä»¶å›¾åƒ"""
        
        with torch.no_grad():
            images = self.model.sample(
                batch_size=num_samples,
                class_labels=None,  # æ— æ¡ä»¶ç”Ÿæˆ
                num_inference_steps=sampling_kwargs.get('num_inference_steps', 
                                                       self.config['inference']['num_inference_steps']),
                guidance_scale=sampling_kwargs.get('guidance_scale', 1.0),  # æ— æ¡ä»¶æ—¶å…³é—­CFG
                eta=sampling_kwargs.get('eta', self.config['inference']['eta']),
                use_ema=sampling_kwargs.get('use_ema', True),
                verbose=sampling_kwargs.get('verbose', True)
            )
        
        return images
    
    def evaluate_generation_quality(
        self, 
        num_samples: int = 1000,
        batch_size: int = 32
    ) -> Dict[str, float]:
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        
        print(f"ğŸ”„ è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œå…±ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬...")
        
        all_images = []
        all_is_scores = []
        
        # åˆ†æ‰¹ç”Ÿæˆ
        num_batches = (num_samples + batch_size - 1) // batch_size
        
        for i in tqdm(range(num_batches), desc="ç”Ÿæˆæ ·æœ¬"):
            current_batch_size = min(batch_size, num_samples - i * batch_size)
            
            # ç”Ÿæˆæ··åˆç±»åˆ«å›¾åƒ
            images, _ = self.generate_mixed_classes(
                num_samples=current_batch_size,
                verbose=False
            )
            
            all_images.append(images.cpu())
        
        # åˆå¹¶æ‰€æœ‰å›¾åƒ
        all_images = torch.cat(all_images, dim=0)
        
        # è®¡ç®—ISåˆ†æ•°
        try:
            is_mean, is_std = self.metrics_calculator.inception_calculator.calculate_inception_score(
                all_images
            )
            print(f"ğŸ“Š ISåˆ†æ•°: {is_mean:.2f} Â± {is_std:.2f}")
        except Exception as e:
            print(f"âš ï¸ ISåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            is_mean, is_std = None, None
        
        # è®¡ç®—å›¾åƒç»Ÿè®¡ä¿¡æ¯
        img_stats = {
            'mean': all_images.mean().item(),
            'std': all_images.std().item(),
            'min': all_images.min().item(),
            'max': all_images.max().item()
        }
        
        # æ£€æŸ¥å›¾åƒè´¨é‡æŒ‡æ ‡
        quality_metrics = {
            'inception_score_mean': is_mean,
            'inception_score_std': is_std,
            'image_mean': img_stats['mean'],
            'image_std': img_stats['std'],
            'pixel_range_min': img_stats['min'],
            'pixel_range_max': img_stats['max'],
            'total_samples': len(all_images)
        }
        
        return quality_metrics
    
    def save_images(
        self, 
        images: torch.Tensor, 
        output_dir: str,
        prefix: str = "generated",
        class_labels: Optional[torch.Tensor] = None
    ):
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ç½‘æ ¼å›¾åƒ
        import torchvision.utils as vutils
        grid_path = os.path.join(output_dir, f"{prefix}_grid.png")
        vutils.save_image(
            images, grid_path,
            nrow=int(np.sqrt(len(images))),
            normalize=True, value_range=(0, 1)
        )
        print(f"ğŸ“¸ ç½‘æ ¼å›¾åƒå·²ä¿å­˜: {grid_path}")
        
        # ä¿å­˜å•ç‹¬å›¾åƒ
        to_pil = transforms.ToPILImage()
        
        for i, img in enumerate(images):
            # ç¡®ä¿å›¾åƒåœ¨[0,1]èŒƒå›´å†…
            img = torch.clamp(img, 0, 1)
            pil_img = to_pil(img)
            
            if class_labels is not None:
                class_name = self.class_names[class_labels[i].item()]
                filename = f"{prefix}_{i:04d}_{class_name}.png"
            else:
                filename = f"{prefix}_{i:04d}.png"
            
            img_path = os.path.join(output_dir, filename)
            pil_img.save(img_path)
        
        print(f"ğŸ’¾ {len(images)}å¼ å•ç‹¬å›¾åƒå·²ä¿å­˜åˆ°: {output_dir}")
    
    def compare_sampling_parameters(
        self, 
        class_id: int = 0,
        num_samples: int = 4
    ):
        """æ¯”è¾ƒä¸åŒé‡‡æ ·å‚æ•°çš„æ•ˆæœ"""
        
        print(f"ğŸ”¬ æ¯”è¾ƒé‡‡æ ·å‚æ•°æ•ˆæœ (ç±»åˆ«: {self.class_names[class_id]})")
        
        # ä¸åŒçš„å‚æ•°ç»„åˆ
        param_configs = [
            {'num_inference_steps': 25, 'guidance_scale': 1.0, 'eta': 0.0, 'name': 'Fast_NoGuidance'},
            {'num_inference_steps': 50, 'guidance_scale': 7.5, 'eta': 0.0, 'name': 'Standard_DDIM'},
            {'num_inference_steps': 75, 'guidance_scale': 8.5, 'eta': 0.3, 'name': 'High_Quality'},
            {'num_inference_steps': 100, 'guidance_scale': 10.0, 'eta': 0.5, 'name': 'Maximum_Quality'},
        ]
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comparison_dir = f"sampling_comparison_{timestamp}"
        os.makedirs(comparison_dir, exist_ok=True)
        
        for config in param_configs:
            print(f"ğŸ”„ æµ‹è¯•é…ç½®: {config['name']}")
            
            # ç”Ÿæˆå›¾åƒ
            images = self.generate_single_class(
                class_id=class_id,
                num_samples=num_samples,
                **{k: v for k, v in config.items() if k != 'name'}
            )
            
            # ä¿å­˜ç»“æœ
            self.save_images(
                images, 
                comparison_dir,
                prefix=config['name']
            )
        
        print(f"âœ… é‡‡æ ·å‚æ•°æ¯”è¾ƒå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {comparison_dir}")
    
    def interpolate_between_classes(
        self, 
        class_a: int, 
        class_b: int,
        num_steps: int = 8,
        num_samples: int = 1
    ):
        """ç±»åˆ«é—´æ’å€¼ç”Ÿæˆ"""
        
        print(f"ğŸŒˆ ç±»åˆ«æ’å€¼: {self.class_names[class_a]} â†’ {self.class_names[class_b]}")
        
        # åˆ›å»ºæ’å€¼æƒé‡
        alphas = torch.linspace(0, 1, num_steps)
        
        all_images = []
        
        for alpha in alphas:
            # æ··åˆç±»åˆ«embeddingï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            if alpha < 0.5:
                current_class = class_a
            else:
                current_class = class_b
            
            images = self.generate_single_class(
                class_id=current_class,
                num_samples=num_samples,
                verbose=False
            )
            
            all_images.append(images)
        
        # åˆå¹¶æ‰€æœ‰å›¾åƒ
        interpolation_images = torch.cat(all_images, dim=0)
        
        # ä¿å­˜æ’å€¼ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        interp_dir = f"interpolation_{timestamp}"
        self.save_images(
            interpolation_images, 
            interp_dir,
            prefix=f"interp_{self.class_names[class_a]}_to_{self.class_names[class_b]}"
        )
        
        print(f"âœ… ç±»åˆ«æ’å€¼å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {interp_dir}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆLDMæ¨ç†')
    parser.add_argument('--config', type=str, default='config_enhanced_unet.yaml', 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--mode', type=str, default='generate',
                       choices=['generate', 'evaluate', 'compare', 'interpolate'],
                       help='æ¨ç†æ¨¡å¼')
    parser.add_argument('--class_id', type=int, default=0,
                       help='ç±»åˆ«ID')
    parser.add_argument('--num_samples', type=int, default=16,
                       help='ç”Ÿæˆæ ·æœ¬æ•°')
    parser.add_argument('--output_dir', type=str, default='inference_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--steps', type=int, default=None,
                       help='æ¨ç†æ­¥æ•°')
    parser.add_argument('--guidance', type=float, default=None,
                       help='CFGå¼•å¯¼å¼ºåº¦')
    parser.add_argument('--eta', type=float, default=None,
                       help='DDIMéšæœºæ€§å‚æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = EnhancedInference(args.config, args.model)
    
    # å‡†å¤‡é‡‡æ ·å‚æ•°
    sampling_kwargs = {}
    if args.steps is not None:
        sampling_kwargs['num_inference_steps'] = args.steps
    if args.guidance is not None:
        sampling_kwargs['guidance_scale'] = args.guidance
    if args.eta is not None:
        sampling_kwargs['eta'] = args.eta
    
    # æ‰§è¡Œæ¨ç†
    if args.mode == 'generate':
        print(f"ğŸ¨ ç”Ÿæˆæ¨¡å¼: ç±»åˆ«{args.class_id}, {args.num_samples}ä¸ªæ ·æœ¬")
        images = inferencer.generate_single_class(
            class_id=args.class_id,
            num_samples=args.num_samples,
            **sampling_kwargs
        )
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"{args.output_dir}_{timestamp}"
        
        inferencer.save_images(images, output_dir, f"class_{args.class_id}")
        
    elif args.mode == 'evaluate':
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å¼: {args.num_samples}ä¸ªæ ·æœ¬")
        quality_metrics = inferencer.evaluate_generation_quality(args.num_samples)
        
        print("\nğŸ“Š ç”Ÿæˆè´¨é‡è¯„ä¼°ç»“æœ:")
        for key, value in quality_metrics.items():
            if value is not None:
                print(f"  {key}: {value:.4f}")
    
    elif args.mode == 'compare':
        print("ğŸ”¬ å‚æ•°æ¯”è¾ƒæ¨¡å¼")
        inferencer.compare_sampling_parameters(args.class_id, args.num_samples)
    
    elif args.mode == 'interpolate':
        if args.class_id + 1 < inferencer.config['unet']['num_classes']:
            print("ğŸŒˆ æ’å€¼æ¨¡å¼")
            inferencer.interpolate_between_classes(
                args.class_id, 
                args.class_id + 1,
                num_samples=args.num_samples
            )
        else:
            print("âš ï¸ ç±»åˆ«IDè¿‡å¤§ï¼Œæ— æ³•è¿›è¡Œæ’å€¼")

if __name__ == "__main__":
    main() 
