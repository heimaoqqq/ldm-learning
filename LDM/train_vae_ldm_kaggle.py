"""
VAE-LDM Kaggleå®Œæ•´è®­ç»ƒè„šæœ¬
ä½¿ç”¨YAMLé…ç½®æ–‡ä»¶ï¼ŒåŒ…å«å®Œæ•´è®­ç»ƒå™¨ï¼Œé€‚é…P100 GPUèµ„æºé™åˆ¶
"""
import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from datetime import datetime
from typing import Dict, Any, Optional

# æ·»åŠ VAEè·¯å¾„ - é€‚é…ä¸åŒç¯å¢ƒ
vae_path_options = [
    '/kaggle/working/VAE',  # Kaggleç¯å¢ƒ
    os.path.join(os.path.dirname(__file__), '..', 'VAE'),  # ç›¸å¯¹è·¯å¾„
    '../VAE'  # å¤‡ç”¨ç›¸å¯¹è·¯å¾„
]

for vae_path in vae_path_options:
    if os.path.exists(vae_path):
        sys.path.append(vae_path)
        print(f"âœ… VAEè·¯å¾„å·²æ·»åŠ : {vae_path}")
        break
else:
    print("âš ï¸ æœªæ‰¾åˆ°VAEç›®å½•ï¼Œå¯èƒ½å¯¼è‡´å¯¼å…¥é”™è¯¯")

from vae_ldm import create_vae_ldm
from dataset_adapter import build_dataloader
from fid_evaluation import FIDEvaluator
from metrics import denormalize_for_metrics

class VAELDMTrainer:
    """VAE-LDMè®­ç»ƒå™¨ - Kaggleä¼˜åŒ–ç‰ˆ"""
    
    def __init__(
        self,
        vae_checkpoint_path: str,
        data_dir: str,
        save_dir: str,
        num_classes: int = 31,
        image_size: int = 32,
        diffusion_steps: int = 1000,
        noise_schedule: str = "cosine",
        batch_size: int = 4,
        lr: float = 1e-4,
        weight_decay: float = 0.01,
        max_epochs: int = 100,
        log_interval: int = 10,
        save_interval_epochs: int = 50,  # epoch-basedä¿å­˜é—´éš”
        eval_interval: int = 10,
        gradient_accumulation_steps: int = 2,
        enable_wandb: bool = False,
        device: str = 'cuda'
    ):
        self.vae_checkpoint_path = vae_checkpoint_path
        self.data_dir = data_dir
        self.save_dir = save_dir
        self.batch_size = batch_size
        self.lr = lr
        self.weight_decay = weight_decay
        self.max_epochs = max_epochs
        self.log_interval = log_interval
        self.save_interval_epochs = save_interval_epochs  # ä¿®æ”¹ä¸ºepoch-based
        self.eval_interval = eval_interval
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.enable_wandb = enable_wandb
        self.device = device
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸ”§ åˆå§‹åŒ–VAE-LDMæ¨¡å‹...")
        self.model = create_vae_ldm(
            vae_checkpoint_path=vae_checkpoint_path,
            image_size=image_size,
            num_classes=num_classes,
            diffusion_steps=diffusion_steps,
            noise_schedule=noise_schedule,
            device=device
        )
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        print("ğŸ“Š åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
        dataloader_config = {
            'dataset': {
                'root_dir': data_dir,
                'batch_size': batch_size,
                'num_workers': 4,
                'val_split': 0.1,
                'shuffle_train': True
            },
            'unet': {
                'num_classes': num_classes
            }
        }
        self.train_loader, self.val_loader = build_dataloader(dataloader_config)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        print("âš™ï¸ åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        self.optimizer = self.model.configure_optimizers(lr=lr, weight_decay=weight_decay)
        
        # åˆå§‹åŒ–FIDè¯„ä¼°å™¨
        print("ğŸ“ˆ åˆå§‹åŒ–FIDè¯„ä¼°å™¨...")
        self.fid_evaluator = FIDEvaluator(device=self.device)
        self.fid_evaluator.compute_real_features(self.val_loader)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.current_step = 0
        self.best_fid = float('inf')
        
        print("âœ… VAELDMTrainer åˆå§‹åŒ–å®Œæˆ")
        
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = []
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch+1}/{self.max_epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            images = batch['image'].to(self.device)
            labels = batch['label'].to(self.device, dtype=torch.long)
            
            # å‰å‘ä¼ æ’­
            losses = self.model(images, labels)
            loss = losses['loss'].mean()
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / self.gradient_accumulation_steps
            loss.backward()
            
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.unet.parameters(), 1.0)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                self.optimizer.step()
                self.optimizer.zero_grad()
                
                self.current_step += 1
                
                # è®°å½•æ—¥å¿—
                if self.current_step % self.log_interval == 0:
                    progress_bar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'step': self.current_step
                    })
            
            epoch_losses.append(loss.item() * self.gradient_accumulation_steps)
            
        return {'train_loss': np.mean(epoch_losses)}
    
    def evaluate(self) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹ï¼ˆè®¡ç®—FIDï¼‰"""
        print("ğŸ“Š å¼€å§‹FIDè¯„ä¼°...")
        self.model.eval()
        
        # ç”Ÿæˆæ ·æœ¬ç”¨äºFIDè®¡ç®—
        num_samples = 150  # 3ç±» Ã— 50æ ·æœ¬
        class_labels = torch.randint(0, 31, (num_samples,), device=self.device, dtype=torch.long)
        
        with torch.no_grad():
            generated_images, _ = self.model.sample(
                num_samples=num_samples,
                class_labels=class_labels,
                use_ddim=True,
                eta=0.0
            )
        
        # åå½’ä¸€åŒ–åˆ°[0,1]ç”¨äºFIDè®¡ç®—
        generated_images = denormalize_for_metrics(generated_images)
        
        # è®¡ç®—FID
        fid_score = self.fid_evaluator.calculate_fid_score(generated_images)
        
        return {'fid': fid_score}
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(self.save_dir, f'checkpoint_epoch_{self.current_epoch}.pt')
        
        self.model.save_checkpoint(
            filepath=checkpoint_path,
            epoch=self.current_epoch,
            optimizer_state=self.optimizer.state_dict()
        )
        
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_model.pt')
            self.model.save_checkpoint(
                filepath=best_path,
                epoch=self.current_epoch,
                optimizer_state=self.optimizer.state_dict()
            )
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹VAE-LDMè®­ç»ƒ")
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(self.train_loader)} batches")
        print(f"ğŸ“ˆ éªŒè¯æ•°æ®: {len(self.val_loader)} batches")
        print(f"ğŸ’¾ æ¯ {self.save_interval_epochs} epochs ä¿å­˜æ¨¡å‹")
        print(f"ğŸ“Š æ¯ {self.eval_interval} epochs è¯„ä¼°FID")
        
        for epoch in range(self.max_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()
            
            print(f"Epoch {epoch+1}/{self.max_epochs} - Train Loss: {train_metrics['train_loss']:.4f}")
            
            # å®šæœŸè¯„ä¼°
            if (epoch + 1) % self.eval_interval == 0:
                eval_metrics = self.evaluate()
                fid_score = eval_metrics['fid']
                
                print(f"ğŸ“Š FID Score: {fid_score:.2f}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = fid_score < self.best_fid
                if is_best:
                    self.best_fid = fid_score
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³FID: {fid_score:.2f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if is_best:
                    self.save_checkpoint(is_best=True)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.save_interval_epochs == 0:
                self.save_checkpoint(is_best=False)
                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: epoch {epoch+1}")
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³FID: {self.best_fid:.2f}")

def load_config(config_path: str = "config_kaggle.yaml") -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        return create_default_config()
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
        return create_default_config()

def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®ï¼ˆå¦‚æœYAMLæ–‡ä»¶ä¸å­˜åœ¨ï¼‰"""
    print("ğŸ“ ä½¿ç”¨é»˜è®¤é…ç½®...")
    return {
        'paths': {
            'vae_checkpoint': '/kaggle/working/VAE/best_fid_model.pt',
            'data_dir': '/kaggle/input/dataset',
            'save_dir': '/kaggle/working/checkpoints'
        },
        'model': {
            'num_classes': 31,
            'image_size': 32,
            'diffusion_steps': 1000,
            'noise_schedule': 'cosine'
        },
        'training': {
            'batch_size': 4,
            'gradient_accumulation_steps': 2,
            'learning_rate': 1e-4,
            'weight_decay': 0.01,
            'max_epochs': 100,
            'log_interval': 10,
            'save_interval_epochs': 50,
            'eval_interval': 10,
            'enable_wandb': False,
            'num_workers': 2
        },
        'evaluation': {
            'eval_classes': 3,
            'samples_per_class': 50,
            'ddim_steps': 50,
            'eta': 0.0,
            'max_real_samples': 500
        },
        'gpu': {
            'device': 'cuda',
            'mixed_precision': False,
            'gradient_clip': 1.0
        },
        'experiment': {
            'name': 'VAE-LDM-Kaggle',
            'description': 'VAEæ½œåœ¨æ‰©æ•£æ¨¡å‹ - Kaggle P100ä¼˜åŒ–ç‰ˆ',
            'version': 'v1.0'
        }
    }

def print_config_summary(config: Dict[str, Any]):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("\n" + "="*80)
    print("ğŸ¯ VAE-LDM Kaggleè®­ç»ƒé…ç½®")
    print("="*80)
    
    print(f"ğŸ“ å®éªŒ: {config['experiment']['name']} {config['experiment']['version']}")
    print(f"ğŸ“„ æè¿°: {config['experiment']['description']}")
    print()
    
    print("ğŸ“ è·¯å¾„é…ç½®:")
    for key, value in config['paths'].items():
        print(f"   {key}: {value}")
    print()
    
    print("ğŸ›ï¸ æ¨¡å‹é…ç½®:")
    for key, value in config['model'].items():
        print(f"   {key}: {value}")
    print()
    
    print("ğŸƒ è®­ç»ƒé…ç½®:")
    train_config = config['training']
    effective_batch = train_config['batch_size'] * train_config['gradient_accumulation_steps']
    print(f"   batch_size: {train_config['batch_size']} (æœ‰æ•ˆbatch: {effective_batch})")
    print(f"   learning_rate: {train_config['learning_rate']}")
    print(f"   max_epochs: {train_config['max_epochs']}")
    print(f"   eval_interval: {train_config['eval_interval']} epochs")
    print()
    
    print("ğŸ“Š è¯„ä¼°é…ç½®:")
    eval_config = config['evaluation']
    total_samples = eval_config['eval_classes'] * eval_config['samples_per_class']
    print(f"   eval_classes: {eval_config['eval_classes']}")
    print(f"   samples_per_class: {eval_config['samples_per_class']} (æ€»è®¡: {total_samples})")
    print(f"   ddim_steps: {eval_config['ddim_steps']}")
    print()
    
    print("ğŸ® GPUé…ç½®:")
    for key, value in config['gpu'].items():
        print(f"   {key}: {value}")
    
    print("="*80)

def validate_paths(config: Dict[str, Any]) -> bool:
    """éªŒè¯è·¯å¾„é…ç½®"""
    paths = config['paths']
    
    # æ£€æŸ¥VAEæ£€æŸ¥ç‚¹
    vae_checkpoint_exists = os.path.exists(paths['vae_checkpoint'])
    if not vae_checkpoint_exists:
        print(f"âš ï¸ VAEæ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {paths['vae_checkpoint']}")
        print("ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„VAEï¼ˆæ€§èƒ½ä¼šä¸‹é™ï¼‰")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. æ£€æŸ¥VAEæ£€æŸ¥ç‚¹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   2. ç¡®ä¿VAEæ¨¡å‹å·²è®­ç»ƒå®Œæˆå¹¶ä¸Šä¼ ")
        print("   3. å¯ä»¥å…ˆç”¨éšæœºVAEè¿›è¡Œæµ‹è¯•")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­
        print("\nâ“ æ˜¯å¦ç»§ç»­è®­ç»ƒï¼Ÿ(Kaggleç¯å¢ƒè‡ªåŠ¨ç»§ç»­)")
        # åœ¨Kaggleç¯å¢ƒä¸­è‡ªåŠ¨ç»§ç»­ï¼Œæœ¬åœ°ç¯å¢ƒå¯ä»¥é€‰æ‹©
        if '/kaggle/' not in paths.get('data_dir', ''):
            response = input("è¾“å…¥ 'y' ç»§ç»­ï¼Œå…¶ä»–é”®é€€å‡º: ").lower()
            if response != 'y':
                return False
    else:
        print(f"âœ… VAEæ£€æŸ¥ç‚¹æ‰¾åˆ°: {paths['vae_checkpoint']}")
    
    # æ£€æŸ¥æ•°æ®é›†
    if not os.path.exists(paths['data_dir']):
        print(f"âŒ æ•°æ®é›†æœªæ‰¾åˆ°: {paths['data_dir']}")
        print("è¯·ç¡®ä¿æ•°æ®é›†å·²ä¸Šä¼ åˆ°Kaggle")
        return False
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(paths['save_dir'], exist_ok=True)
    
    print("âœ… è·¯å¾„éªŒè¯å®Œæˆ")
    return True

def convert_config_to_trainer_args(config: Dict[str, Any]) -> Dict[str, Any]:
    """å°†YAMLé…ç½®è½¬æ¢ä¸ºè®­ç»ƒå™¨å‚æ•°"""
    return {
        # è·¯å¾„é…ç½®
        'vae_checkpoint_path': config['paths']['vae_checkpoint'],
        'data_dir': config['paths']['data_dir'],
        'save_dir': config['paths']['save_dir'],
        
        # æ¨¡å‹é…ç½®
        'num_classes': config['model']['num_classes'],
        'image_size': config['model']['image_size'],
        'diffusion_steps': config['model']['diffusion_steps'],
        'noise_schedule': config['model']['noise_schedule'],
        
        # è®­ç»ƒé…ç½®
        'batch_size': config['training']['batch_size'],
        'lr': config['training']['learning_rate'],
        'weight_decay': config['training']['weight_decay'],
        'max_epochs': config['training']['max_epochs'],
        'log_interval': config['training']['log_interval'],
        'save_interval_epochs': config['training']['save_interval_epochs'],
        'eval_interval': config['training']['eval_interval'],
        'gradient_accumulation_steps': config['training']['gradient_accumulation_steps'],
        'enable_wandb': config['training']['enable_wandb'],
        
        # GPUé…ç½®
        'device': config['gpu']['device'],
    }

def save_training_info(config: Dict[str, Any], save_dir: str):
    """ä¿å­˜è®­ç»ƒä¿¡æ¯"""
    info = {
        'start_time': datetime.now().isoformat(),
        'config': config,
        'system_info': {
            'python_version': sys.version,
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'
        }
    }
    
    info_path = os.path.join(save_dir, 'training_info.yaml')
    with open(info_path, 'w', encoding='utf-8') as f:
        yaml.dump(info, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ğŸ“‹ è®­ç»ƒä¿¡æ¯å·²ä¿å­˜: {info_path}")

def main():
    """Kaggleç¯å¢ƒè®­ç»ƒä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨VAE-LDM Kaggleè®­ç»ƒ (YAMLé…ç½®ç‰ˆ)")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_config_summary(config)
    
    # éªŒè¯è·¯å¾„
    if not validate_paths(config):
        return
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    save_training_info(config, config['paths']['save_dir'])
    
    print("\nğŸ”§ å¼€å§‹åˆå§‹åŒ–è®­ç»ƒå™¨...")
    
    try:
        # è½¬æ¢é…ç½®å¹¶åˆ›å»ºè®­ç»ƒå™¨
        trainer_args = convert_config_to_trainer_args(config)
        trainer = VAELDMTrainer(**trainer_args)
        
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print(f"ğŸ ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main() 
