"""
VAE Fine-tuning - äº‘æœåŠ¡å™¨å®Œæ•´ç‰ˆæœ¬
é’ˆå¯¹Oxford-IIIT Petæ•°æ®é›†ä¼˜åŒ–AutoencoderKL
æ•°æ®é›†è·¯å¾„ï¼š/data
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
import gc
from tqdm import tqdm
import matplotlib.pyplot as plt

# äº‘ç¯å¢ƒåˆå§‹åŒ–
print("ğŸŒ äº‘æœåŠ¡å™¨VAE Fine-tuningç¯å¢ƒåˆå§‹åŒ–...")
print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# å†…å­˜ä¼˜åŒ–è®¾ç½®
torch.backends.cudnn.benchmark = True
# æ·»åŠ å†…å­˜åˆ†é…ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸš€ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
def check_and_install_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–"""
    try:
        import diffusers
        print("âœ… diffusers å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£… diffusers...")
        os.system("pip install diffusers transformers accelerate -q")
    
    try:
        import torchvision
        print("âœ… torchvision å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£… torchvision...")
        os.system("pip install torchvision -q")

check_and_install_dependencies()

# å¯¼å…¥å¿…è¦æ¨¡å—
from diffusers import AutoencoderKL
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import glob
from sklearn.model_selection import train_test_split

# æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
try:
    import torch
    # æ£€æŸ¥PyTorchç‰ˆæœ¬å¹¶ä½¿ç”¨æ­£ç¡®çš„autocastå’ŒGradScaler
    if hasattr(torch, 'amp') and hasattr(torch.amp, 'autocast'):
        from torch.amp import autocast, GradScaler
        AUTOCAST_DEVICE = 'cuda'
        GRADSCALER_DEVICE = 'cuda'
        MIXED_PRECISION_AVAILABLE = True
        print("âœ… æ–°ç‰ˆæ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
    else:
        from torch.cuda.amp import autocast, GradScaler
        AUTOCAST_DEVICE = None
        GRADSCALER_DEVICE = None
        MIXED_PRECISION_AVAILABLE = True
        print("âœ… ä¼ ç»Ÿæ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
except ImportError:
    MIXED_PRECISION_AVAILABLE = False
    print("âš ï¸  æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨")

class OxfordPetDataset(Dataset):
    """Oxford-IIIT Petæ•°æ®é›†ç±»"""
    
    def __init__(self, data_root, annotation_file, transform=None):
        self.data_root = data_root
        self.transform = transform
        
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        self.samples = []
        with open(annotation_file, 'r') as f:
            for line in f:
                if line.startswith('#'):
                    continue  # è·³è¿‡æ³¨é‡Šè¡Œ
                parts = line.strip().split()
                if len(parts) >= 2:
                    image_id = parts[0]  # å›¾åƒID
                    class_id = int(parts[1]) - 1  # ç±»åˆ«ID (è½¬ä¸º0-indexed)
                    self.samples.append((image_id, class_id))
        
        print(f"åŠ è½½äº† {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        image_id, class_id = self.samples[idx]
        img_path = os.path.join(self.data_root, "images", f"{image_id}.jpg")
        
        # æœ‰äº›å›¾åƒå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ‰©å±•å
        if not os.path.exists(img_path):
            for ext in ['.jpeg', '.png', '.JPEG', '.PNG']:
                alt_path = os.path.join(self.data_root, "images", f"{image_id}{ext}")
                if os.path.exists(alt_path):
                    img_path = alt_path
                    break
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, class_id
        except Exception as e:
            print(f"é”™è¯¯åŠ è½½å›¾åƒ {img_path}: {e}")
            # è¿”å›ä¸€ä¸ªå ä½å›¾åƒå’Œç±»åˆ«
            placeholder = torch.zeros((3, 256, 256))
            return placeholder, class_id

def build_pet_dataloader(root_dir, batch_size=8, num_workers=0, val_split=0.2):
    """æ„å»ºOxford-IIIT Petæ•°æ®é›†çš„åŠ è½½å™¨"""
    print(f"ğŸ” åŠ è½½Oxford-IIIT Petæ•°æ®é›†: {root_dir}")
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•ç»“æ„
    train_annotation_file = os.path.join(root_dir, "annotations", "trainval.txt")
    test_annotation_file = os.path.join(root_dir, "annotations", "test.txt")
    
    if not os.path.exists(root_dir):
        raise ValueError(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {root_dir}")
    
    if not os.path.exists(train_annotation_file) or not os.path.exists(test_annotation_file):
        raise ValueError(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åŠ è½½è®­ç»ƒå’ŒéªŒè¯é›†
    train_val_dataset = OxfordPetDataset(
        data_root=root_dir,
        annotation_file=train_annotation_file,
        transform=transform
    )
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(len(train_val_dataset) * (1 - val_split))
    val_size = len(train_val_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        train_val_dataset, [train_size, val_size], 
        generator=torch.Generator().manual_seed(42)
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers, 
        pin_memory=True if torch.cuda.is_available() else False,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
    
    return train_loader, val_loader, len(train_dataset), len(val_dataset)

class CloudGaitDataset(Dataset):
    """äº‘ç¯å¢ƒæ•°æ®é›†ç±»"""
    
    def __init__(self, image_paths, class_ids, transform=None):
        self.image_paths = image_paths
        self.class_ids = class_ids
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        label = self.class_ids[idx]
        
        # ç¡®ä¿æ ‡ç­¾åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if label < 0 or label > 30:
            label = max(0, min(30, label))
        
        return image, label

def build_cloud_dataloader(root_dir, batch_size=8, num_workers=0, val_split=0.3):
    """å†…å­˜ä¼˜åŒ–çš„äº‘ç¯å¢ƒæ•°æ®åŠ è½½å™¨"""
    print(f"ğŸ” æ‰«æäº‘æ•°æ®é›†ç›®å½•: {root_dir}")
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    all_image_paths = []
    all_class_ids = []
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    if not os.path.exists(root_dir):
        raise ValueError(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {root_dir}")
    
    # æ”¯æŒID_1, ID_2, ... ID_31æ–‡ä»¶å¤¹ç»“æ„
    id_folders = []
    for item in os.listdir(root_dir):
        item_path = os.path.join(root_dir, item)
        if os.path.isdir(item_path) and item.startswith('ID_'):
            id_folders.append(item)
    
    if id_folders:
        print(f"ğŸ“ å‘ç°IDæ–‡ä»¶å¤¹ç»“æ„: {len(id_folders)} ä¸ªæ–‡ä»¶å¤¹")
        
        for folder_name in sorted(id_folders):
            folder_path = os.path.join(root_dir, folder_name)
            
            try:
                # æå–ç±»åˆ«ID
                class_id = int(folder_name.split('_')[1]) - 1  # è½¬æ¢ä¸º0-based
                
                if not (0 <= class_id <= 30):
                    print(f"âš ï¸  è·³è¿‡è¶…å‡ºèŒƒå›´çš„ç±»åˆ«: {folder_name}")
                    continue
                
                # æ”¶é›†å›¾ç‰‡
                folder_images = []
                for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                    folder_images.extend(glob.glob(os.path.join(folder_path, ext)))
                
                print(f"  {folder_name}: {len(folder_images)} å¼ å›¾ç‰‡")
                
                all_image_paths.extend(folder_images)
                all_class_ids.extend([class_id] * len(folder_images))
                
            except (ValueError, IndexError) as e:
                print(f"âš ï¸  æ— æ³•è§£ææ–‡ä»¶å¤¹å {folder_name}: {e}")
                continue

    if not all_image_paths:
        raise ValueError(f"âŒ åœ¨ {root_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")

    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(all_image_paths)}")
    print(f"  ç±»åˆ«æ•°: {len(set(all_class_ids))}")

    # æ•°æ®é›†åˆ’åˆ†
    train_paths, val_paths, train_ids, val_ids = train_test_split(
        all_image_paths, all_class_ids, 
        test_size=val_split, 
        random_state=42,
        stratify=all_class_ids if len(set(all_class_ids)) > 1 else None
    )

    print(f"  è®­ç»ƒé›†: {len(train_paths)} å¼ ")
    print(f"  éªŒè¯é›†: {len(val_paths)} å¼ ")

    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    train_dataset = CloudGaitDataset(train_paths, train_ids, transform=transform)
    val_dataset = CloudGaitDataset(val_paths, val_ids, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=num_workers, pin_memory=True if torch.cuda.is_available() else False)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, 
                           num_workers=num_workers, pin_memory=True if torch.cuda.is_available() else False)
    
    return train_loader, val_loader, len(train_dataset), len(val_dataset)

class CloudVAEFineTuner:
    """äº‘ç¯å¢ƒVAE Fine-tuningå™¨"""
    
    def __init__(self, data_dir='./data'):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ äº‘VAE Fine-tuning è®¾å¤‡: {self.device}")
        
        # å†…å­˜ä¼˜åŒ–é…ç½®
        self.config = {
            'batch_size': 8,  # å‡å°batch sizeä»¥èŠ‚çœæ˜¾å­˜
            'gradient_accumulation_steps': 2,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ä¿æŒæœ‰æ•ˆbatch size
            'learning_rate': 1e-5,  # è¾ƒå°å­¦ä¹ ç‡ä¿æŠ¤é¢„è®­ç»ƒæƒé‡
            'weight_decay': 0.01,
            'max_epochs': 25,  # å®Œæ•´è®­ç»ƒ
            'data_dir': data_dir,
            'save_dir': './vae_finetuned',
            'reconstruction_weight': 1.0,
            'kl_weight': 1e-5,  # å¤§å¹…é™ä½KLæƒé‡ï¼Œæ›´å…³æ³¨é‡å»ºè´¨é‡
            'perceptual_weight': 0.3,  # æ¢å¤åˆç†çš„æ„ŸçŸ¥æŸå¤±æƒé‡
            'use_perceptual_loss': True,  # ä¿æŒæ„ŸçŸ¥æŸå¤±å¼€å¯
            'save_every_epochs': 5,  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡
            'eval_every_epochs': 2,  # æ¯2ä¸ªepochè¯„ä¼°ä¸€æ¬¡
            'use_gradient_checkpointing': True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            'mixed_precision': MIXED_PRECISION_AVAILABLE,  # æ··åˆç²¾åº¦è®­ç»ƒ
            'safe_mixed_precision': True,  # å®‰å…¨æ¨¡å¼ï¼šé‡åˆ°é—®é¢˜æ—¶è‡ªåŠ¨ç¦ç”¨æ··åˆç²¾åº¦
        }
        self.best_model_checkpoint_path = None # ç”¨äºè·Ÿè¸ªè¦åˆ é™¤çš„æ—§æœ€ä½³æ¨¡å‹
        
        os.makedirs(self.config['save_dir'], exist_ok=True)
        
        # 1. åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°‘num_workers
        print("ğŸ“ åŠ è½½Oxford-IIIT Petæ•°æ®é›†...")
        self.train_loader, self.val_loader, train_size, val_size = build_pet_dataloader(
            root_dir=self.config['data_dir'],
            batch_size=self.config['batch_size'],
            num_workers=0,  # å‡å°‘workersä»¥èŠ‚çœå†…å­˜
            val_split=0.2
        )
        print(f"   è®­ç»ƒé›†: {len(self.train_loader)} batches ({train_size} samples)")
        print(f"   éªŒè¯é›†: {len(self.val_loader)} batches ({val_size} samples)")
        print(f"   æœ‰æ•ˆbatch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
        
        # 2. åŠ è½½é¢„è®­ç»ƒVAE
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL...")
        self.vae = AutoencoderKL.from_pretrained(
            "runwayml/stable-diffusion-v1-5", 
            subfolder="vae"
        ).to(self.device).float()
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config['use_gradient_checkpointing']:
            if hasattr(self.vae, 'enable_gradient_checkpointing'):
                self.vae.enable_gradient_checkpointing()
                print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        # è§£å†»VAEå‚æ•°ç”¨äºfine-tuning
        for param in self.vae.parameters():
            param.requires_grad = True
        
        print(f"ğŸ”“ VAEå‚æ•°å·²è§£å†»ç”¨äºfine-tuning")
        
        # 3. é…ç½®ä¼˜åŒ–å™¨
        self.setup_optimizer()
        
        # 4. æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.setup_perceptual_loss()
        
        # 5. æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config['mixed_precision']:
            if GRADSCALER_DEVICE:
                self.scaler = GradScaler(GRADSCALER_DEVICE)
            else:
                self.scaler = GradScaler()
            print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'recon_loss': [],
            'kl_loss': [],
            'perceptual_loss': [],
            'val_mse': []
        }
        
        # æ¸…ç†åˆå§‹åŒ–åçš„å†…å­˜
        self.clear_memory()
        
        print("âœ… äº‘VAE Fine-tuningå™¨åˆå§‹åŒ–å®Œæˆ!")
    
    def setup_optimizer(self):
        """è®¾ç½®åˆ†å±‚ä¼˜åŒ–å™¨"""
        # åªfine-tune decoder + encoderçš„æœ€åå‡ å±‚
        finetune_params = []
        
        # Decoder - å…¨éƒ¨fine-tune
        finetune_params.extend(self.vae.decoder.parameters())
        
        # Encoder - åªfine-tuneæœ€å2ä¸ªblock
        encoder_blocks = list(self.vae.encoder.down_blocks)
        for block in encoder_blocks[-2:]:  # æœ€å2ä¸ªblock
            finetune_params.extend(block.parameters())
        
        # Post quant conv
        finetune_params.extend(self.vae.quant_conv.parameters())
        finetune_params.extend(self.vae.post_quant_conv.parameters())
        
        total_params = sum(p.numel() for p in finetune_params)
        print(f"ğŸ¯ Fine-tuneå‚æ•°æ•°é‡: {total_params:,}")
        
        self.optimizer = torch.optim.AdamW(
            finetune_params,
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.config['max_epochs']
        )
    
    def setup_perceptual_loss(self):
        """è®¾ç½®å®Œæ•´æ„ŸçŸ¥æŸå¤±"""
        if not self.config['use_perceptual_loss']:
            print("ğŸš« æ„ŸçŸ¥æŸå¤±å·²å…³é—­ä»¥èŠ‚çœå†…å­˜")
            self.perceptual_net = None
            return
            
        try:
            from torchvision.models import vgg16, VGG16_Weights
            self.perceptual_net = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(self.device).float()
            for param in self.perceptual_net.parameters():
                param.requires_grad = False
            self.perceptual_net.eval()
            print("âœ… å®Œæ•´VGG16æ„ŸçŸ¥æŸå¤±å·²è®¾ç½® (FP32)")
        except:
            print("âš ï¸  VGG16ä¸å¯ç”¨ï¼Œè·³è¿‡æ„ŸçŸ¥æŸå¤±")
            self.perceptual_net = None
    
    def compute_perceptual_loss(self, real, fake):
        """è®¡ç®—å®Œæ•´æ„ŸçŸ¥æŸå¤±"""
        if self.perceptual_net is None:
            return torch.tensor(0.0, device=self.device, dtype=torch.float32)
        
        # Ensure network and inputs are float32
        self.perceptual_net.float()
        real = real.float()
        fake = fake.float()
        
        # ç¡®ä¿è¾“å…¥æ˜¯3é€šé“
        if real.size(1) == 1:
            real = real.repeat(1, 3, 1, 1)
        if fake.size(1) == 1:
            fake = fake.repeat(1, 3, 1, 1)
        
        # è®¡ç®—VGGç‰¹å¾ï¼Œå®Œå…¨é¿å…autocastçš„å½±å“
        with torch.no_grad():
            real_features = self.perceptual_net(real)
        
        # ç¡®ä¿fake_featuresè®¡ç®—ä¹Ÿä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
        fake_features = self.perceptual_net(fake)
        
        # Ensure MSEæŸå¤±è®¡ç®—ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
        return F.mse_loss(real_features.float(), fake_features.float())
    
    def vae_loss(self, images):
        """VAEæŸå¤±å‡½æ•° - VAEæ“ä½œå¼ºåˆ¶FP32ï¼Œå…¶ä½™ä¾èµ–å¤–éƒ¨autocast"""
        # images è¾“å…¥æ—¶ï¼Œåœ¨æ··åˆç²¾åº¦æ¨¡å¼ä¸‹å·²åœ¨ train_epoch ä¸­è¢«è½¬æ¢ä¸º .float(), 
        # å¹¶ä¸”æ­¤å‡½æ•°åœ¨é¡¶å±‚ autocast(dtype=torch.float16) ä¸Šä¸‹æ–‡ä¸­è¢«è°ƒç”¨ã€‚

        try:
            # 1. VAEæ“ä½œï¼šå¼ºåˆ¶åœ¨FP32ä¸‹æ‰§è¡Œï¼Œå±€éƒ¨è¦†ç›–å¤–éƒ¨autocast
            if AUTOCAST_DEVICE: # æ–°ç‰ˆPyTorch
                with autocast(AUTOCAST_DEVICE, enabled=False, dtype=torch.float32):
                    posterior = self.vae.encode(images.float()).latent_dist
                    latents = posterior.sample().float()
                    reconstructed = self.vae.decode(latents).sample.float()
            else: # ä¼ ç»Ÿç‰ˆæœ¬ PyTorch (autocast æŒ‡ torch.cuda.amp.autocast)
                with autocast(enabled=False):
                    posterior = self.vae.encode(images.float()).latent_dist
                    latents = posterior.sample().float()
                    reconstructed = self.vae.decode(latents).sample.float()

            # 2. é‡å»ºæŸå¤±ï¼šè¾“å…¥ç¡®ä¿æ˜¯FP32
            recon_loss = self.mse_loss(reconstructed.float(), images.float()) # images å·²æ˜¯ .float()
            
            # 3. KLæ•£åº¦æŸå¤±ï¼šç»“æœè½¬æ¢ä¸ºFP32
            kl_loss = posterior.kl().mean().float()
            
            # 4. æ„ŸçŸ¥æŸå¤±ï¼šè¾“å…¥ç¡®ä¿æ˜¯FP32ã€‚compute_perceptual_losså†…éƒ¨å·²å¤„ç†å¥½ç½‘ç»œç²¾åº¦ã€‚
            if self.perceptual_net:
                perceptual_loss = self.compute_perceptual_loss(images.float(), reconstructed.float())
            else:
                perceptual_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
            
            # 5. æ€»æŸå¤±ï¼šæ‰€æœ‰ç»„ä»¶éƒ½åº”æ˜¯FP32
            total_loss = (
                self.config['reconstruction_weight'] * recon_loss +
                self.config['kl_weight'] * kl_loss +
                self.config['perceptual_weight'] * perceptual_loss
            )
            
            # 6. è¿”å›å€¼ï¼šç¡®ä¿æ‰€æœ‰è¿”å›çš„å¼ é‡æ˜¯FP32
            return {
                'total_loss': total_loss.float(),
                'recon_loss': recon_loss.float(),
                'kl_loss': kl_loss.float(),
                'perceptual_loss': perceptual_loss.float(),
                'reconstructed': reconstructed.float(),
                'latents': latents.float()
            }
            
        except Exception as e:
            error_str = str(e).lower()
            if "dtype" in error_str or "type" in error_str or "expected scalar type" in error_str:
                print(f"âš ï¸ VAEæŸå¤±è®¡ç®—ä¸­çš„æ•°æ®ç±»å‹é”™è¯¯: {e}")
                return {
                    'total_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32, requires_grad=True),
                    'recon_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32),
                    'kl_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32),
                    'perceptual_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32),
                    'reconstructed': images.float(),
                    'latents': torch.zeros_like(images, dtype=torch.float32)
                }
            else:
                print(f"ğŸ”¥ VAE_LOSSå†…éƒ¨å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œç±»å‹: {type(e)}, å†…å®¹: {e} ğŸ”¥")
                import traceback
                traceback.print_exc()
                raise e
    
    def clear_memory(self):
        """å¢å¼ºå†…å­˜æ¸…ç†"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
    
    def train_epoch(self, epoch: int):
        """å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒepoch"""
        self.vae.train()
        
        total_loss = 0
        total_recon = 0
        total_kl = 0
        total_perceptual = 0
        num_batches = 0
        
        # é‡ç½®æ¢¯åº¦ç´¯ç§¯
        self.optimizer.zero_grad()
        
        pbar = tqdm(self.train_loader, desc=f"Fine-tune Epoch {epoch+1}")
        
        for batch_idx, batch in enumerate(pbar):
            images, _ = batch  # å¿½ç•¥æ ‡ç­¾
            images = images.to(self.device, non_blocking=True)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.config['mixed_precision']:
                if AUTOCAST_DEVICE:
                    with autocast(AUTOCAST_DEVICE, dtype=torch.float16):
                        loss_dict = self.vae_loss(images)
                        loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                else:
                    with autocast(): # Old API for mixed precision context
                        loss_dict = self.vae_loss(images)
                        loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                
                # ç¡®ä¿æŸå¤±æ˜¯ float32 ç±»å‹å†è¿›è¡Œç¼©æ”¾å’Œåå‘ä¼ æ’­
                self.scaler.scale(loss.float()).backward()
            else:
                # å‰å‘ä¼ æ’­ (æ— æ··åˆç²¾åº¦)
                loss_dict = self.vae_loss(images)
                loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                loss.backward() # ç›´æ¥åå‘ä¼ æ’­
            
            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:
                try:
                    if self.config['mixed_precision']:
                        # æ¢¯åº¦è£å‰ª
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                        
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                        self.optimizer.step()
                    
                    self.optimizer.zero_grad()
                    
                except RuntimeError as grad_e:
                    if "dtype" in str(grad_e).lower() or "type" in str(grad_e).lower():
                        print(f"âš ï¸ æ¢¯åº¦æ­¥éª¤æ•°æ®ç±»å‹é”™è¯¯ï¼Œè·³è¿‡æ­¤æ­¥éª¤: {grad_e}")
                        self.optimizer.zero_grad()
                        continue
                    else:
                        raise grad_e
            
            # è®°å½•æŸå¤±ï¼ˆæ¢å¤åˆ°åŸå§‹scaleï¼‰
            actual_loss = loss.item() * self.config['gradient_accumulation_steps']
            total_loss += actual_loss
            total_recon += loss_dict['recon_loss'].item()
            total_kl += loss_dict['kl_loss'].item()
            total_perceptual += loss_dict['perceptual_loss'].item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{actual_loss:.4f}',
                'Recon': f'{loss_dict["recon_loss"].item():.4f}',
                'KL': f'{loss_dict["kl_loss"].item():.4f}',
                'Perc': f'{loss_dict["perceptual_loss"].item():.4f}'
            })
            
            # ä¿å­˜ç¬¬ä¸€ä¸ªbatchçš„é‡å»ºæ ·æœ¬
            if batch_idx == 0:
                self.save_reconstruction_samples(
                    images, loss_dict['reconstructed'], epoch
                )
            
            # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†
            if batch_idx % 20 == 0:  # æ¯20ä¸ªbatchæ¸…ç†ä¸€æ¬¡
                del images, loss_dict, loss
                self.clear_memory()
        
        # å¤„ç†æœ€åä¸å®Œæ•´çš„æ¢¯åº¦ç´¯ç§¯
        if (len(self.train_loader)) % self.config['gradient_accumulation_steps'] != 0:
            if self.config['mixed_precision']:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                self.optimizer.step()
            self.optimizer.zero_grad()
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()
        
        avg_metrics = {
            'avg_loss': total_loss / num_batches,
            'avg_recon': total_recon / num_batches,
            'avg_kl': total_kl / num_batches,
            'avg_perceptual': total_perceptual / num_batches
        }
        
        self.clear_memory()
        return avg_metrics
    
    def save_reconstruction_samples(self, original, reconstructed, epoch):
        """ä¿å­˜é‡å»ºæ ·æœ¬å¯¹æ¯”"""
        with torch.no_grad():
            # åå½’ä¸€åŒ–
            def denormalize(tensor):
                return torch.clamp((tensor + 1) / 2, 0, 1)
            
            batch_size = original.size(0)  # è·å–å®é™…çš„batchå¤§å°
            
            # ç¡®ä¿ä¸è¶…è¿‡batch sizeçš„èŒƒå›´
            orig = denormalize(original[:batch_size]).cpu()
            recon = denormalize(reconstructed[:batch_size]).cpu()
            
            # åŠ¨æ€è°ƒæ•´å›¾è¡¨å¤§å°
            fig, axes = plt.subplots(2, batch_size, figsize=(batch_size * 3, 6))
            
            # å¤„ç†å•æ ·æœ¬æƒ…å†µï¼ˆbatch_size=1ï¼‰
            if batch_size == 1:
                # åŸå›¾
                axes[0].imshow(orig[0].permute(1, 2, 0))
                axes[0].set_title(f'Original')
                axes[0].axis('off')
                
                # é‡å»ºå›¾
                axes[1].imshow(recon[0].permute(1, 2, 0))
                axes[1].set_title(f'Reconstructed')
                axes[1].axis('off')
            else:
                # å¤šæ ·æœ¬æƒ…å†µ
                for i in range(batch_size):
                    # åŸå›¾
                    axes[0, i].imshow(orig[i].permute(1, 2, 0))
                    axes[0, i].set_title(f'Original {i+1}')
                    axes[0, i].axis('off')
                    
                    # é‡å»ºå›¾
                    axes[1, i].imshow(recon[i].permute(1, 2, 0))
                    axes[1, i].set_title(f'Reconstructed {i+1}')
                    axes[1, i].axis('off')
            
            plt.suptitle(f'VAE Reconstruction - Epoch {epoch+1}')
            plt.tight_layout()
            plt.savefig(f'{self.config["save_dir"]}/reconstruction_epoch_{epoch+1}.png',
                       dpi=150, bbox_inches='tight')
            plt.close()
    
    @torch.no_grad()
    def evaluate_reconstruction_quality(self, val_loader):
        """å†…å­˜ä¼˜åŒ–çš„è¯„ä¼°å‡½æ•°"""
        self.vae.eval()
        
        total_mse = 0
        total_samples = 0
        latent_stats = []
        
        # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡
        max_eval_batches = min(20, len(val_loader))
        
        for batch_idx, batch in enumerate(val_loader):
            if batch_idx >= max_eval_batches:
                break
                
            images, _ = batch
            images = images.to(self.device, non_blocking=True)
            
            # VAEé‡å»º
            if self.config['mixed_precision']:
                if AUTOCAST_DEVICE:
                    # æ–°ç‰ˆPyTorch
                    with autocast(AUTOCAST_DEVICE, dtype=torch.float16):
                        posterior = self.vae.encode(images).latent_dist
                        latents = posterior.sample()
                        reconstructed = self.vae.decode(latents).sample
                else:
                    # ä¼ ç»Ÿç‰ˆæœ¬
                    with autocast():
                        posterior = self.vae.encode(images).latent_dist
                        latents = posterior.sample()
                        reconstructed = self.vae.decode(latents).sample
            else:
                posterior = self.vae.encode(images).latent_dist
                latents = posterior.sample()
                reconstructed = self.vae.decode(latents).sample
            
            # è®¡ç®—MSE
            mse = F.mse_loss(reconstructed, images, reduction='sum')
            total_mse += mse.item()
            total_samples += images.size(0)
            
            # æ”¶é›†æ½œåœ¨è¡¨ç¤ºç»Ÿè®¡
            latent_stats.append({
                'mean': latents.mean().item(),
                'std': latents.std().item(),
                'min': latents.min().item(),
                'max': latents.max().item()
            })
            
            # æ¸…ç†å†…å­˜
            del images, posterior, latents, reconstructed, mse
            
            if batch_idx % 5 == 0:
                self.clear_memory()
        
        avg_mse = total_mse / total_samples
        
        # è®¡ç®—å¹³å‡æ½œåœ¨ç©ºé—´ç»Ÿè®¡
        avg_latent_stats = {
            'mean': np.mean([s['mean'] for s in latent_stats]),
            'std': np.mean([s['std'] for s in latent_stats]),
            'min': np.min([s['min'] for s in latent_stats]),
            'max': np.max([s['max'] for s in latent_stats])
        }
        
        print(f"ğŸ“Š é‡å»ºè´¨é‡MSE: {avg_mse:.6f}")
        print(f"ğŸ§  æ½œåœ¨ç©ºé—´ç»Ÿè®¡: å‡å€¼={avg_latent_stats['mean']:.4f}, "
              f"æ ‡å‡†å·®={avg_latent_stats['std']:.4f}, "
              f"èŒƒå›´=[{avg_latent_stats['min']:.2f}, {avg_latent_stats['max']:.2f}]")
        
        return avg_mse, avg_latent_stats
    
    def save_finetuned_vae(self, epoch, metrics):
        """ä¿å­˜fine-tuned VAE"""
        save_path = f'{self.config["save_dir"]}/vae_finetuned_epoch_{epoch+1}.pth'
        
        torch.save({
            'vae_state_dict': self.vae.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epoch': epoch,
            'metrics': metrics,
            'config': self.config,
            'train_history': self.train_history
        }, save_path)
        
        print(f"ğŸ’¾ Fine-tuned VAEä¿å­˜: {save_path}")
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if len(self.train_history['epoch']) == 0:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ€»æŸå¤±
        axes[0, 0].plot(self.train_history['epoch'], self.train_history['train_loss'])
        axes[0, 0].set_title('Training Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].grid(True)
        
        # é‡å»ºæŸå¤±
        axes[0, 1].plot(self.train_history['epoch'], self.train_history['recon_loss'])
        axes[0, 1].set_title('Reconstruction Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('MSE Loss')
        axes[0, 1].grid(True)
        
        # KLæŸå¤±
        axes[1, 0].plot(self.train_history['epoch'], self.train_history['kl_loss'])
        axes[1, 0].set_title('KL Divergence Loss')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('KL Loss')
        axes[1, 0].grid(True)
        
        # éªŒè¯MSE
        if self.train_history['val_mse']:
            axes[1, 1].plot(self.train_history['epoch'][::self.config['eval_every_epochs']], 
                           self.train_history['val_mse'])
            axes[1, 1].set_title('Validation MSE')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('MSE')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{self.config["save_dir"]}/training_history.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def finetune(self):
        """ä¸»è¦fine-tuningæµç¨‹"""
        print("ğŸš€ å¼€å§‹äº‘VAE Fine-tuningå®Œæ•´è®­ç»ƒ...")
        print("=" * 60)
        
        if torch.cuda.is_available(): # ç¡®ä¿åªåœ¨CUDAå¯ç”¨æ—¶è®¾ç½®
            torch.autograd.set_detect_anomaly(True)
            print("âš ï¸ PyTorch å¼‚å¸¸æ£€æµ‹å·²å¯ç”¨ (ç”¨äºè°ƒè¯•ï¼Œå¯èƒ½å½±å“é€Ÿåº¦)")
        
        best_recon_loss = float('inf')
        
        for epoch in range(self.config['max_epochs']):
            print(f"\nğŸ“… Epoch {epoch+1}/{self.config['max_epochs']}")
            
            train_metrics = self.train_epoch(epoch)
            
            self.train_history['epoch'].append(epoch + 1)
            self.train_history['train_loss'].append(train_metrics['avg_loss'])
            self.train_history['recon_loss'].append(train_metrics['avg_recon'])
            self.train_history['kl_loss'].append(train_metrics['avg_kl'])
            self.train_history['perceptual_loss'].append(train_metrics['avg_perceptual'])
            
            if epoch % self.config['eval_every_epochs'] == 0 or epoch == self.config['max_epochs'] - 1:
                val_mse, latent_stats = self.evaluate_reconstruction_quality(self.val_loader)
                self.train_history['val_mse'].append(val_mse)
            
            print(f"ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"   æ€»æŸå¤±: {train_metrics['avg_loss']:.4f}")
            print(f"   é‡å»ºæŸå¤±: {train_metrics['avg_recon']:.4f}")
            print(f"   KLæŸå¤±: {train_metrics['avg_kl']:.4f}")
            print(f"   æ„ŸçŸ¥æŸå¤±: {train_metrics['avg_perceptual']:.4f}")
            print(f"   å­¦ä¹ ç‡: {self.scheduler.get_last_lr()[0]:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹é€»è¾‘ä¿®æ”¹
            current_epoch_model_path = f'{self.config["save_dir"]}/vae_finetuned_epoch_{epoch+1}.pth'
            is_best_model_save = False
            if train_metrics['avg_recon'] < best_recon_loss:
                best_recon_loss = train_metrics['avg_recon']
                
                # å°è¯•åˆ é™¤ä¸Šä¸€ä¸ªè¢«æ ‡è®°ä¸º"æœ€ä½³"çš„æ¨¡å‹æ–‡ä»¶
                if self.best_model_checkpoint_path and os.path.exists(self.best_model_checkpoint_path):
                    # æ£€æŸ¥è¿™ä¸ªæ—§çš„æœ€ä½³æ¨¡å‹æ˜¯å¦ä¹Ÿæ˜¯ä¸€ä¸ªå®šæœŸä¿å­˜ç‚¹
                    try:
                        # ä»æ–‡ä»¶åæå–æ—§çš„epochå·
                        old_epoch_str = self.best_model_checkpoint_path.split('_epoch_')[-1].split('.pth')[0]
                        old_epoch_idx = int(old_epoch_str) - 1 # epochå·è½¬ä¸º0-indexed
                        is_periodic_save = (old_epoch_idx % self.config['save_every_epochs'] == 0)
                        
                        if not is_periodic_save:
                            print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹: {self.best_model_checkpoint_path}")
                            os.remove(self.best_model_checkpoint_path)
                        else:
                            print(f"â„¹ï¸ ä¿ç•™æ—§çš„æœ€ä½³æ¨¡å‹ (åŒæ—¶ä¹Ÿæ˜¯å®šæœŸä¿å­˜ç‚¹): {self.best_model_checkpoint_path}")
                    except Exception as e_remove:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æˆ–æ£€æŸ¥æ—§çš„æœ€ä½³æ¨¡å‹: {e_remove}")

                self.save_finetuned_vae(epoch, train_metrics) # ä¿å­˜å½“å‰æ¨¡å‹
                self.best_model_checkpoint_path = current_epoch_model_path # æ›´æ–°æœ€ä½³æ¨¡å‹è·¯å¾„
                print(f"âœ… ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (é‡å»ºæŸå¤±: {best_recon_loss:.4f}): {current_epoch_model_path}")
                is_best_model_save = True
            
            if epoch % self.config['save_every_epochs'] == 0:
                if not is_best_model_save: # å¦‚æœæ­¤epochå·²ä½œä¸ºæœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œåˆ™ä¸å†é‡å¤ä¿å­˜
                    self.save_finetuned_vae(epoch, train_metrics)
                self.plot_training_history()
            
            print(f"ğŸ† å½“å‰æœ€ä½³é‡å»ºæŸå¤±: {best_recon_loss:.4f}")
            self.clear_memory()
        
        # è®­ç»ƒå®Œæˆ
        print("\n" + "=" * 60)
        print("ğŸ‰ äº‘VAE Fine-tuning å®Œæ•´è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³é‡å»ºæŸå¤±: {best_recon_loss:.4f}")
        
        # æœ€ç»ˆè¯„ä¼°
        final_mse, final_latent_stats = self.evaluate_reconstruction_quality(self.val_loader)
        print(f"ğŸ¯ æœ€ç»ˆé‡å»ºMSE: {final_mse:.6f}")
        
        # ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒå†å²
        self.plot_training_history()
        
        return best_recon_loss

class PureFP32CloudVAEFineTuner(CloudVAEFineTuner):
    """çº¯FP32ç²¾åº¦è®­ç»ƒå™¨ï¼Œå®Œå…¨é¿å…æ··åˆç²¾åº¦é—®é¢˜"""
    
    def __init__(self, data_dir):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ çº¯FP32æ¨¡å¼ - äº‘VAE Fine-tuning è®¾å¤‡: {self.device}")
        
        # çº¯FP32æ¨¡å¼é…ç½®
        self.config = {
            'batch_size': 6,  # ç¨å¾®å‡å°batch size
            'gradient_accumulation_steps': 3,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
            'learning_rate': 1e-5,
            'weight_decay': 0.01,
            'max_epochs': 20,
            'data_dir': data_dir,
            'save_dir': './vae_finetuned',
            'reconstruction_weight': 1.0,
            'kl_weight': 1e-5,
            'perceptual_weight': 0.3,
            'use_perceptual_loss': True,
            'save_every_epochs': 5,
            'eval_every_epochs': 2,
            'use_gradient_checkpointing': True,
            'mixed_precision': False,  # å¼ºåˆ¶ç¦ç”¨æ··åˆç²¾åº¦
            'safe_mixed_precision': False,
        }
        self.best_model_checkpoint_path = None
        
        os.makedirs(self.config['save_dir'], exist_ok=True)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("ğŸ“ åŠ è½½Oxford-IIIT Petæ•°æ®é›† (çº¯FP32æ¨¡å¼)...")
        self.train_loader, self.val_loader, train_size, val_size = build_pet_dataloader(
            root_dir=self.config['data_dir'],
            batch_size=self.config['batch_size'],
            num_workers=0,
            val_split=0.2
        )
        print(f"   è®­ç»ƒé›†: {len(self.train_loader)} batches ({train_size} samples)")
        print(f"   éªŒè¯é›†: {len(self.val_loader)} batches ({val_size} samples)")
        
        # åŠ è½½é¢„è®­ç»ƒVAE (çº¯FP32æ¨¡å¼)
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL (çº¯FP32æ¨¡å¼)...")
        self.vae = AutoencoderKL.from_pretrained(
            "runwayml/stable-diffusion-v1-5", 
            subfolder="vae"
        ).to(self.device).float()
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if hasattr(self.vae, 'enable_gradient_checkpointing'):
            self.vae.enable_gradient_checkpointing()
            print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        for param in self.vae.parameters():
            param.requires_grad = True
        
        # é…ç½®ä¼˜åŒ–å™¨
        self.setup_optimizer()
        self.mse_loss = nn.MSELoss()
        self.setup_perceptual_loss()
        
        # è®°å½•è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'recon_loss': [],
            'kl_loss': [],
            'perceptual_loss': [],
            'val_mse': []
        }
        
        self.clear_memory()
        print("âœ… çº¯FP32æ¨¡å¼Fine-tuningå™¨åˆå§‹åŒ–å®Œæˆ!")
    
    # è¦†ç›–vae_losså‡½æ•°ï¼Œç¡®ä¿å®Œå…¨ä½¿ç”¨FP32
    def vae_loss(self, images):
        """çº¯FP32 VAEæŸå¤±å‡½æ•°"""
        # ç¡®ä¿è¾“å…¥ä¸ºfloat32
        images = images.float()
        
        # å‰å‘ä¼ æ’­ (çº¯FP32)
        posterior = self.vae.encode(images).latent_dist
        latents = posterior.sample().float()
        reconstructed = self.vae.decode(latents).sample.float()
        
        # æŸå¤±è®¡ç®— (çº¯FP32)
        recon_loss = self.mse_loss(reconstructed, images)
        
        # æ›´ç¨³å®šçš„KLæ•£åº¦è®¡ç®—ï¼Œé˜²æ­¢NaN
        try:
            # åŸå§‹KLæ•£åº¦è®¡ç®—å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
            kl_raw = posterior.kl()
            
            # æ£€æŸ¥å¹¶å¤„ç†NaNæˆ–æ— ç©·å¤§å€¼
            if torch.isnan(kl_raw).any() or torch.isinf(kl_raw).any():
                print("âš ï¸ æ£€æµ‹åˆ°KLæ•£åº¦ä¸­çš„NaN/Infå€¼ï¼Œä½¿ç”¨æ›¿ä»£è®¡ç®—æ–¹æ³•")
                # æ›¿ä»£è®¡ç®—æ–¹æ³• - åŸºäºå‡å€¼å’Œæ–¹å·®çš„ç›´æ¥è®¡ç®—
                mean, var = posterior.mean, posterior.var
                # æ·»åŠ å°çš„epsiloné˜²æ­¢log(0)
                eps = 1e-8
                # æ ‡å‡†KLæ•£åº¦å…¬å¼: KL = 0.5 * (Î¼Â² + ÏƒÂ² - log(ÏƒÂ²) - 1)
                kl_loss = 0.5 * torch.mean(mean.pow(2) + var - torch.log(var + eps) - 1)
            else:
                kl_loss = kl_raw.mean()
                
            # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
            if torch.isnan(kl_loss) or torch.isinf(kl_loss):
                print("âš ï¸ KLæŸå¤±ä»ç„¶åŒ…å«NaN/Infï¼Œä½¿ç”¨å¸¸æ•°æ›¿ä»£")
                kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
        except Exception as kl_e:
            print(f"âš ï¸ KLæ•£åº¦è®¡ç®—é”™è¯¯: {kl_e}ï¼Œä½¿ç”¨æ›¿ä»£å€¼")
            kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
        
        # ä¸ä½¿ç”¨æ„ŸçŸ¥æŸå¤±
        perceptual_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
        
        # è®¡ç®—æ€»æŸå¤±
        total_loss = (
            self.config['reconstruction_weight'] * recon_loss +
            self.config['kl_weight'] * kl_loss
        )
        
        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("âš ï¸ æ€»æŸå¤±åŒ…å«NaN/Infï¼Œä»…ä½¿ç”¨é‡å»ºæŸå¤±")
            total_loss = self.config['reconstruction_weight'] * recon_loss
        
        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss,
            'perceptual_loss': perceptual_loss,
            'reconstructed': reconstructed,
            'latents': latents
        }

def create_ultra_low_memory_finetuner(data_dir, images_dir=None, annotations_dir=None, use_cpu=False):
    """åˆ›å»ºè¶…ä½å†…å­˜é…ç½®çš„Fine-tunerï¼Œé€‚é…Kaggleç¯å¢ƒ"""
    print("ğŸ†˜ å¯åŠ¨è¶…ä½å†…å­˜æ¨¡å¼..." + ("(CPUè®­ç»ƒ)" if use_cpu else ""))
    
    # å¦‚æœæ²¡æœ‰æä¾›images_dirå’Œannotations_dirï¼Œåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    if images_dir is None:
        # æœ¬åœ°ç¯å¢ƒä½¿ç”¨data_dirå†…éƒ¨çš„imagesç›®å½•
        images_dir = os.path.join(data_dir, "images")
    if annotations_dir is None:
        # æœ¬åœ°ç¯å¢ƒä½¿ç”¨data_dirå†…éƒ¨çš„annotationsç›®å½•
        annotations_dir = os.path.join(data_dir, "annotations")
    
    class UltraLowMemoryDataset(Dataset):
        """è¶…ä½å†…å­˜æ•°æ®é›†ç±»ï¼Œé€‚é…Kaggleç¯å¢ƒ"""
        
        def __init__(self, images_dir, annotation_file, transform=None):
            self.images_dir = images_dir
            self.transform = transform
            
            # åŠ è½½æ ‡æ³¨æ–‡ä»¶
            self.samples = []
            with open(annotation_file, 'r') as f:
                for line in f:
                    if line.startswith('#'):
                        continue  # è·³è¿‡æ³¨é‡Šè¡Œ
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        image_id = parts[0]  # å›¾åƒID
                        class_id = int(parts[1]) - 1  # ç±»åˆ«ID (è½¬ä¸º0-indexed)
                        self.samples.append((image_id, class_id))
            
            print(f"åŠ è½½äº† {len(self.samples)} ä¸ªæ ·æœ¬")
        
        def __len__(self):
            return len(self.samples)
        
        def __getitem__(self, idx):
            image_id, class_id = self.samples[idx]
            img_path = os.path.join(self.images_dir, f"{image_id}.jpg")
            
            # æœ‰äº›å›¾åƒå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ‰©å±•å
            if not os.path.exists(img_path):
                for ext in ['.jpeg', '.png', '.JPEG', '.PNG']:
                    alt_path = os.path.join(self.images_dir, f"{image_id}{ext}")
                    if os.path.exists(alt_path):
                        img_path = alt_path
                        break
            
            try:
                image = Image.open(img_path).convert('RGB')
                if self.transform:
                    image = self.transform(image)
                return image, class_id
            except Exception as e:
                print(f"é”™è¯¯åŠ è½½å›¾åƒ {img_path}: {e}")
                # è¿”å›ä¸€ä¸ªå ä½å›¾åƒå’Œç±»åˆ«
                placeholder = torch.zeros((3, 128, 128))
                return placeholder, class_id
    
    class UltraLowMemoryFineTuner(CloudVAEFineTuner):
        def __init__(self, data_dir, images_dir, annotations_dir, use_cpu=False):
            self.device = 'cpu' if use_cpu else ('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ğŸš€ è¶…ä½å†…å­˜æ¨¡å¼ - VAE Fine-tuning è®¾å¤‡: {self.device}")
            
            # è¶…ä½å†…å­˜é…ç½®
            self.config = {
                'batch_size': 4 if self.device == 'cuda' else 2,  # GPUä½¿ç”¨æ›´å¤§batchï¼ŒCPUæ›´å°
                'gradient_accumulation_steps': 4,
                'learning_rate': 1e-5,
                'weight_decay': 0.01,
                'max_epochs': 15 if self.device == 'cuda' else 5,  # CPUæ¨¡å¼å‡å°‘è®­ç»ƒè½®æ¬¡
                'data_dir': data_dir,
                'images_dir': images_dir,
                'annotations_dir': annotations_dir,
                'save_dir': '/kaggle/working/vae_finetuned' if os.path.exists('/kaggle') else './vae_finetuned',
                'reconstruction_weight': 1.0,
                'kl_weight': 0.05,
                'perceptual_weight': 0.0,  # ç¦ç”¨æ„ŸçŸ¥æŸå¤±ä»¥èŠ‚çœå†…å­˜
                'use_perceptual_loss': False,
                'save_every_epochs': 5,
                'eval_every_epochs': 5,
                'use_gradient_checkpointing': self.device == 'cuda',  # CPUæ¨¡å¼ä¸å¯ç”¨
                'mixed_precision': False if self.device == 'cpu' else MIXED_PRECISION_AVAILABLE,
                'image_size': 128,  # é™ä½å›¾åƒå°ºå¯¸
            }
            
            os.makedirs(self.config['save_dir'], exist_ok=True)
            
            # é€‚åº”æ›´å°å›¾åƒå°ºå¯¸çš„æ•°æ®è½¬æ¢
            transform = transforms.Compose([
                transforms.Resize((self.config['image_size'], self.config['image_size'])),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
            # åŠ è½½è®­ç»ƒå’ŒéªŒè¯é›†
            print("ğŸ“ åŠ è½½è¶…ä½å†…å­˜æ¨¡å¼æ•°æ®é›†...")
            train_annotation_file = os.path.join(annotations_dir, "trainval.txt")
            train_val_dataset = UltraLowMemoryDataset(
                images_dir=images_dir,
                annotation_file=train_annotation_file,
                transform=transform
            )
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            train_size = int(len(train_val_dataset) * 0.8)  # ä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†
            val_size = len(train_val_dataset) - train_size
            train_dataset, val_dataset = torch.utils.data.random_split(
                train_val_dataset, [train_size, val_size], 
                generator=torch.Generator().manual_seed(42)
            )
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            self.train_loader = DataLoader(
                train_dataset, 
                batch_size=self.config['batch_size'], 
                shuffle=True, 
                num_workers=0, 
                pin_memory=False,
                drop_last=True
            )
            
            self.val_loader = DataLoader(
                val_dataset, 
                batch_size=self.config['batch_size'], 
                shuffle=False, 
                num_workers=0,
                pin_memory=False
            )
            
            print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
            print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
            print(f"  å›¾åƒå°ºå¯¸: {self.config['image_size']}x{self.config['image_size']}")
            print(f"  æœ‰æ•ˆbatch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
            
            # åŠ è½½VAE
            print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL (è¶…ä½å†…å­˜æ¨¡å¼)...")
            self.vae = AutoencoderKL.from_pretrained(
                "runwayml/stable-diffusion-v1-5", 
                subfolder="vae"
            ).to(self.device).float()
            
            # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if self.device != 'cpu' and hasattr(self.vae, 'enable_gradient_checkpointing'):
                self.vae.enable_gradient_checkpointing()
                print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
            
            for param in self.vae.parameters():
                param.requires_grad = True
            
            self.setup_optimizer()
            self.mse_loss = nn.MSELoss()
            # è¶…ä½å†…å­˜æ¨¡å¼ä¸ä½¿ç”¨æ„ŸçŸ¥æŸå¤±
            self.perceptual_net = None
            
            if self.config['mixed_precision'] and self.device != 'cpu':
                self.scaler = GradScaler()
                print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
                
            self.train_history = {
                'epoch': [], 'train_loss': [], 'recon_loss': [],
                'kl_loss': [], 'perceptual_loss': [], 'val_mse': []
            }
            
            self.clear_memory()
            print("âœ… è¶…ä½å†…å­˜æ¨¡å¼Fine-tuningå™¨åˆå§‹åŒ–å®Œæˆ!")
        
        def setup_perceptual_loss(self):
            """è®¾ç½®æ„ŸçŸ¥æŸå¤±"""
            if not self.config['use_perceptual_loss']:
                print("ğŸš« æ„ŸçŸ¥æŸå¤±å·²ç¦ç”¨")
                self.perceptual_net = None
                return
                
            try:
                from torchvision.models import vgg16, VGG16_Weights
                self.perceptual_net = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(self.device).float()
                for param in self.perceptual_net.parameters():
                    param.requires_grad = False
                self.perceptual_net.eval()
                print("âœ… VGG16æ„ŸçŸ¥æŸå¤±å·²è®¾ç½® (FP32)")
            except:
                print("âš ï¸ VGG16ä¸å¯ç”¨ï¼Œè·³è¿‡æ„ŸçŸ¥æŸå¤±")
                self.perceptual_net = None
                
        def compute_perceptual_loss(self, real, fake):
            """è®¡ç®—æ„ŸçŸ¥æŸå¤±"""
            if self.perceptual_net is None:
                return torch.tensor(0.0, device=self.device, dtype=torch.float32)
            
            # ç¡®ä¿ç½‘ç»œå’Œè¾“å…¥æ˜¯float32
            self.perceptual_net.float()
            real = real.float()
            fake = fake.float()
            
            # ç¡®ä¿è¾“å…¥æ˜¯3é€šé“
            if real.size(1) == 1:
                real = real.repeat(1, 3, 1, 1)
            if fake.size(1) == 1:
                fake = fake.repeat(1, 3, 1, 1)
            
            # è®¡ç®—VGGç‰¹å¾
            with torch.no_grad():
                real_features = self.perceptual_net(real)
            
            fake_features = self.perceptual_net(fake)
            
            # è®¡ç®—MSEæŸå¤±
            return F.mse_loss(real_features.float(), fake_features.float())
            
        def vae_loss(self, images):
            """è¶…ä½å†…å­˜VAEæŸå¤±å‡½æ•°"""
            try:
                # ç¡®ä¿è¾“å…¥ä¸ºfloat32
                images = images.float()
                
                # VAEå‰å‘ä¼ æ’­
                posterior = self.vae.encode(images).latent_dist
                latents = posterior.sample().float()
                reconstructed = self.vae.decode(latents).sample.float()
                
                # é‡å»ºæŸå¤±
                recon_loss = self.mse_loss(reconstructed, images)
                
                # æ›´ç¨³å®šçš„KLæ•£åº¦è®¡ç®—ï¼Œé˜²æ­¢NaN
                try:
                    # åŸå§‹KLæ•£åº¦è®¡ç®—å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
                    kl_raw = posterior.kl()
                    
                    # æ£€æŸ¥å¹¶å¤„ç†NaNæˆ–æ— ç©·å¤§å€¼
                    if torch.isnan(kl_raw).any() or torch.isinf(kl_raw).any():
                        print("âš ï¸ æ£€æµ‹åˆ°KLæ•£åº¦ä¸­çš„NaN/Infå€¼ï¼Œä½¿ç”¨æ›¿ä»£è®¡ç®—æ–¹æ³•")
                        # æ›¿ä»£è®¡ç®—æ–¹æ³•
                        mean, var = posterior.mean, posterior.var
                        eps = 1e-8
                        kl_loss = 0.5 * torch.mean(mean.pow(2) + var - torch.log(var + eps) - 1)
                    else:
                        kl_loss = kl_raw.mean()
                        
                    # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
                    if torch.isnan(kl_loss) or torch.isinf(kl_loss):
                        print("âš ï¸ KLæŸå¤±ä»ç„¶åŒ…å«NaN/Infï¼Œä½¿ç”¨å¸¸æ•°æ›¿ä»£")
                        kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
                except Exception as kl_e:
                    print(f"âš ï¸ KLæ•£åº¦è®¡ç®—é”™è¯¯: {kl_e}ï¼Œä½¿ç”¨æ›¿ä»£å€¼")
                    kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
                
                # ä¸ä½¿ç”¨æ„ŸçŸ¥æŸå¤±
                perceptual_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
                
                # è®¡ç®—æ€»æŸå¤±
                total_loss = (
                    self.config['reconstruction_weight'] * recon_loss +
                    self.config['kl_weight'] * kl_loss
                )
                
                # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
                if torch.isnan(total_loss) or torch.isinf(total_loss):
                    print("âš ï¸ æ€»æŸå¤±åŒ…å«NaN/Infï¼Œä»…ä½¿ç”¨é‡å»ºæŸå¤±")
                    total_loss = self.config['reconstruction_weight'] * recon_loss
                
                return {
                    'total_loss': total_loss,
                    'recon_loss': recon_loss,
                    'kl_loss': kl_loss,
                    'perceptual_loss': perceptual_loss,
                    'reconstructed': reconstructed,
                    'latents': latents
                }
            except Exception as e:
                print(f"ğŸ”¥ è¶…ä½å†…å­˜VAE_LOSSå†…éƒ¨å‘ç”Ÿé”™è¯¯ï¼Œç±»å‹: {type(e)}, å†…å®¹: {e} ğŸ”¥")
                import traceback
                traceback.print_exc()
                raise e
    
    return UltraLowMemoryFineTuner(data_dir, images_dir, annotations_dir, use_cpu=use_cpu)

def create_kaggle_trainer(data_dir, images_dir, annotations_dir):
    """åˆ›å»ºé€‚ç”¨äºKaggle P100 GPUçš„é«˜æ€§èƒ½è®­ç»ƒå™¨"""
    print("ğŸš€ é…ç½®Kaggle P100 GPUè®­ç»ƒå™¨...")
    
    class KaggleOxfordPetDataset(Dataset):
        """é€‚é…Kaggleè·¯å¾„çš„Oxford-IIIT Petæ•°æ®é›†ç±»"""
        
        def __init__(self, images_dir, annotation_file, transform=None):
            self.images_dir = images_dir
            self.transform = transform
            
            # åŠ è½½æ ‡æ³¨æ–‡ä»¶
            self.samples = []
            with open(annotation_file, 'r') as f:
                for line in f:
                    if line.startswith('#'):
                        continue  # è·³è¿‡æ³¨é‡Šè¡Œ
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        image_id = parts[0]  # å›¾åƒID
                        class_id = int(parts[1]) - 1  # ç±»åˆ«ID (è½¬ä¸º0-indexed)
                        self.samples.append((image_id, class_id))
            
            print(f"åŠ è½½äº† {len(self.samples)} ä¸ªæ ·æœ¬")
        
        def __len__(self):
            return len(self.samples)
        
        def __getitem__(self, idx):
            image_id, class_id = self.samples[idx]
            img_path = os.path.join(self.images_dir, f"{image_id}.jpg")
            
            # æœ‰äº›å›¾åƒå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ‰©å±•å
            if not os.path.exists(img_path):
                for ext in ['.jpeg', '.png', '.JPEG', '.PNG']:
                    alt_path = os.path.join(self.images_dir, f"{image_id}{ext}")
                    if os.path.exists(alt_path):
                        img_path = alt_path
                        break
            
            try:
                image = Image.open(img_path).convert('RGB')
                if self.transform:
                    image = self.transform(image)
                return image, class_id
            except Exception as e:
                print(f"é”™è¯¯åŠ è½½å›¾åƒ {img_path}: {e}")
                # è¿”å›ä¸€ä¸ªå ä½å›¾åƒå’Œç±»åˆ«
                placeholder = torch.zeros((3, 256, 256))
                return placeholder, class_id
    
    class KaggleVAEFineTuner(CloudVAEFineTuner):
        def __init__(self, data_dir, images_dir, annotations_dir):
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"ğŸš€ Kaggle P100 - VAE Fine-tuning è®¾å¤‡: {self.device}")
            
            # P100é…ç½® - 16GBæ˜¾å­˜
            self.config = {
                'batch_size': 12,  # å¢å¤§batch size
                'gradient_accumulation_steps': 2,  # ä¿æŒé€‚åº¦çš„æ¢¯åº¦ç´¯ç§¯
                'learning_rate': 1e-5,
                'weight_decay': 0.01,
                'max_epochs': 20,  # å¢åŠ è®­ç»ƒè½®æ¬¡
                'data_dir': data_dir,
                'images_dir': images_dir,
                'annotations_dir': annotations_dir,
                'save_dir': '/kaggle/working/vae_finetuned',
                'reconstruction_weight': 1.0,
                'kl_weight': 1e-5,  # ä¿æŒæ ‡å‡†KLæƒé‡
                'perceptual_weight': 0.3,  # å¯ç”¨æ„ŸçŸ¥æŸå¤±
                'use_perceptual_loss': True,  # å¯ç”¨æ„ŸçŸ¥æŸå¤±
                'save_every_epochs': 2,  # æ¯2ä¸ªepochä¿å­˜ä¸€æ¬¡
                'eval_every_epochs': 2,  # æ¯2ä¸ªepochè¯„ä¼°ä¸€æ¬¡
                'use_gradient_checkpointing': True,
                'mixed_precision': MIXED_PRECISION_AVAILABLE,
                'image_size': 256,  # ä½¿ç”¨å…¨å°ºå¯¸å›¾åƒ
            }
            
            os.makedirs(self.config['save_dir'], exist_ok=True)
            
            # é«˜æ€§èƒ½å›¾åƒè½¬æ¢
            transform = transforms.Compose([
                transforms.Resize((self.config['image_size'], self.config['image_size'])),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
            # åŠ è½½è®­ç»ƒå’ŒéªŒè¯é›†
            print("ğŸ“ åŠ è½½Kaggleè·¯å¾„çš„Oxford-IIIT Petæ•°æ®é›†...")
            train_annotation_file = os.path.join(annotations_dir, "trainval.txt")
            train_val_dataset = KaggleOxfordPetDataset(
                images_dir=images_dir,
                annotation_file=train_annotation_file,
                transform=transform
            )
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            train_size = int(len(train_val_dataset) * 0.8)
            val_size = len(train_val_dataset) - train_size
            train_dataset, val_dataset = torch.utils.data.random_split(
                train_val_dataset, [train_size, val_size], 
                generator=torch.Generator().manual_seed(42)
            )
            
            # åˆ›å»ºé«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨
            self.train_loader = DataLoader(
                train_dataset, 
                batch_size=self.config['batch_size'], 
                shuffle=True, 
                num_workers=2,  # Kaggleç¯å¢ƒæœ‰å¤šä¸ªCPUæ ¸å¿ƒ
                pin_memory=True,  # å¯ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“
                drop_last=True
            )
            
            self.val_loader = DataLoader(
                val_dataset, 
                batch_size=self.config['batch_size'], 
                shuffle=False, 
                num_workers=2,
                pin_memory=True
            )
            
            print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
            print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
            print(f"  å›¾åƒå°ºå¯¸: {self.config['image_size']}x{self.config['image_size']}")
            print(f"  æœ‰æ•ˆbatch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
            
            # åŠ è½½é¢„è®­ç»ƒVAE
            print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL...")
            self.vae = AutoencoderKL.from_pretrained(
                "runwayml/stable-diffusion-v1-5", 
                subfolder="vae"
            ).to(self.device).float()
            
            # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if hasattr(self.vae, 'enable_gradient_checkpointing'):
                self.vae.enable_gradient_checkpointing()
                print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
            
            for param in self.vae.parameters():
                param.requires_grad = True
            
            # é…ç½®ä¼˜åŒ–å™¨
            self.setup_optimizer()
            self.mse_loss = nn.MSELoss()
            self.setup_perceptual_loss()
            
            if self.config['mixed_precision']:
                self.scaler = GradScaler()
                print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
                
            self.train_history = {
                'epoch': [], 'train_loss': [], 'recon_loss': [],
                'kl_loss': [], 'perceptual_loss': [], 'val_mse': []
            }
            
            self.clear_memory()
            print("âœ… Kaggle P100è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ!")
            
        def setup_perceptual_loss(self):
            """è®¾ç½®æ„ŸçŸ¥æŸå¤±"""
            if not self.config['use_perceptual_loss']:
                print("ğŸš« æ„ŸçŸ¥æŸå¤±å·²ç¦ç”¨")
                self.perceptual_net = None
                return
                
            try:
                from torchvision.models import vgg16, VGG16_Weights
                self.perceptual_net = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(self.device).float()
                for param in self.perceptual_net.parameters():
                    param.requires_grad = False
                self.perceptual_net.eval()
                print("âœ… VGG16æ„ŸçŸ¥æŸå¤±å·²è®¾ç½® (FP32)")
            except:
                print("âš ï¸ VGG16ä¸å¯ç”¨ï¼Œè·³è¿‡æ„ŸçŸ¥æŸå¤±")
                self.perceptual_net = None
                
        def compute_perceptual_loss(self, real, fake):
            """è®¡ç®—æ„ŸçŸ¥æŸå¤±"""
            if self.perceptual_net is None:
                return torch.tensor(0.0, device=self.device, dtype=torch.float32)
            
            # ç¡®ä¿ç½‘ç»œå’Œè¾“å…¥æ˜¯float32
            self.perceptual_net.float()
            real = real.float()
            fake = fake.float()
            
            # ç¡®ä¿è¾“å…¥æ˜¯3é€šé“
            if real.size(1) == 1:
                real = real.repeat(1, 3, 1, 1)
            if fake.size(1) == 1:
                fake = fake.repeat(1, 3, 1, 1)
            
            # è®¡ç®—VGGç‰¹å¾
            with torch.no_grad():
                real_features = self.perceptual_net(real)
            
            fake_features = self.perceptual_net(fake)
            
            # è®¡ç®—MSEæŸå¤±
            return F.mse_loss(real_features.float(), fake_features.float())
            
        def vae_loss(self, images):
            """ç¨³å®šç‰ˆVAEæŸå¤±å‡½æ•°"""
            try:
                # ç¡®ä¿è¾“å…¥ä¸ºfloat32
                images = images.float()
                
                # VAEå‰å‘ä¼ æ’­
                posterior = self.vae.encode(images).latent_dist
                latents = posterior.sample().float()
                reconstructed = self.vae.decode(latents).sample.float()
                
                # é‡å»ºæŸå¤±
                recon_loss = self.mse_loss(reconstructed, images)
                
                # æ›´ç¨³å®šçš„KLæ•£åº¦è®¡ç®—ï¼Œé˜²æ­¢NaN
                try:
                    # åŸå§‹KLæ•£åº¦è®¡ç®—å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
                    kl_raw = posterior.kl()
                    
                    # æ£€æŸ¥å¹¶å¤„ç†NaNæˆ–æ— ç©·å¤§å€¼
                    if torch.isnan(kl_raw).any() or torch.isinf(kl_raw).any():
                        print("âš ï¸ æ£€æµ‹åˆ°KLæ•£åº¦ä¸­çš„NaN/Infå€¼ï¼Œä½¿ç”¨æ›¿ä»£è®¡ç®—æ–¹æ³•")
                        # æ›¿ä»£è®¡ç®—æ–¹æ³•
                        mean, var = posterior.mean, posterior.var
                        eps = 1e-8
                        kl_loss = 0.5 * torch.mean(mean.pow(2) + var - torch.log(var + eps) - 1)
                    else:
                        kl_loss = kl_raw.mean()
                        
                    # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
                    if torch.isnan(kl_loss) or torch.isinf(kl_loss):
                        print("âš ï¸ KLæŸå¤±ä»ç„¶åŒ…å«NaN/Infï¼Œä½¿ç”¨å¸¸æ•°æ›¿ä»£")
                        kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
                except Exception as kl_e:
                    print(f"âš ï¸ KLæ•£åº¦è®¡ç®—é”™è¯¯: {kl_e}ï¼Œä½¿ç”¨æ›¿ä»£å€¼")
                    kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
                
                # æ„ŸçŸ¥æŸå¤±
                if self.perceptual_net:
                    perceptual_loss = self.compute_perceptual_loss(images, reconstructed)
                else:
                    perceptual_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
                
                # è®¡ç®—æ€»æŸå¤±
                total_loss = (
                    self.config['reconstruction_weight'] * recon_loss +
                    self.config['kl_weight'] * kl_loss +
                    self.config['perceptual_weight'] * perceptual_loss
                )
                
                # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
                if torch.isnan(total_loss) or torch.isinf(total_loss):
                    print("âš ï¸ æ€»æŸå¤±åŒ…å«NaN/Infï¼Œä»…ä½¿ç”¨é‡å»ºæŸå¤±")
                    total_loss = self.config['reconstruction_weight'] * recon_loss
                
                return {
                    'total_loss': total_loss,
                    'recon_loss': recon_loss,
                    'kl_loss': kl_loss,
                    'perceptual_loss': perceptual_loss,
                    'reconstructed': reconstructed,
                    'latents': latents
                }
            except Exception as e:
                print(f"ğŸ”¥ VAE_LOSSå†…éƒ¨å‘ç”Ÿé”™è¯¯ï¼Œç±»å‹: {type(e)}, å†…å®¹: {e} ğŸ”¥")
                import traceback
                traceback.print_exc()
                raise e
    
    return KaggleVAEFineTuner(data_dir, images_dir, annotations_dir)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ å¯åŠ¨äº‘VAE Fine-tuningå®Œæ•´è®­ç»ƒ (Oxford-IIIT Petæ•°æ®é›†)...")
    
    # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
    if torch.cuda.is_available():
        # å¯ç”¨å†…å­˜åˆ†ç‰‡ç®¡ç†
        torch.cuda.set_per_process_memory_fraction(0.95)  # ä½¿ç”¨95%çš„æ˜¾å­˜
        torch.cuda.empty_cache()
        print(f"ğŸ”§ CUDAå†…å­˜ä¼˜åŒ–è®¾ç½®å®Œæˆ")
    
    # Kaggleç¯å¢ƒæ•°æ®ç›®å½•
    data_dir = '/kaggle/input/dataset-test'
    images_dir = '/kaggle/input/dataset-test/images/images'
    annotations_dir = '/kaggle/input/dataset-test/annotations/annotations'
    
    print(f"ğŸ“Š æ•°æ®é›†è·¯å¾„é…ç½®:")
    print(f"  ä¸»ç›®å½•: {data_dir}")
    print(f"  å›¾åƒç›®å½•: {images_dir}")
    print(f"  æ ‡æ³¨ç›®å½•: {annotations_dir}")
    
    if not os.path.exists(images_dir) or not os.path.exists(annotations_dir):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {images_dir} æˆ– {annotations_dir}")
        return None
    
    try:
        # ä½¿ç”¨é€‚åˆP100 16GBæ˜¾å­˜çš„é«˜æ€§èƒ½é…ç½®
        print("ğŸ’¡ ä½¿ç”¨P100 GPUé«˜æ€§èƒ½é…ç½®...")
        kaggle_trainer = create_kaggle_trainer(data_dir, images_dir, annotations_dir)
        best_loss = kaggle_trainer.finetune()
        print(f"\nğŸ¯ P100 GPU VAE Fine-tuningå®Œæˆï¼Œæœ€ä½³é‡å»ºæŸå¤±: {best_loss:.4f}")
        print("ğŸ’¡ VAEå·²ç»é€‚é…Oxford-IIIT Petæ•°æ®é›†ï¼Œå¯ä»¥ç”¨äºLDMè®­ç»ƒ")
        return best_loss
    
    except Exception as e:
        print("ğŸ”¥ Top-level exception caught in main. Full traceback follows: ğŸ”¥")
        import traceback
        traceback.print_exc()

        error_str_lower = str(e).lower()
        print(f"ğŸ•µï¸â€â™€ï¸ æ•è·åˆ°å¼‚å¸¸è¯¦æƒ…: ç±»å‹={type(e)}, å†…å®¹='{str(e)}'")

        if "out of memory" in error_str_lower:
            print("ğŸ’¡ å°è¯•ä½¿ç”¨é™çº§é…ç½®è¿›è¡Œè®­ç»ƒ...")
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                
                # é™çº§é…ç½®è®­ç»ƒ
                fallback_trainer = create_ultra_low_memory_finetuner(data_dir, use_cpu=False)
                best_loss = fallback_trainer.finetune()
                print(f"\nğŸ¯ é™çº§é…ç½®VAE Fine-tuningå®Œæˆï¼Œæœ€ä½³é‡å»ºæŸå¤±: {best_loss:.4f}")
                return best_loss
            except Exception as fallback_e:
                print(f"âŒ é™çº§é…ç½®ä¹Ÿå¤±è´¥äº†: {fallback_e}")
                traceback.print_exc()
                return None
        else:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æœªåˆ†ç±»çš„é”™è¯¯: {e}")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            return None

if __name__ == "__main__":
    main() 
import matplotlib.pyplot as plt

# äº‘ç¯å¢ƒåˆå§‹åŒ–
print("ğŸŒ äº‘æœåŠ¡å™¨VAE Fine-tuningç¯å¢ƒåˆå§‹åŒ–...")
print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# å†…å­˜ä¼˜åŒ–è®¾ç½®
torch.backends.cudnn.benchmark = True
# æ·»åŠ å†…å­˜åˆ†é…ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸš€ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
def check_and_install_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–"""
    try:
        import diffusers
        print("âœ… diffusers å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£… diffusers...")
        os.system("pip install diffusers transformers accelerate -q")
    
    try:
        import torchvision
        print("âœ… torchvision å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£… torchvision...")
        os.system("pip install torchvision -q")

check_and_install_dependencies()

# å¯¼å…¥å¿…è¦æ¨¡å—
from diffusers import AutoencoderKL
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import glob
from sklearn.model_selection import train_test_split

# æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
try:
    import torch
    # æ£€æŸ¥PyTorchç‰ˆæœ¬å¹¶ä½¿ç”¨æ­£ç¡®çš„autocastå’ŒGradScaler
    if hasattr(torch, 'amp') and hasattr(torch.amp, 'autocast'):
        from torch.amp import autocast, GradScaler
        AUTOCAST_DEVICE = 'cuda'
        GRADSCALER_DEVICE = 'cuda'
        MIXED_PRECISION_AVAILABLE = True
        print("âœ… æ–°ç‰ˆæ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
    else:
        from torch.cuda.amp import autocast, GradScaler
        AUTOCAST_DEVICE = None
        GRADSCALER_DEVICE = None
        MIXED_PRECISION_AVAILABLE = True
        print("âœ… ä¼ ç»Ÿæ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
except ImportError:
    MIXED_PRECISION_AVAILABLE = False
    print("âš ï¸  æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨")

class CloudGaitDataset(Dataset):
    """äº‘ç¯å¢ƒæ•°æ®é›†ç±»"""
    
    def __init__(self, image_paths, class_ids, transform=None):
        self.image_paths = image_paths
        self.class_ids = class_ids
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        label = self.class_ids[idx]
        
        # ç¡®ä¿æ ‡ç­¾åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if label < 0 or label > 30:
            label = max(0, min(30, label))
        
        return image, label

def build_cloud_dataloader(root_dir, batch_size=8, num_workers=0, val_split=0.3):
    """å†…å­˜ä¼˜åŒ–çš„äº‘ç¯å¢ƒæ•°æ®åŠ è½½å™¨"""
    print(f"ğŸ” æ‰«æäº‘æ•°æ®é›†ç›®å½•: {root_dir}")
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    all_image_paths = []
    all_class_ids = []
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    if not os.path.exists(root_dir):
        raise ValueError(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {root_dir}")
    
    # æ”¯æŒID_1, ID_2, ... ID_31æ–‡ä»¶å¤¹ç»“æ„
    id_folders = []
    for item in os.listdir(root_dir):
        item_path = os.path.join(root_dir, item)
        if os.path.isdir(item_path) and item.startswith('ID_'):
            id_folders.append(item)
    
    if id_folders:
        print(f"ğŸ“ å‘ç°IDæ–‡ä»¶å¤¹ç»“æ„: {len(id_folders)} ä¸ªæ–‡ä»¶å¤¹")
        
        for folder_name in sorted(id_folders):
            folder_path = os.path.join(root_dir, folder_name)
            
            try:
                # æå–ç±»åˆ«ID
                class_id = int(folder_name.split('_')[1]) - 1  # è½¬æ¢ä¸º0-based
                
                if not (0 <= class_id <= 30):
                    print(f"âš ï¸  è·³è¿‡è¶…å‡ºèŒƒå›´çš„ç±»åˆ«: {folder_name}")
                    continue
                
                # æ”¶é›†å›¾ç‰‡
                folder_images = []
                for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                    folder_images.extend(glob.glob(os.path.join(folder_path, ext)))
                
                print(f"  {folder_name}: {len(folder_images)} å¼ å›¾ç‰‡")
                
                all_image_paths.extend(folder_images)
                all_class_ids.extend([class_id] * len(folder_images))
                
            except (ValueError, IndexError) as e:
                print(f"âš ï¸  æ— æ³•è§£ææ–‡ä»¶å¤¹å {folder_name}: {e}")
                continue

    if not all_image_paths:
        raise ValueError(f"âŒ åœ¨ {root_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")

    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(all_image_paths)}")
    print(f"  ç±»åˆ«æ•°: {len(set(all_class_ids))}")

    # æ•°æ®é›†åˆ’åˆ†
    train_paths, val_paths, train_ids, val_ids = train_test_split(
        all_image_paths, all_class_ids, 
        test_size=val_split, 
        random_state=42,
        stratify=all_class_ids if len(set(all_class_ids)) > 1 else None
    )

    print(f"  è®­ç»ƒé›†: {len(train_paths)} å¼ ")
    print(f"  éªŒè¯é›†: {len(val_paths)} å¼ ")

    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    train_dataset = CloudGaitDataset(train_paths, train_ids, transform=transform)
    val_dataset = CloudGaitDataset(val_paths, val_ids, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=num_workers, pin_memory=True if torch.cuda.is_available() else False)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, 
                           num_workers=num_workers, pin_memory=True if torch.cuda.is_available() else False)
    
    return train_loader, val_loader, len(train_dataset), len(val_dataset)

class CloudVAEFineTuner:
    """äº‘ç¯å¢ƒVAE Fine-tuningå™¨"""
    
    def __init__(self, data_dir='/kaggle/input/dataset/dataset'):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ äº‘VAE Fine-tuning è®¾å¤‡: {self.device}")
        
        # å†…å­˜ä¼˜åŒ–é…ç½®
        self.config = {
            'batch_size': 8,  # å‡å°batch sizeä»¥èŠ‚çœæ˜¾å­˜
            'gradient_accumulation_steps': 2,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ä¿æŒæœ‰æ•ˆbatch size
            'learning_rate': 1e-5,  # è¾ƒå°å­¦ä¹ ç‡ä¿æŠ¤é¢„è®­ç»ƒæƒé‡
            'weight_decay': 0.01,
            'max_epochs': 25,  # å®Œæ•´è®­ç»ƒ
            'data_dir': data_dir,
            'save_dir': './vae_finetuned',
            'reconstruction_weight': 1.0,
            'kl_weight': 1e-5,  # å¤§å¹…é™ä½KLæƒé‡ï¼Œæ›´å…³æ³¨é‡å»ºè´¨é‡
            'perceptual_weight': 0.3,  # æ¢å¤åˆç†çš„æ„ŸçŸ¥æŸå¤±æƒé‡
            'use_perceptual_loss': True,  # ä¿æŒæ„ŸçŸ¥æŸå¤±å¼€å¯
            'save_every_epochs': 5,  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡
            'eval_every_epochs': 2,  # æ¯2ä¸ªepochè¯„ä¼°ä¸€æ¬¡
            'use_gradient_checkpointing': True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            'mixed_precision': MIXED_PRECISION_AVAILABLE,  # æ··åˆç²¾åº¦è®­ç»ƒ
            'safe_mixed_precision': True,  # å®‰å…¨æ¨¡å¼ï¼šé‡åˆ°é—®é¢˜æ—¶è‡ªåŠ¨ç¦ç”¨æ··åˆç²¾åº¦
        }
        self.best_model_checkpoint_path = None # ç”¨äºè·Ÿè¸ªè¦åˆ é™¤çš„æ—§æœ€ä½³æ¨¡å‹
        
        os.makedirs(self.config['save_dir'], exist_ok=True)
        
        # 1. åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°‘num_workers
        print("ğŸ“ åŠ è½½äº‘æ•°æ®é›†...")
        self.train_loader, self.val_loader, train_size, val_size = build_cloud_dataloader(
            root_dir=self.config['data_dir'],
            batch_size=self.config['batch_size'],
            num_workers=0,  # å‡å°‘workersä»¥èŠ‚çœå†…å­˜
            val_split=0.3
        )
        print(f"   è®­ç»ƒé›†: {len(self.train_loader)} batches ({train_size} samples)")
        print(f"   éªŒè¯é›†: {len(self.val_loader)} batches ({val_size} samples)")
        print(f"   æœ‰æ•ˆbatch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
        
        # 2. åŠ è½½é¢„è®­ç»ƒVAE
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL...")
        self.vae = AutoencoderKL.from_pretrained(
            "runwayml/stable-diffusion-v1-5", 
            subfolder="vae"
        ).to(self.device).float()
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config['use_gradient_checkpointing']:
            if hasattr(self.vae, 'enable_gradient_checkpointing'):
                self.vae.enable_gradient_checkpointing()
                print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        # è§£å†»VAEå‚æ•°ç”¨äºfine-tuning
        for param in self.vae.parameters():
            param.requires_grad = True
        
        print(f"ğŸ”“ VAEå‚æ•°å·²è§£å†»ç”¨äºfine-tuning")
        
        # 3. é…ç½®ä¼˜åŒ–å™¨
        self.setup_optimizer()
        
        # 4. æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.setup_perceptual_loss()
        
        # 5. æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config['mixed_precision']:
            if GRADSCALER_DEVICE:
                self.scaler = GradScaler(GRADSCALER_DEVICE)
            else:
                self.scaler = GradScaler()
            print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'recon_loss': [],
            'kl_loss': [],
            'perceptual_loss': [],
            'val_mse': []
        }
        
        # æ¸…ç†åˆå§‹åŒ–åçš„å†…å­˜
        self.clear_memory()
        
        print("âœ… äº‘VAE Fine-tuningå™¨åˆå§‹åŒ–å®Œæˆ!")
    
    def setup_optimizer(self):
        """è®¾ç½®åˆ†å±‚ä¼˜åŒ–å™¨"""
        # åªfine-tune decoder + encoderçš„æœ€åå‡ å±‚
        finetune_params = []
        
        # Decoder - å…¨éƒ¨fine-tune
        finetune_params.extend(self.vae.decoder.parameters())
        
        # Encoder - åªfine-tuneæœ€å2ä¸ªblock
        encoder_blocks = list(self.vae.encoder.down_blocks)
        for block in encoder_blocks[-2:]:  # æœ€å2ä¸ªblock
            finetune_params.extend(block.parameters())
        
        # Post quant conv
        finetune_params.extend(self.vae.quant_conv.parameters())
        finetune_params.extend(self.vae.post_quant_conv.parameters())
        
        total_params = sum(p.numel() for p in finetune_params)
        print(f"ğŸ¯ Fine-tuneå‚æ•°æ•°é‡: {total_params:,}")
        
        self.optimizer = torch.optim.AdamW(
            finetune_params,
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.config['max_epochs']
        )
    
    def setup_perceptual_loss(self):
        """è®¾ç½®å®Œæ•´æ„ŸçŸ¥æŸå¤±"""
        if not self.config['use_perceptual_loss']:
            print("ğŸš« æ„ŸçŸ¥æŸå¤±å·²å…³é—­ä»¥èŠ‚çœå†…å­˜")
            self.perceptual_net = None
            return
            
        try:
            from torchvision.models import vgg16, VGG16_Weights
            self.perceptual_net = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(self.device).float()
            for param in self.perceptual_net.parameters():
                param.requires_grad = False
            self.perceptual_net.eval()
            print("âœ… å®Œæ•´VGG16æ„ŸçŸ¥æŸå¤±å·²è®¾ç½® (FP32)")
        except:
            print("âš ï¸  VGG16ä¸å¯ç”¨ï¼Œè·³è¿‡æ„ŸçŸ¥æŸå¤±")
            self.perceptual_net = None
    
    def compute_perceptual_loss(self, real, fake):
        """è®¡ç®—å®Œæ•´æ„ŸçŸ¥æŸå¤±"""
        if self.perceptual_net is None:
            return torch.tensor(0.0, device=self.device, dtype=torch.float32)
        
        # Ensure network and inputs are float32
        self.perceptual_net.float()
        real = real.float()
        fake = fake.float()
        
        # ç¡®ä¿è¾“å…¥æ˜¯3é€šé“
        if real.size(1) == 1:
            real = real.repeat(1, 3, 1, 1)
        if fake.size(1) == 1:
            fake = fake.repeat(1, 3, 1, 1)
        
        # è®¡ç®—VGGç‰¹å¾ï¼Œå®Œå…¨é¿å…autocastçš„å½±å“
        with torch.no_grad():
            real_features = self.perceptual_net(real)
        
        # ç¡®ä¿fake_featuresè®¡ç®—ä¹Ÿä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
        fake_features = self.perceptual_net(fake)
        
        # Ensure MSEæŸå¤±è®¡ç®—ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
        return F.mse_loss(real_features.float(), fake_features.float())
    
    def vae_loss(self, images):
        """VAEæŸå¤±å‡½æ•° - VAEæ“ä½œå¼ºåˆ¶FP32ï¼Œå…¶ä½™ä¾èµ–å¤–éƒ¨autocast"""
        # images è¾“å…¥æ—¶ï¼Œåœ¨æ··åˆç²¾åº¦æ¨¡å¼ä¸‹å·²åœ¨ train_epoch ä¸­è¢«è½¬æ¢ä¸º .float(), 
        # å¹¶ä¸”æ­¤å‡½æ•°åœ¨é¡¶å±‚ autocast(dtype=torch.float16) ä¸Šä¸‹æ–‡ä¸­è¢«è°ƒç”¨ã€‚

        try:
            # 1. VAEæ“ä½œï¼šå¼ºåˆ¶åœ¨FP32ä¸‹æ‰§è¡Œï¼Œå±€éƒ¨è¦†ç›–å¤–éƒ¨autocast
            if AUTOCAST_DEVICE: # æ–°ç‰ˆPyTorch
                with autocast(AUTOCAST_DEVICE, enabled=False, dtype=torch.float32):
                    posterior = self.vae.encode(images.float()).latent_dist
                    latents = posterior.sample().float()
                    reconstructed = self.vae.decode(latents).sample.float()
            else: # ä¼ ç»Ÿç‰ˆæœ¬ PyTorch (autocast æŒ‡ torch.cuda.amp.autocast)
                with autocast(enabled=False):
                    posterior = self.vae.encode(images.float()).latent_dist
                    latents = posterior.sample().float()
                    reconstructed = self.vae.decode(latents).sample.float()

            # 2. é‡å»ºæŸå¤±ï¼šè¾“å…¥ç¡®ä¿æ˜¯FP32
            recon_loss = self.mse_loss(reconstructed.float(), images.float()) # images å·²æ˜¯ .float()
            
            # 3. KLæ•£åº¦æŸå¤±ï¼šç»“æœè½¬æ¢ä¸ºFP32
            kl_loss = posterior.kl().mean().float()
            
            # 4. æ„ŸçŸ¥æŸå¤±ï¼šè¾“å…¥ç¡®ä¿æ˜¯FP32ã€‚compute_perceptual_losså†…éƒ¨å·²å¤„ç†å¥½ç½‘ç»œç²¾åº¦ã€‚
            if self.perceptual_net:
                perceptual_loss = self.compute_perceptual_loss(images.float(), reconstructed.float())
            else:
                perceptual_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
            
            # 5. æ€»æŸå¤±ï¼šæ‰€æœ‰ç»„ä»¶éƒ½åº”æ˜¯FP32
            total_loss = (
                self.config['reconstruction_weight'] * recon_loss +
                self.config['kl_weight'] * kl_loss +
                self.config['perceptual_weight'] * perceptual_loss
            )
            
            # 6. è¿”å›å€¼ï¼šç¡®ä¿æ‰€æœ‰è¿”å›çš„å¼ é‡æ˜¯FP32
            return {
                'total_loss': total_loss.float(),
                'recon_loss': recon_loss.float(),
                'kl_loss': kl_loss.float(),
                'perceptual_loss': perceptual_loss.float(),
                'reconstructed': reconstructed.float(),
                'latents': latents.float()
            }
            
        except Exception as e:
            error_str = str(e).lower()
            if "dtype" in error_str or "type" in error_str or "expected scalar type" in error_str:
                print(f"âš ï¸ VAEæŸå¤±è®¡ç®—ä¸­çš„æ•°æ®ç±»å‹é”™è¯¯: {e}")
                return {
                    'total_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32, requires_grad=True),
                    'recon_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32),
                    'kl_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32),
                    'perceptual_loss': torch.tensor(0.0, device=self.device, dtype=torch.float32),
                    'reconstructed': images.float(),
                    'latents': torch.zeros_like(images, dtype=torch.float32)
                }
            else:
                print(f"ğŸ”¥ VAE_LOSSå†…éƒ¨å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œç±»å‹: {type(e)}, å†…å®¹: {e} ğŸ”¥")
                import traceback
                traceback.print_exc()
                raise e
    
    def clear_memory(self):
        """å¢å¼ºå†…å­˜æ¸…ç†"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
    
    def train_epoch(self, epoch: int):
        """å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒepoch"""
        self.vae.train()
        
        total_loss = 0
        total_recon = 0
        total_kl = 0
        total_perceptual = 0
        num_batches = 0
        
        # é‡ç½®æ¢¯åº¦ç´¯ç§¯
        self.optimizer.zero_grad()
        
        pbar = tqdm(self.train_loader, desc=f"Fine-tune Epoch {epoch+1}")
        
        for batch_idx, batch in enumerate(pbar):
            images, _ = batch  # å¿½ç•¥æ ‡ç­¾
            images = images.to(self.device, non_blocking=True)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.config['mixed_precision']:
                if AUTOCAST_DEVICE:
                    with autocast(AUTOCAST_DEVICE, dtype=torch.float16):
                        loss_dict = self.vae_loss(images)
                        loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                else:
                    with autocast(): # Old API for mixed precision context
                        loss_dict = self.vae_loss(images)
                        loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                
                # ç¡®ä¿æŸå¤±æ˜¯ float32 ç±»å‹å†è¿›è¡Œç¼©æ”¾å’Œåå‘ä¼ æ’­
                self.scaler.scale(loss.float()).backward()
            else:
                # å‰å‘ä¼ æ’­ (æ— æ··åˆç²¾åº¦)
                loss_dict = self.vae_loss(images)
                loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                loss.backward() # ç›´æ¥åå‘ä¼ æ’­
            
            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:
                try:
                    if self.config['mixed_precision']:
                        # æ¢¯åº¦è£å‰ª
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                        
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                        self.optimizer.step()
                    
                    self.optimizer.zero_grad()
                    
                except RuntimeError as grad_e:
                    if "dtype" in str(grad_e).lower() or "type" in str(grad_e).lower():
                        print(f"âš ï¸ æ¢¯åº¦æ­¥éª¤æ•°æ®ç±»å‹é”™è¯¯ï¼Œè·³è¿‡æ­¤æ­¥éª¤: {grad_e}")
                        self.optimizer.zero_grad()
                        continue
                    else:
                        raise grad_e
            
            # è®°å½•æŸå¤±ï¼ˆæ¢å¤åˆ°åŸå§‹scaleï¼‰
            actual_loss = loss.item() * self.config['gradient_accumulation_steps']
            total_loss += actual_loss
            total_recon += loss_dict['recon_loss'].item()
            total_kl += loss_dict['kl_loss'].item()
            total_perceptual += loss_dict['perceptual_loss'].item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{actual_loss:.4f}',
                'Recon': f'{loss_dict["recon_loss"].item():.4f}',
                'KL': f'{loss_dict["kl_loss"].item():.4f}',
                'Perc': f'{loss_dict["perceptual_loss"].item():.4f}'
            })
            
            # ä¿å­˜ç¬¬ä¸€ä¸ªbatchçš„é‡å»ºæ ·æœ¬
            if batch_idx == 0:
                self.save_reconstruction_samples(
                    images, loss_dict['reconstructed'], epoch
                )
            
            # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†
            if batch_idx % 20 == 0:  # æ¯20ä¸ªbatchæ¸…ç†ä¸€æ¬¡
                del images, loss_dict, loss
                self.clear_memory()
        
        # å¤„ç†æœ€åä¸å®Œæ•´çš„æ¢¯åº¦ç´¯ç§¯
        if (len(self.train_loader)) % self.config['gradient_accumulation_steps'] != 0:
            if self.config['mixed_precision']:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                self.optimizer.step()
            self.optimizer.zero_grad()
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()
        
        avg_metrics = {
            'avg_loss': total_loss / num_batches,
            'avg_recon': total_recon / num_batches,
            'avg_kl': total_kl / num_batches,
            'avg_perceptual': total_perceptual / num_batches
        }
        
        self.clear_memory()
        return avg_metrics
    
    def save_reconstruction_samples(self, original, reconstructed, epoch):
        """ä¿å­˜é‡å»ºæ ·æœ¬å¯¹æ¯”"""
        with torch.no_grad():
            # åå½’ä¸€åŒ–
            def denormalize(tensor):
                return torch.clamp((tensor + 1) / 2, 0, 1)
            
            orig = denormalize(original[:8]).cpu()
            recon = denormalize(reconstructed[:8]).cpu()
            
            fig, axes = plt.subplots(2, 8, figsize=(24, 6))
            
            for i in range(8):
                # åŸå›¾
                axes[0, i].imshow(orig[i].permute(1, 2, 0))
                axes[0, i].set_title(f'Original {i+1}')
                axes[0, i].axis('off')
                
                # é‡å»ºå›¾
                axes[1, i].imshow(recon[i].permute(1, 2, 0))
                axes[1, i].set_title(f'Reconstructed {i+1}')
                axes[1, i].axis('off')
            
            plt.suptitle(f'VAE Reconstruction - Epoch {epoch+1}')
            plt.tight_layout()
            plt.savefig(f'{self.config["save_dir"]}/reconstruction_epoch_{epoch+1}.png',
                       dpi=150, bbox_inches='tight')
            plt.close()
    
    @torch.no_grad()
    def evaluate_reconstruction_quality(self, val_loader):
        """å†…å­˜ä¼˜åŒ–çš„è¯„ä¼°å‡½æ•°"""
        self.vae.eval()
        
        total_mse = 0
        total_samples = 0
        latent_stats = []
        
        # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡
        max_eval_batches = min(20, len(val_loader))
        
        for batch_idx, batch in enumerate(val_loader):
            if batch_idx >= max_eval_batches:
                break
                
            images, _ = batch
            images = images.to(self.device, non_blocking=True)
            
            # VAEé‡å»º
            if self.config['mixed_precision']:
                if AUTOCAST_DEVICE:
                    # æ–°ç‰ˆPyTorch
                    with autocast(AUTOCAST_DEVICE, dtype=torch.float16):
                        posterior = self.vae.encode(images).latent_dist
                        latents = posterior.sample()
                        reconstructed = self.vae.decode(latents).sample
                else:
                    # ä¼ ç»Ÿç‰ˆæœ¬
                    with autocast():
                        posterior = self.vae.encode(images).latent_dist
                        latents = posterior.sample()
                        reconstructed = self.vae.decode(latents).sample
            else:
                posterior = self.vae.encode(images).latent_dist
                latents = posterior.sample()
                reconstructed = self.vae.decode(latents).sample
            
            # è®¡ç®—MSE
            mse = F.mse_loss(reconstructed, images, reduction='sum')
            total_mse += mse.item()
            total_samples += images.size(0)
            
            # æ”¶é›†æ½œåœ¨è¡¨ç¤ºç»Ÿè®¡
            latent_stats.append({
                'mean': latents.mean().item(),
                'std': latents.std().item(),
                'min': latents.min().item(),
                'max': latents.max().item()
            })
            
            # æ¸…ç†å†…å­˜
            del images, posterior, latents, reconstructed, mse
            
            if batch_idx % 5 == 0:
                self.clear_memory()
        
        avg_mse = total_mse / total_samples
        
        # è®¡ç®—å¹³å‡æ½œåœ¨ç©ºé—´ç»Ÿè®¡
        avg_latent_stats = {
            'mean': np.mean([s['mean'] for s in latent_stats]),
            'std': np.mean([s['std'] for s in latent_stats]),
            'min': np.min([s['min'] for s in latent_stats]),
            'max': np.max([s['max'] for s in latent_stats])
        }
        
        print(f"ğŸ“Š é‡å»ºè´¨é‡MSE: {avg_mse:.6f}")
        print(f"ğŸ§  æ½œåœ¨ç©ºé—´ç»Ÿè®¡: å‡å€¼={avg_latent_stats['mean']:.4f}, "
              f"æ ‡å‡†å·®={avg_latent_stats['std']:.4f}, "
              f"èŒƒå›´=[{avg_latent_stats['min']:.2f}, {avg_latent_stats['max']:.2f}]")
        
        return avg_mse, avg_latent_stats
    
    def save_finetuned_vae(self, epoch, metrics):
        """ä¿å­˜fine-tuned VAE"""
        save_path = f'{self.config["save_dir"]}/vae_finetuned_epoch_{epoch+1}.pth'
        
        torch.save({
            'vae_state_dict': self.vae.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epoch': epoch,
            'metrics': metrics,
            'config': self.config,
            'train_history': self.train_history
        }, save_path)
        
        print(f"ğŸ’¾ Fine-tuned VAEä¿å­˜: {save_path}")
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if len(self.train_history['epoch']) == 0:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ€»æŸå¤±
        axes[0, 0].plot(self.train_history['epoch'], self.train_history['train_loss'])
        axes[0, 0].set_title('Training Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].grid(True)
        
        # é‡å»ºæŸå¤±
        axes[0, 1].plot(self.train_history['epoch'], self.train_history['recon_loss'])
        axes[0, 1].set_title('Reconstruction Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('MSE Loss')
        axes[0, 1].grid(True)
        
        # KLæŸå¤±
        axes[1, 0].plot(self.train_history['epoch'], self.train_history['kl_loss'])
        axes[1, 0].set_title('KL Divergence Loss')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('KL Loss')
        axes[1, 0].grid(True)
        
        # éªŒè¯MSE
        if self.train_history['val_mse']:
            axes[1, 1].plot(self.train_history['epoch'][::self.config['eval_every_epochs']], 
                           self.train_history['val_mse'])
            axes[1, 1].set_title('Validation MSE')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('MSE')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{self.config["save_dir"]}/training_history.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def finetune(self):
        """ä¸»è¦fine-tuningæµç¨‹"""
        print("ğŸš€ å¼€å§‹äº‘VAE Fine-tuningå®Œæ•´è®­ç»ƒ...")
        print("=" * 60)
        
        if torch.cuda.is_available(): # ç¡®ä¿åªåœ¨CUDAå¯ç”¨æ—¶è®¾ç½®
            torch.autograd.set_detect_anomaly(True)
            print("âš ï¸ PyTorch å¼‚å¸¸æ£€æµ‹å·²å¯ç”¨ (ç”¨äºè°ƒè¯•ï¼Œå¯èƒ½å½±å“é€Ÿåº¦)")
        
        best_recon_loss = float('inf')
        
        for epoch in range(self.config['max_epochs']):
            print(f"\nğŸ“… Epoch {epoch+1}/{self.config['max_epochs']}")
            
            train_metrics = self.train_epoch(epoch)
            
            self.train_history['epoch'].append(epoch + 1)
            self.train_history['train_loss'].append(train_metrics['avg_loss'])
            self.train_history['recon_loss'].append(train_metrics['avg_recon'])
            self.train_history['kl_loss'].append(train_metrics['avg_kl'])
            self.train_history['perceptual_loss'].append(train_metrics['avg_perceptual'])
            
            if epoch % self.config['eval_every_epochs'] == 0 or epoch == self.config['max_epochs'] - 1:
                val_mse, latent_stats = self.evaluate_reconstruction_quality(self.val_loader)
                self.train_history['val_mse'].append(val_mse)
            
            print(f"ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"   æ€»æŸå¤±: {train_metrics['avg_loss']:.4f}")
            print(f"   é‡å»ºæŸå¤±: {train_metrics['avg_recon']:.4f}")
            print(f"   KLæŸå¤±: {train_metrics['avg_kl']:.4f}")
            print(f"   æ„ŸçŸ¥æŸå¤±: {train_metrics['avg_perceptual']:.4f}")
            print(f"   å­¦ä¹ ç‡: {self.scheduler.get_last_lr()[0]:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹é€»è¾‘ä¿®æ”¹
            current_epoch_model_path = f'{self.config["save_dir"]}/vae_finetuned_epoch_{epoch+1}.pth'
            is_best_model_save = False
            if train_metrics['avg_recon'] < best_recon_loss:
                best_recon_loss = train_metrics['avg_recon']
                
                # å°è¯•åˆ é™¤ä¸Šä¸€ä¸ªè¢«æ ‡è®°ä¸º"æœ€ä½³"çš„æ¨¡å‹æ–‡ä»¶
                if self.best_model_checkpoint_path and os.path.exists(self.best_model_checkpoint_path):
                    # æ£€æŸ¥è¿™ä¸ªæ—§çš„æœ€ä½³æ¨¡å‹æ˜¯å¦ä¹Ÿæ˜¯ä¸€ä¸ªå®šæœŸä¿å­˜ç‚¹
                    try:
                        # ä»æ–‡ä»¶åæå–æ—§çš„epochå·
                        old_epoch_str = self.best_model_checkpoint_path.split('_epoch_')[-1].split('.pth')[0]
                        old_epoch_idx = int(old_epoch_str) - 1 # epochå·è½¬ä¸º0-indexed
                        is_periodic_save = (old_epoch_idx % self.config['save_every_epochs'] == 0)
                        
                        if not is_periodic_save:
                            print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹: {self.best_model_checkpoint_path}")
                            os.remove(self.best_model_checkpoint_path)
                        else:
                            print(f"â„¹ï¸ ä¿ç•™æ—§çš„æœ€ä½³æ¨¡å‹ (åŒæ—¶ä¹Ÿæ˜¯å®šæœŸä¿å­˜ç‚¹): {self.best_model_checkpoint_path}")
                    except Exception as e_remove:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æˆ–æ£€æŸ¥æ—§çš„æœ€ä½³æ¨¡å‹: {e_remove}")

                self.save_finetuned_vae(epoch, train_metrics) # ä¿å­˜å½“å‰æ¨¡å‹
                self.best_model_checkpoint_path = current_epoch_model_path # æ›´æ–°æœ€ä½³æ¨¡å‹è·¯å¾„
                print(f"âœ… ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (é‡å»ºæŸå¤±: {best_recon_loss:.4f}): {current_epoch_model_path}")
                is_best_model_save = True
            
            if epoch % self.config['save_every_epochs'] == 0:
                if not is_best_model_save: # å¦‚æœæ­¤epochå·²ä½œä¸ºæœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œåˆ™ä¸å†é‡å¤ä¿å­˜
                    self.save_finetuned_vae(epoch, train_metrics)
                self.plot_training_history()
            
            print(f"ğŸ† å½“å‰æœ€ä½³é‡å»ºæŸå¤±: {best_recon_loss:.4f}")
            self.clear_memory()
        
        # è®­ç»ƒå®Œæˆ
        print("\n" + "=" * 60)
        print("ğŸ‰ äº‘VAE Fine-tuning å®Œæ•´è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³é‡å»ºæŸå¤±: {best_recon_loss:.4f}")
        
        # æœ€ç»ˆè¯„ä¼°
        final_mse, final_latent_stats = self.evaluate_reconstruction_quality(self.val_loader)
        print(f"ğŸ¯ æœ€ç»ˆé‡å»ºMSE: {final_mse:.6f}")
        
        # ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒå†å²
        self.plot_training_history()
        
        return best_recon_loss

def create_emergency_low_memory_finetuner(data_dir):
    """åˆ›å»ºæä½å†…å­˜é…ç½®çš„Fine-tuner"""
    print("ğŸ†˜ å¯åŠ¨åº”æ€¥ä½å†…å­˜æ¨¡å¼...")
    
    class EmergencyCloudVAEFineTuner(CloudVAEFineTuner):
        def __init__(self, data_dir):
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"ğŸš€ åº”æ€¥æ¨¡å¼ - äº‘VAE Fine-tuning è®¾å¤‡: {self.device}")
            
            # æç«¯å†…å­˜ä¼˜åŒ–é…ç½®
            self.config = {
                'batch_size': 4,  # æå°batch size
                'gradient_accumulation_steps': 4,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
                'learning_rate': 1e-5,
                'weight_decay': 0.01,
                'max_epochs': 15,  # å‡å°‘è®­ç»ƒè½®æ¬¡
                'data_dir': data_dir,
                'save_dir': './vae_finetuned',
                'reconstruction_weight': 1.0,
                'kl_weight': 0.05,  # è¿›ä¸€æ­¥é™ä½KLæƒé‡
                'perceptual_weight': 0.1,  # ä¿æŒè½»é‡æ„ŸçŸ¥æŸå¤±
                'use_perceptual_loss': True,  # ä¿æŒæ„ŸçŸ¥æŸå¤±å¼€å¯
                'save_every_epochs': 10,
                'eval_every_epochs': 5,
                'use_gradient_checkpointing': True,
                'mixed_precision': MIXED_PRECISION_AVAILABLE,
            }
            
            # ç›´æ¥åˆå§‹åŒ–çˆ¶ç±»çš„ç»„ä»¶ï¼Œè·³è¿‡çˆ¶ç±»__init__
            os.makedirs(self.config['save_dir'], exist_ok=True)
            
            # æ•°æ®åŠ è½½å™¨
            print("ğŸ“ åŠ è½½åº”æ€¥æ¨¡å¼æ•°æ®é›†...")
            self.train_loader, self.val_loader, train_size, val_size = build_cloud_dataloader(
                root_dir=self.config['data_dir'],
                batch_size=self.config['batch_size'],
                num_workers=0,
                val_split=0.3
            )
            print(f"   åº”æ€¥æ¨¡å¼batch size: {self.config['batch_size']}")
            print(f"   æœ‰æ•ˆbatch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
            
            # åŠ è½½VAE
            print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL (åº”æ€¥æ¨¡å¼)...")
            self.vae = AutoencoderKL.from_pretrained(
                "runwayml/stable-diffusion-v1-5", 
                subfolder="vae"
            ).to(self.device).float()
            
            # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if hasattr(self.vae, 'enable_gradient_checkpointing'):
                self.vae.enable_gradient_checkpointing()
                print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
            
            for param in self.vae.parameters():
                param.requires_grad = True
            
            self.setup_optimizer()
            self.mse_loss = nn.MSELoss()
            self.setup_perceptual_loss()  # ä¼šè¢«é…ç½®è·³è¿‡
            
            if self.config['mixed_precision']:
                self.scaler = GradScaler()
                print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
            
            self.train_history = {
                'epoch': [], 'train_loss': [], 'recon_loss': [],
                'kl_loss': [], 'perceptual_loss': [], 'val_mse': []
            }
            
            self.clear_memory()
            print("âœ… åº”æ€¥æ¨¡å¼Fine-tuningå™¨åˆå§‹åŒ–å®Œæˆ!")
        
        def setup_perceptual_loss(self):
            """åº”æ€¥æ¨¡å¼è½»é‡çº§æ„ŸçŸ¥æŸå¤±"""
            try:
                from torchvision.models import vgg16, VGG16_Weights
                # åº”æ€¥æ¨¡å¼ä½¿ç”¨æ›´å°‘çš„VGGå±‚
                self.perceptual_net = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:8].to(self.device)
                for param in self.perceptual_net.parameters():
                    param.requires_grad = False
                self.perceptual_net.eval()
                print("âœ… åº”æ€¥æ¨¡å¼è½»é‡çº§VGGæ„ŸçŸ¥æŸå¤±å·²è®¾ç½®")
            except:
                print("âš ï¸  VGG16ä¸å¯ç”¨ï¼Œåº”æ€¥æ¨¡å¼è·³è¿‡æ„ŸçŸ¥æŸå¤±")
                self.perceptual_net = None
    
    return EmergencyCloudVAEFineTuner(data_dir)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ å¯åŠ¨äº‘VAE Fine-tuningå®Œæ•´è®­ç»ƒ...")
    
    # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
    if torch.cuda.is_available():
        # å¯ç”¨å†…å­˜åˆ†ç‰‡ç®¡ç†
        torch.cuda.set_per_process_memory_fraction(0.95)  # ä½¿ç”¨95%çš„æ˜¾å­˜
        torch.cuda.empty_cache()
        print(f"ğŸ”§ CUDAå†…å­˜ä¼˜åŒ–è®¾ç½®å®Œæˆ")
    
    # äº‘ç¯å¢ƒæ•°æ®è·¯å¾„
    data_dir = '/kaggle/input/dataset/dataset'
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return None
    
    try:
        # åˆ›å»ºfine-tuner
        finetuner = CloudVAEFineTuner(data_dir=data_dir)
        
        # å¼€å§‹å®Œæ•´fine-tuning
        best_loss = finetuner.finetune()
        
        print(f"\nğŸ¯ VAE Fine-tuningå®Œæˆï¼Œæœ€ä½³é‡å»ºæŸå¤±: {best_loss:.4f}")
        print("ğŸ’¡ VAEå·²ç»é€‚é…å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åŸŸï¼Œå¯ä»¥ç”¨äºLDMè®­ç»ƒ")
        
        return best_loss

    except Exception as e: # Catch broader exceptions for dtype issues first
        print("ğŸ”¥ Top-level exception caught in main. Full traceback follows: ğŸ”¥")
        import traceback
        traceback.print_exc() # PRINT FULL TRACEBACK IMMEDIATELY

        error_str_lower = str(e).lower()
        # Using original error string for debug print for more context
        print(f"ğŸ•µï¸â€â™€ï¸ æ•è·åˆ°å¼‚å¸¸è¯¦æƒ…: ç±»å‹={type(e)}, å†…å®¹='{str(e)}'")

        dtype_error_keywords = ["unsupported dtype", "dtype", "expected scalar type", "type mismatch"]
        is_dtype_error = any(keyword in error_str_lower for keyword in dtype_error_keywords)

        if is_dtype_error:
            print(f"âŒ æ•°æ®ç±»å‹ç›¸å…³é”™è¯¯è¯Šæ–­ (å·²ç”± traceback.print_exc() æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯): {e}")
            print("ğŸ”„ æ£€æµ‹åˆ°æ•°æ®ç±»å‹é—®é¢˜ï¼Œå°è¯•å®Œå…¨ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒé‡è¯•...")
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            try:
                print("ğŸ“¦ åˆ›å»ºçº¯FP32ç²¾åº¦è®­ç»ƒå™¨...")
                pure_fp32_finetuner = PureFP32CloudVAEFineTuner(data_dir)
                best_loss = pure_fp32_finetuner.finetune()
                print(f"\nğŸ¯ çº¯FP32æ¨¡å¼VAE Fine-tuningå®Œæˆï¼Œæœ€ä½³é‡å»ºæŸå¤±: {best_loss:.4f}")
                return best_loss
            except Exception as fp32_e:
                print(f"âŒ çº¯FP32æ¨¡å¼ä¹Ÿå¤±è´¥äº†. Full traceback follows: {fp32_e}")
                traceback.print_exc()
                return None

        elif "out of memory" in error_str_lower:
            print(f"âŒ æ˜¾å­˜ä¸è¶³é”™è¯¯ (å·²ç”± traceback.print_exc() æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯): {e}")
            # ... (rest of OOM handling remains the same, including its own traceback on failure)
            print("ğŸ†˜ å¯åŠ¨åº”æ€¥ä½å†…å­˜æ¨¡å¼...")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            try:
                emergency_finetuner = create_emergency_low_memory_finetuner(data_dir)
                best_loss = emergency_finetuner.finetune()
                print(f"\nğŸ¯ åº”æ€¥æ¨¡å¼VAE Fine-tuningå®Œæˆï¼Œæœ€ä½³é‡å»ºæŸå¤±: {best_loss:.4f}")
                return best_loss
            except Exception as emergency_e:
                print(f"âŒ åº”æ€¥æ¨¡å¼ä¹Ÿå¤±è´¥äº†. Full traceback follows: {emergency_e}")
                traceback.print_exc()
                print("ğŸ’¡ æœ€ç»ˆå»ºè®®: 1. é‡å¯å†…æ ¸æ¸…ç†æ‰€æœ‰å†…å­˜ 2. æ‰‹åŠ¨è®¾ç½®batch_size=2 3. ä½¿ç”¨CPUè®­ç»ƒï¼ˆéå¸¸æ…¢ï¼‰")
                return None
        else:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æœªåˆ†ç±»çš„é”™è¯¯ (å·²ç”± traceback.print_exc() æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯): {e}")
            # Fallback for any other exception, already printed by initial traceback
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            return None

if __name__ == "__main__":
    main() 
