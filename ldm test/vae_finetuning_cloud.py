"""
VAE Fine-tuning - äº‘æœåŠ¡å™¨å®Œæ•´ç‰ˆæœ¬
é’ˆå¯¹Oxford-IIIT Petæ•°æ®é›†ä¼˜åŒ–AutoencoderKL
æ•°æ®é›†è·¯å¾„ï¼š/data
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
import gc
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from diffusers import AutoencoderKL
from transformers import get_scheduler

# äº‘ç¯å¢ƒåˆå§‹åŒ–
print("ğŸŒ äº‘æœåŠ¡å™¨VAE Fine-tuningç¯å¢ƒåˆå§‹åŒ–...")
print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# å†…å­˜ä¼˜åŒ–è®¾ç½®
torch.backends.cudnn.benchmark = True
# æ·»åŠ å†…å­˜åˆ†é…ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸš€ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
def check_and_install_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–"""
    try:
        import diffusers
        print("âœ… diffusers å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£… diffusers...")
        os.system("pip install diffusers transformers accelerate -q")
    
    try:
        import torchvision
        print("âœ… torchvision å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£… torchvision...")
        os.system("pip install torchvision -q")

check_and_install_dependencies()

# å¯¼å…¥å¿…è¦æ¨¡å—
from diffusers import AutoencoderKL
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import glob
from sklearn.model_selection import train_test_split
from transformers import get_scheduler

# æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
try:
    import torch
    # æ£€æŸ¥PyTorchç‰ˆæœ¬å¹¶ä½¿ç”¨æ­£ç¡®çš„autocastå’ŒGradScaler
    if hasattr(torch, 'amp') and hasattr(torch.amp, 'autocast'):
        from torch.amp import autocast, GradScaler
        AUTOCAST_DEVICE = 'cuda'
        GRADSCALER_DEVICE = 'cuda'
        MIXED_PRECISION_AVAILABLE = True
        print("âœ… æ–°ç‰ˆæ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
    else:
        from torch.cuda.amp import autocast, GradScaler
        AUTOCAST_DEVICE = None
        GRADSCALER_DEVICE = None
        MIXED_PRECISION_AVAILABLE = True
        print("âœ… ä¼ ç»Ÿæ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
except ImportError:
    MIXED_PRECISION_AVAILABLE = False
    print("âš ï¸  æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨")

print("DEBUG PY: Reached point 1 - After initial setup and mixed precision check.")

class OxfordPetDataset(Dataset):
    """Oxford-IIIT Petæ•°æ®é›†ç±»"""
    
    def __init__(self, images_dir, annotation_file, transform=None):
        self.images_dir = images_dir
        self.transform = transform
        
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        self.samples = []
        with open(annotation_file, 'r') as f:
            for line in f:
                if line.startswith('#'):
                    continue  # è·³è¿‡æ³¨é‡Šè¡Œ
                parts = line.strip().split()
                if len(parts) >= 2:
                    image_id = parts[0]  # å›¾åƒID
                    class_id = int(parts[1]) - 1  # ç±»åˆ«ID (è½¬ä¸º0-indexed)
                    self.samples.append((image_id, class_id))
        
        print(f"åŠ è½½äº† {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        image_id, class_id = self.samples[idx]
        img_path = os.path.join(self.images_dir, f"{image_id}.jpg")
        
        # æœ‰äº›å›¾åƒå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ‰©å±•å
        if not os.path.exists(img_path):
            for ext in ['.jpeg', '.png', '.JPEG', '.PNG']:
                alt_path = os.path.join(self.images_dir, f"{image_id}{ext}")
                if os.path.exists(alt_path):
                    img_path = alt_path
                    break
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, class_id
        except Exception as e:
            print(f"é”™è¯¯åŠ è½½å›¾åƒ {img_path}: {e}")
            # è¿”å›ä¸€ä¸ªå ä½å›¾åƒå’Œç±»åˆ«
            placeholder = torch.zeros((3, 256, 256))
            return placeholder, class_id

def build_pet_dataloader(images_dir, annotations_dir, batch_size=8, num_workers=0, val_split=0.2):
    """æ„å»ºOxford Petæ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨"""
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # æ›´æ–°æ—¥å¿—å¹¶ç¡®ä¿ä½¿ç”¨ list.txt
    print(f"ğŸ“ åŠ è½½Oxford-IIIT Petæ•°æ®é›†...")
    annotation_file = os.path.join(annotations_dir, "list.txt")
    
    dataset = OxfordPetDataset(
        images_dir=images_dir,
        annotation_file=annotation_file,
        transform=transform
    )
    
    # åˆ†å‰²æ•°æ®é›†
    train_size = int(len(dataset) * (1 - val_split))
    val_size = len(dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers, 
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
    
    return train_loader, val_loader, len(train_dataset), len(val_dataset)

class VAEFineTuner:
    """ä¼˜åŒ–çš„VAEå¾®è°ƒå™¨ï¼Œä¸“ä¸ºP100 GPUè®¾è®¡"""
    def __init__(self, images_dir, annotations_dir, output_dir='/kaggle/working/vae_finetuned'):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ VAE Fine-tuning è®¾å¤‡: {self.device}")
        
        # é’ˆå¯¹P100 GPUä¼˜åŒ–çš„é…ç½®
        self.config = {
            'batch_size': 12,
            'gradient_accumulation_steps': 2,
            'learning_rate': 1e-5,
            'weight_decay': 0.01,
            'max_epochs': 16,
            'images_dir': images_dir,
            'annotations_dir': annotations_dir,
            'save_dir': output_dir,
            'reconstruction_weight': 1.0,
            'kl_weight': 1e-7,  # é™ä½KLæƒé‡ï¼Œæ›´æ³¨é‡é‡å»ºè´¨é‡
            'perceptual_weight': 0.1,  # é™ä½æ„ŸçŸ¥æŸå¤±æƒé‡
            'use_perceptual_loss': True,
            'save_every_epochs': 2,
            'eval_every_epochs': 2,
            'use_gradient_checkpointing': True,
            'mixed_precision': MIXED_PRECISION_AVAILABLE,
            'image_size': 256,
        }
        self.best_model_checkpoint_path = None
        
        os.makedirs(self.config['save_dir'], exist_ok=True)
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader, train_size, val_size = build_pet_dataloader(
            images_dir=self.config['images_dir'],
            annotations_dir=self.config['annotations_dir'],
            batch_size=self.config['batch_size'],
            num_workers=2,
            val_split=0.2
        )
        
        print(f"  æœ‰æ•ˆbatch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
        print(f"  å›¾åƒå°ºå¯¸: {self.config['image_size']}x{self.config['image_size']}")
        
        # åŠ è½½VAE
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒAutoencoderKL...")
        self.vae = AutoencoderKL.from_pretrained(
            "runwayml/stable-diffusion-v1-5", 
            subfolder="vae"
        ).to(self.device).float()
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if hasattr(self.vae, 'enable_gradient_checkpointing'):
            self.vae.enable_gradient_checkpointing()
            print("âœ… VAEæ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        # è§£å†»å‚æ•°
        for param in self.vae.parameters():
            param.requires_grad = True
        
        self.setup_optimizer()
        self.mse_loss = nn.MSELoss()
        self.setup_perceptual_loss()
        
        if self.config['mixed_precision']:
            self.scaler = GradScaler()
            print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®­ç»ƒå†å²è®°å½•
        self.train_history = {
            'epoch': [], 'train_loss': [], 'recon_loss': [],
            'kl_loss': [], 'perceptual_loss': [], 'val_mse': [], 'val_epoch': []
        }
        
        self.clear_memory()
        print("âœ… VAEå¾®è°ƒå™¨åˆå§‹åŒ–å®Œæˆ!")
    
    def setup_optimizer(self):
        """è®¾ç½®åˆ†å±‚ä¼˜åŒ–å™¨ï¼Œå¹¶ä½¿ç”¨å¸¦é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        finetune_params = list(self.vae.decoder.parameters()) + \
                        list(self.vae.encoder.down_blocks[-2].parameters()) + \
                        list(self.vae.encoder.down_blocks[-1].parameters()) + \
                        list(self.vae.quant_conv.parameters()) + \
                        list(self.vae.post_quant_conv.parameters())
        
        total_params = sum(p.numel() for p in finetune_params)
        print(f"ğŸ¯ Fine-tuneå‚æ•°æ•°é‡: {total_params:,}")
        
        self.optimizer = torch.optim.AdamW(
            finetune_params,
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
        num_update_steps_per_epoch = len(self.train_loader) // self.config['gradient_accumulation_steps']
        num_training_steps = self.config['max_epochs'] * num_update_steps_per_epoch
        # è®¾ç½®é¢„çƒ­æ­¥æ•°ï¼Œæ€»æ­¥æ•°çš„10%
        num_warmup_steps = int(num_training_steps * 0.1)

        print(f"ğŸ”„ ä¼˜åŒ–å™¨è®¾ç½® - ä½¿ç”¨å¸¦é¢„çƒ­çš„è°ƒåº¦å™¨:")
        print(f"   æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}")
        print(f"   é¢„çƒ­æ­¥æ•°: {num_warmup_steps} (å æ€»æ­¥æ•°çš„10%)")
        print(f"   æƒé‡é…ç½®: KLæŸå¤±={self.config['kl_weight']}, æ„ŸçŸ¥æŸå¤±={self.config['perceptual_weight']}")

        # ä½¿ç”¨ transformers æä¾›çš„å¸¦é¢„çƒ­çš„è°ƒåº¦å™¨
        self.scheduler = get_scheduler(
            "cosine",
            optimizer=self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
        )
    
    def setup_perceptual_loss(self):
        """è®¾ç½®VGGæ„ŸçŸ¥æŸå¤±"""
        if not self.config['use_perceptual_loss']:
            print("ğŸš« æ„ŸçŸ¥æŸå¤±å·²å…³é—­ä»¥èŠ‚çœå†…å­˜")
            self.perceptual_net = None
            return
            
        try:
            from torchvision.models import vgg16, VGG16_Weights
            self.perceptual_net = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(self.device).float()
            for param in self.perceptual_net.parameters():
                param.requires_grad = False
            self.perceptual_net.eval()
            print("âœ… VGG16æ„ŸçŸ¥æŸå¤±å·²è®¾ç½® (ä½¿ç”¨å‰16å±‚)")
        except:
            print("âš ï¸  VGG16ä¸å¯ç”¨ï¼Œè·³è¿‡æ„ŸçŸ¥æŸå¤±")
            self.perceptual_net = None
    
    def compute_perceptual_loss(self, real, fake):
        """è®¡ç®—æ„ŸçŸ¥æŸå¤±"""
        if self.perceptual_net is None:
            return torch.tensor(0.0, device=self.device, dtype=torch.float32)
        
        self.perceptual_net.float()
        real = real.float()
        fake = fake.float()
        
        if real.size(1) == 1:
            real = real.repeat(1, 3, 1, 1)
        if fake.size(1) == 1:
            fake = fake.repeat(1, 3, 1, 1)
        
        with torch.no_grad():
            real_features = self.perceptual_net(real)
        
        fake_features = self.perceptual_net(fake)
        return F.mse_loss(real_features.float(), fake_features.float())
    
    def vae_loss(self, images):
        """VAEæŸå¤±å‡½æ•°è®¡ç®—"""
        try:
            images = images.float()
            posterior = self.vae.encode(images).latent_dist
            latents = posterior.sample().float()
            reconstructed = self.vae.decode(latents).sample.float()

            # é‡å»ºæŸå¤±
            recon_loss = self.mse_loss(reconstructed, images)
            
            # KLæ•£åº¦æŸå¤±
            try:
                kl_raw = posterior.kl()
                if torch.isnan(kl_raw).any() or torch.isinf(kl_raw).any():
                    print("âš ï¸ æ£€æµ‹åˆ°KLæ•£åº¦ä¸­çš„NaN/Infå€¼ï¼Œä½¿ç”¨æ›¿ä»£è®¡ç®—æ–¹æ³•")
                    mean, var = posterior.mean, posterior.var
                    eps = 1e-8
                    kl_loss = 0.5 * torch.mean(mean.pow(2) + var - torch.log(var + eps) - 1)
                else:
                    kl_loss = kl_raw.mean()
                if torch.isnan(kl_loss) or torch.isinf(kl_loss):
                    print("âš ï¸ KLæŸå¤±ä»ç„¶åŒ…å«NaN/Infï¼Œä½¿ç”¨å¸¸æ•°æ›¿ä»£")
                    kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
            except Exception as kl_e:
                print(f"âš ï¸ KLæ•£åº¦è®¡ç®—é”™è¯¯: {kl_e}ï¼Œä½¿ç”¨æ›¿ä»£å€¼")
                kl_loss = torch.tensor(0.1, device=self.device, dtype=torch.float32)
            
            # æ„ŸçŸ¥æŸå¤±
            if self.perceptual_net:
                perceptual_loss = self.compute_perceptual_loss(images, reconstructed)
            else:
                perceptual_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
            
            # æ€»æŸå¤±
            total_loss = (
                self.config['reconstruction_weight'] * recon_loss +
                self.config['kl_weight'] * kl_loss +
                self.config['perceptual_weight'] * perceptual_loss
            )
            
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print("âš ï¸ æ€»æŸå¤±åŒ…å«NaN/Infï¼Œä»…ä½¿ç”¨é‡å»ºæŸå¤±")
                total_loss = self.config['reconstruction_weight'] * recon_loss
                
            return {
                'total_loss': total_loss,
                'recon_loss': recon_loss,
                'kl_loss': kl_loss,
                'perceptual_loss': perceptual_loss,
                'reconstructed': reconstructed,
                'latents': latents
            }
        except Exception as e:
            print(f"ğŸ”¥ VAE_LOSSå†…éƒ¨å‘ç”Ÿé”™è¯¯ï¼Œç±»å‹: {type(e)}, å†…å®¹: {e} ğŸ”¥")
            import traceback
            traceback.print_exc()
            raise e
    
    def clear_memory(self):
        """æ¸…ç†å†…å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def train_epoch(self, epoch: int):
        """ä¼˜åŒ–çš„è®­ç»ƒepoch (ä½¿ç”¨æ›´å¥½çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥)"""
        self.vae.train()
        
        total_loss = 0
        total_recon = 0
        total_kl = 0
        total_perceptual = 0
        num_batches = 0
        
        self.optimizer.zero_grad()
        
        pbar = tqdm(self.train_loader, desc=f"Fine-tune Epoch {epoch+1}")
        
        for batch_idx, batch in enumerate(pbar):
            images, _ = batch
            images = images.to(self.device, non_blocking=True)
            
            if self.config['mixed_precision']:
                if AUTOCAST_DEVICE:
                    with autocast(AUTOCAST_DEVICE, dtype=torch.float16):
                        loss_dict = self.vae_loss(images)
                        loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                else:
                    with autocast():
                        loss_dict = self.vae_loss(images)
                        loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                
                self.scaler.scale(loss.float()).backward()
            else:
                loss_dict = self.vae_loss(images)
                loss = loss_dict['total_loss'] / self.config['gradient_accumulation_steps']
                loss.backward()
            
            if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:
                try:
                    if self.config['mixed_precision']:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                        self.optimizer.step()
                    
                    self.scheduler.step() # æ¯æ­¥æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                    self.optimizer.zero_grad()
                    
                except RuntimeError as grad_e:
                    if "dtype" in str(grad_e).lower() or "type" in str(grad_e).lower():
                        print(f"âš ï¸ æ¢¯åº¦æ­¥éª¤æ•°æ®ç±»å‹é”™è¯¯ï¼Œè·³è¿‡æ­¤æ­¥éª¤: {grad_e}")
                        self.optimizer.zero_grad()
                        continue
                    else:
                        raise grad_e
            
            actual_loss = loss.item() * self.config['gradient_accumulation_steps']
            total_loss += actual_loss
            total_recon += loss_dict['recon_loss'].item()
            total_kl += loss_dict['kl_loss'].item()
            total_perceptual += loss_dict['perceptual_loss'].item()
            num_batches += 1
            
            pbar.set_postfix({
                'Loss': f'{actual_loss:.4f}',
                'Recon': f'{loss_dict["recon_loss"].item():.4f}',
                'KL': f'{loss_dict["kl_loss"].item():.4f}',
                'Perc': f'{loss_dict["perceptual_loss"].item():.4f}'
            })
            
            if batch_idx == 0:
                self.save_reconstruction_samples(
                    images, loss_dict['reconstructed'], epoch
                )
            
            if batch_idx % 20 == 0:
                del images, loss_dict, loss
                self.clear_memory()
        
        if (len(self.train_loader)) % self.config['gradient_accumulation_steps'] != 0:
            if self.config['mixed_precision']:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                self.optimizer.step()
            
            self.scheduler.step() # ç¡®ä¿æœ€åä¸€æ¬¡æ›´æ–°å­¦ä¹ ç‡
            self.optimizer.zero_grad()
        
        avg_metrics = {
            'avg_loss': total_loss / num_batches,
            'avg_recon': total_recon / num_batches,
            'avg_kl': total_kl / num_batches,
            'avg_perceptual': total_perceptual / num_batches
        }
        
        self.clear_memory()
        return avg_metrics
    
    def save_reconstruction_samples(self, original, reconstructed, epoch):
        """ä¿å­˜é‡å»ºæ ·æœ¬"""
        try:
            def denormalize(tensor):
                tensor = tensor.clone().detach().cpu()
                tensor = tensor * 0.5 + 0.5  # åå½’ä¸€åŒ–
                tensor = tensor.clamp(0, 1)
                return tensor
            
            with torch.no_grad():
                # é€‰æ‹©æœ€å¤š8å¼ å›¾åƒç”¨äºå¯è§†åŒ–
                num_images = min(8, original.size(0))
                original = denormalize(original[:num_images])
                reconstructed = denormalize(reconstructed[:num_images])
                
                # åˆ›å»ºç½‘æ ¼
                grid = torch.zeros((3, original.size(2) * 2, original.size(3) * num_images))
                for i in range(num_images):
                    grid[:, :original.size(2), i*original.size(3):(i+1)*original.size(3)] = original[i]
                    grid[:, original.size(2):, i*original.size(3):(i+1)*original.size(3)] = reconstructed[i]
                
                # ä¿å­˜å›¾åƒ
                from torchvision.utils import save_image
                save_image(grid, f"{self.config['save_dir']}/recon_epoch_{epoch+1}.png")
                print(f"âœ… ä¿å­˜äº†é‡å»ºæ ·æœ¬ epoch {epoch+1}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜é‡å»ºæ ·æœ¬å¤±è´¥: {e}")
    
    @torch.no_grad()
    def evaluate_reconstruction_quality(self, val_loader):
        """è¯„ä¼°é‡å»ºè´¨é‡"""
        self.vae.eval()
        total_mse = 0
        total_samples = 0
        latent_means = []
        latent_stds = []
        
        for batch in tqdm(val_loader, desc="è¯„ä¼°é‡å»ºè´¨é‡"):
            images, _ = batch
            images = images.to(self.device)
            
            try:
                posterior = self.vae.encode(images).latent_dist
                latents = posterior.sample()
                reconstructions = self.vae.decode(latents).sample
            
                # è®¡ç®—MSE
                mse = F.mse_loss(reconstructions, images, reduction='none').mean([1, 2, 3])
                total_mse += mse.sum().item()
                total_samples += images.size(0)
            
                # æ”¶é›†æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯
                latent_means.append(posterior.mean.cpu().flatten(1).mean(0).numpy())
                latent_stds.append(posterior.var.sqrt().cpu().flatten(1).mean(0).numpy())
            except RuntimeError as e:
                print(f"âš ï¸ è¯„ä¼°ä¸­çš„CUDAé”™è¯¯: {e}")
                self.clear_memory()
        
        avg_mse = total_mse / total_samples if total_samples > 0 else float('inf')
        
        # æ±‡æ€»æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯
        latent_stats = {
            'mean': np.mean(latent_means, axis=0) if latent_means else None,
            'std': np.mean(latent_stds, axis=0) if latent_stds else None
        }
        
        print(f"ğŸ“Š éªŒè¯é›†å¹³å‡MSE: {avg_mse:.6f}")
        self.clear_memory()
        
        return avg_mse, latent_stats
    
    def save_finetuned_vae(self, epoch):
        """ä»¥diffuserså…¼å®¹çš„æ ¼å¼ä¿å­˜å¾®è°ƒåçš„VAEæ¨¡å‹"""
        try:
            # æ¨¡å‹å°†ä¿å­˜åˆ°ä¸€ä¸ªç›®å½•ä¸­, e.g., /.../vae_finetuned_epoch_3/
            model_path = os.path.join(self.config['save_dir'], f"vae_finetuned_epoch_{epoch+1}")
            self.vae.save_pretrained(model_path)
            print(f"ğŸ’¾ VAEæ¨¡å‹å·²ä¿å­˜åˆ°ç›®å½•: {model_path}")
            return model_path
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        plt.figure(figsize=(14, 10))
        axes = plt.subplot(2, 2, 1)
        plt.plot(self.train_history['epoch'], self.train_history['train_loss'])
        plt.title('æ€»æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        
        plt.subplot(2, 2, 2)
        plt.plot(self.train_history['epoch'], self.train_history['kl_loss'])
        plt.title('KLæ•£åº¦æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('KL Loss')
        plt.grid(True)
        
        plt.subplot(2, 2, 3)
        plt.plot(self.train_history['epoch'], self.train_history['perceptual_loss'])
        plt.title('æ„ŸçŸ¥æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Perceptual Loss')
        plt.grid(True)
        
        plt.subplot(2, 2, 4)
        if self.train_history['val_epoch']:
            plt.plot(self.train_history['val_epoch'], self.train_history['val_mse'])
            plt.title('éªŒè¯é›†MSE')
            plt.xlabel('Epoch')
            plt.ylabel('MSE')
            plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{self.config["save_dir"]}/training_history.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def finetune(self):
        """ä¼˜åŒ–ç‰ˆæœ¬çš„fine-tuningæµç¨‹ï¼Œæ·»åŠ æ›´å¥½çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
        print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆVAE Fine-tuningè®­ç»ƒ...")
        print("=" * 60)
        
        if torch.cuda.is_available():
            torch.autograd.set_detect_anomaly(True)
            print("âš ï¸ PyTorch å¼‚å¸¸æ£€æµ‹å·²å¯ç”¨ (ç”¨äºè°ƒè¯•ï¼Œå¯èƒ½å½±å“é€Ÿåº¦)")
        
        best_recon_loss = float('inf')
        
        for epoch in range(self.config['max_epochs']):
            print(f"\nğŸ“… Epoch {epoch+1}/{self.config['max_epochs']}")
            
            # è°ƒç”¨æ–°çš„train_epochæ–¹æ³•
            train_metrics = self.train_epoch(epoch)
            
            # è®°å½•è®­ç»ƒå†å²
            self.train_history['epoch'].append(epoch + 1)
            self.train_history['train_loss'].append(train_metrics['avg_loss'])
            self.train_history['recon_loss'].append(train_metrics['avg_recon'])
            self.train_history['kl_loss'].append(train_metrics['avg_kl'])
            self.train_history['perceptual_loss'].append(train_metrics['avg_perceptual'])
            
            # è¿›è¡ŒéªŒè¯
            if epoch % self.config['eval_every_epochs'] == 0 or epoch == self.config['max_epochs'] - 1:
                val_mse, latent_stats = self.evaluate_reconstruction_quality(self.val_loader)
                self.train_history['val_mse'].append(val_mse)
                self.train_history['val_epoch'].append(epoch + 1)
            
            # å­¦ä¹ ç‡æ˜¯æ¯æ­¥æ›´æ–°çš„ï¼Œè¿™é‡Œè·å–æœ€æ–°çš„å­¦ä¹ ç‡å€¼
            current_lr = self.optimizer.param_groups[0]['lr']
            
            print(f"ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"   æ€»æŸå¤±: {train_metrics['avg_loss']:.4f}")
            print(f"   é‡å»ºæŸå¤±: {train_metrics['avg_recon']:.4f}")
            print(f"   KLæŸå¤±: {train_metrics['avg_kl']:.4f}")
            print(f"   æ„ŸçŸ¥æŸå¤±: {train_metrics['avg_perceptual']:.4f}")
            print(f"   å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹é€»è¾‘
            is_best_model_save = False
            if train_metrics['avg_recon'] < best_recon_loss:
                best_recon_loss = train_metrics['avg_recon']
                
                # å°è¯•åˆ é™¤ä¸Šä¸€ä¸ªè¢«æ ‡è®°ä¸º"æœ€ä½³"çš„æ¨¡å‹ç›®å½•
                if self.best_model_checkpoint_path and os.path.isdir(self.best_model_checkpoint_path):
                    try:
                        import shutil
                        # ä»ç›®å½•åæå–æ—§çš„epochå·
                        old_epoch_str = self.best_model_checkpoint_path.split('_epoch_')[-1]
                        old_epoch_idx = int(old_epoch_str) - 1
                        
                        is_periodic_save = (old_epoch_idx % self.config['save_every_epochs'] == 0)
                        
                        if not is_periodic_save:
                            print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹ç›®å½•: {self.best_model_checkpoint_path}")
                            shutil.rmtree(self.best_model_checkpoint_path)
                        else:
                            print(f"â„¹ï¸ ä¿ç•™æ—§çš„æœ€ä½³æ¨¡å‹ (åŒæ—¶ä¹Ÿæ˜¯å®šæœŸä¿å­˜ç‚¹): {self.best_model_checkpoint_path}")
                    except Exception as e_remove:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æˆ–æ£€æŸ¥æ—§çš„æœ€ä½³æ¨¡å‹ç›®å½•: {e_remove}")

                saved_path = self.save_finetuned_vae(epoch)  # ä¿å­˜å½“å‰æ¨¡å‹
                if saved_path:
                    self.best_model_checkpoint_path = saved_path  # æ›´æ–°æœ€ä½³æ¨¡å‹è·¯å¾„
                    print(f"âœ… ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (é‡å»ºæŸå¤±: {best_recon_loss:.4f}): {self.best_model_checkpoint_path}")
                is_best_model_save = True
            
            if epoch % self.config['save_every_epochs'] == 0:
                if not is_best_model_save:  # å¦‚æœæ­¤epochå·²ä½œä¸ºæœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œåˆ™ä¸å†é‡å¤ä¿å­˜
                    self.save_finetuned_vae(epoch)
                self.plot_training_history()
            
            print(f"ğŸ† å½“å‰æœ€ä½³é‡å»ºæŸå¤±: {best_recon_loss:.4f}")
            self.clear_memory()
        
        # è®­ç»ƒå®Œæˆ
        print("\n" + "=" * 60)
        print("ğŸ‰ ä¼˜åŒ–ç‰ˆVAE Fine-tuningè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³é‡å»ºæŸå¤±: {best_recon_loss:.4f}")
        
        # æœ€ç»ˆè¯„ä¼°
        final_mse, final_latent_stats = self.evaluate_reconstruction_quality(self.val_loader)
        print(f"ğŸ¯ æœ€ç»ˆé‡å»ºMSE: {final_mse:.6f}")
        
        # ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒå†å²
        self.plot_training_history()
        
        return best_recon_loss

# ä¸»å‡½æ•°
def run_vae_training(images_dir=None, annotations_dir=None, output_dir=None):
    """è¿è¡ŒVAEå¾®è°ƒè®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹VAEå¾®è°ƒè®­ç»ƒ (é’ˆå¯¹P100 GPUä¼˜åŒ–)...")

    if torch.cuda.is_available():
        torch.cuda.set_per_process_memory_fraction(0.95)
        torch.cuda.empty_cache()
        print(f"ğŸ”§ CUDAå†…å­˜ä¼˜åŒ–è®¾ç½®å®Œæˆ")

    # é»˜è®¤è·¯å¾„è®¾ç½®
    if images_dir is None:
        images_dir = '/kaggle/input/dataset-test/images/images'
    if annotations_dir is None:
        annotations_dir = '/kaggle/input/dataset-test/annotations/annotations'
    if output_dir is None:
        output_dir = '/kaggle/working/vae_finetuned'

    print(f"ğŸ“Š æ•°æ®é›†è·¯å¾„é…ç½®:")
    print(f"  å›¾åƒç›®å½•: {images_dir}")
    print(f"  æ ‡æ³¨ç›®å½•: {annotations_dir}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")

    try:
        print("ğŸ’¡ ä½¿ç”¨P100 GPUä¼˜åŒ–é…ç½®...")
        trainer = VAEFineTuner(images_dir, annotations_dir, output_dir)
        best_loss = trainer.finetune()
        print(f"\nğŸ¯ VAEå¾®è°ƒå®Œæˆï¼Œæœ€ä½³é‡å»ºæŸå¤±: {best_loss:.4f}")
        return best_loss
    except Exception as e:
        print("ğŸ”¥ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:")
        import traceback
        traceback.print_exc()
        return None

# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if __name__ == "__main__":
    print("VAEå¾®è°ƒè„šæœ¬å·²åŠ è½½ï¼Œå¼€å§‹æ‰§è¡Œè®­ç»ƒ...")
    
    # Kaggleç¯å¢ƒé»˜è®¤è·¯å¾„
    images_dir = '/kaggle/input/dataset-test/images/images'
    annotations_dir = '/kaggle/input/dataset-test/annotations/annotations'
    output_dir = '/kaggle/working/vae_finetuned'
    
    # è‡ªåŠ¨å¼€å§‹è®­ç»ƒ
    run_vae_training(
        images_dir=images_dir,
        annotations_dir=annotations_dir,
        output_dir=output_dir
    )
