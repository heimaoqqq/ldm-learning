import torch
import torch.nn as nn
import os
import argparse
import glob
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms
from diffusers import AutoencoderKL
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
import shutil
from tqdm import tqdm

# --- Helper Functions and Classes ---

def check_and_install_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–ï¼Œå¹¶å¼ºåˆ¶ä½¿ç”¨ä¸Kaggleç¯å¢ƒå…¼å®¹çš„PyTorchå’ŒNumPyç‰ˆæœ¬ã€‚"""
    try:
        import torch
        import numpy as np
        import diffusers
        import torch_fidelity
        
        # éªŒè¯ç‰ˆæœ¬
        torch_ok = torch.__version__.startswith("2.1.2")
        # ç¡®ä¿NumPyç‰ˆæœ¬æ˜¯1.x
        numpy_ok = int(np.__version__.split('.')[0]) < 2
        
        if torch_ok and numpy_ok:
             print("âœ… æ‰€æœ‰ä¾èµ–å·²å®‰è£…ä¸”ç‰ˆæœ¬å…¼å®¹ã€‚")
             return
        else:
            # å¦‚æœç‰ˆæœ¬ä¸å¯¹ï¼Œè§¦å‘é‡æ–°å®‰è£…æµç¨‹
            error_msg = []
            if not torch_ok: error_msg.append(f"PyTorch (å½“å‰: {torch.__version__}, éœ€è¦: 2.1.2.x)")
            if not numpy_ok: error_msg.append(f"NumPy (å½“å‰: {np.__version__}, éœ€è¦: <2.0)")
            raise ImportError(f"ç‰ˆæœ¬ä¸åŒ¹é…: {', '.join(error_msg)}")

    except ImportError as e:
        print(f"ğŸ“¦ æ£€æµ‹åˆ°ä¾èµ–é—®é¢˜: {e}")
        print("   å°†å¼€å§‹ä¿®å¤æµç¨‹...")
        
        # å®šä¹‰å®‰è£…å‘½ä»¤
        # å¼ºåˆ¶å®‰è£… NumPy < 2.0 æ¥è§£å†³ ABI å…¼å®¹æ€§é—®é¢˜
        pytorch_install_cmd = "pip install --upgrade --force-reinstall torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 -q"
        other_deps_install_cmd = "pip install diffusers transformers accelerate scikit-image torch-fidelity tqdm numpy==1.26.4 -q"

        print("   ç¬¬ä¸€æ­¥: å¼ºåˆ¶é‡è£…ä¸Kaggle CUDA 12.1 å…¼å®¹çš„PyTorchç‰ˆæœ¬...")
        os.system(pytorch_install_cmd)
        
        print("   ç¬¬äºŒæ­¥: å®‰è£…å…¶ä»–å¿…è¦çš„åº“å¹¶å›ºå®šNumPyç‰ˆæœ¬åˆ°1.26.4...")
        os.system(other_deps_install_cmd)
        
        print("\n" + "="*50)
        print("âœ… ä¾èµ–ä¿®å¤å®Œæˆã€‚")
        print("ğŸ›‘ è¯·åŠ¡å¿…é‡æ–°è¿è¡Œæ­¤è„šæœ¬/å•å…ƒæ ¼ä»¥ä½¿æ›´æ”¹ç”Ÿæ•ˆã€‚")
        print("="*50)
        
        import sys
        sys.exit(0)

class KaggleOxfordPetDataset(torch.utils.data.Dataset):
    """
    é€‚é…Kaggleè·¯å¾„çš„Oxford-IIIT Petæ•°æ®é›†ç±»
    (ä»è®­ç»ƒè„šæœ¬ä¸­å¤åˆ¶ï¼Œç”¨äºåŠ è½½æ•°æ®)
    """
    def __init__(self, images_dir, annotation_file, transform=None):
        self.images_dir = images_dir
        self.transform = transform
        
        self.samples = []
        try:
            with open(annotation_file, 'r') as f:
                for line in f:
                    if line.startswith('#'):
                        continue
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        image_id = parts[0]
                        class_id = int(parts[1]) - 1
                        self.samples.append((image_id, class_id))
            print(f"åŠ è½½äº† {len(self.samples)} ä¸ªæ ·æœ¬")
        except FileNotFoundError:
            print(f"âŒ æ ‡æ³¨æ–‡ä»¶æœªæ‰¾åˆ°: {annotation_file}")
            # å¦‚æœæ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ç›´æ¥æ‰«æå›¾åƒç›®å½•
            image_files = glob.glob(os.path.join(images_dir, "*.jpg"))
            image_files.extend(glob.glob(os.path.join(images_dir, "*.png")))
            for img_path in image_files:
                image_id = os.path.splitext(os.path.basename(img_path))[0]
                self.samples.append((image_id, 0)) # ä½¿ç”¨é»˜è®¤ç±»åˆ«0
            print(f"âš ï¸  æ ‡æ³¨æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå·²ç›´æ¥ä»ç›®å½•åŠ è½½ {len(self.samples)} å¼ å›¾ç‰‡ã€‚")

    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        image_id, class_id = self.samples[idx]
        img_path = os.path.join(self.images_dir, f"{image_id}.jpg")
        if not os.path.exists(img_path):
            for ext in ['.jpeg', '.png', '.JPEG', '.PNG']:
                alt_path = os.path.join(self.images_dir, f"{image_id}{ext}")
                if os.path.exists(alt_path):
                    img_path = alt_path
                    break
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, class_id
        except Exception as e:
            print(f"é”™è¯¯åŠ è½½å›¾åƒ {img_path}: {e}")
            placeholder = torch.zeros((3, 256, 256))
            return placeholder, class_id

@torch.no_grad()
def test_vae(args):
    """ä¸»æµ‹è¯•å‡½æ•°"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒçš„ Stable Diffusion VAE ç»“æ„...")
    vae = AutoencoderKL.from_pretrained(
        "runwayml/stable-diffusion-v1-5", 
        subfolder="vae"
    )
    
    print(f"ğŸ’¾ åŠ è½½æ‚¨å¾®è°ƒè¿‡çš„VAEæƒé‡ä»: {args.checkpoint_path}")
    try:
        checkpoint = torch.load(args.checkpoint_path, map_location=device)
        if 'vae_state_dict' in checkpoint:
            vae.load_state_dict(checkpoint['vae_state_dict'])
        else:
            # å…¼å®¹ç›´æ¥ä¿å­˜state_dictçš„æƒ…å†µ
            vae.load_state_dict(checkpoint)
        print("âœ… æƒé‡åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"ğŸ”¥ åŠ è½½æƒé‡å¤±è´¥: {e}")
        return

    vae = vae.to(device).eval()

    # 2. å‡†å¤‡æ•°æ®
    transform = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # æ™ºèƒ½åˆ¤æ–­æ ‡æ³¨æ–‡ä»¶è·¯å¾„
    if os.path.basename(args.annotations_dir) == 'annotations':
        annotation_file = os.path.join(args.annotations_dir, "list.txt")
    else: # å‡è®¾ annotations_dir å°±æ˜¯æ–‡ä»¶æœ¬èº«
        annotation_file = args.annotations_dir

    print(f"ğŸ“ å‡†å¤‡æ•°æ®é›†...")
    dataset = KaggleOxfordPetDataset(
        images_dir=args.images_dir,
        annotation_file=annotation_file,
        transform=transform
    )
    
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=args.num_samples, shuffle=True
    )

    # 3. æ‰§è¡Œé‡å»ºå’Œè¯„ä¼°
    try:
        original_images, _ = next(iter(dataloader))
    except StopIteration:
        print("âŒ æ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼Œæ— æ³•è·å–å›¾åƒã€‚")
        return
        
    original_images = original_images.to(device)
    
    print("âœ¨ æ‰§è¡Œé‡å»º...")
    reconstructed_images = vae.decode(vae.encode(original_images).latent_dist.sample()).sample

    # åå½’ä¸€åŒ–ä»¥ä¾¿æ˜¾ç¤ºå’Œè®¡ç®—æŒ‡æ ‡
    def denormalize(tensor):
        return torch.clamp((tensor + 1) / 2, 0, 1)

    original_images_denorm = denormalize(original_images).cpu().permute(0, 2, 3, 1).numpy()
    reconstructed_images_denorm = denormalize(reconstructed_images).cpu().permute(0, 2, 3, 1).numpy()
    
    # 4. è®¡ç®—æŒ‡æ ‡
    print("\n--- è´¨é‡è¯„ä¼°æŒ‡æ ‡ ---")
    all_psnr = []
    all_ssim = []
    for i in range(args.num_samples):
        orig_img = original_images_denorm[i]
        recon_img = reconstructed_images_denorm[i]
        
        # data_range æ˜¯åƒç´ å€¼çš„èŒƒå›´
        current_psnr = psnr(orig_img, recon_img, data_range=1.0)
        current_ssim = ssim(orig_img, recon_img, data_range=1.0, channel_axis=-1, win_size=7)
        
        all_psnr.append(current_psnr)
        all_ssim.append(current_ssim)
        print(f"  æ ·æœ¬ {i+1}: PSNR = {current_psnr:.2f} dB, SSIM = {current_ssim:.4f}")
        
    print(f"------------------------")
    print(f"ğŸ“Š å¹³å‡å€¼: PSNR = {np.mean(all_psnr):.2f} dB, SSIM = {np.mean(all_ssim):.4f}")
    print("------------------------")
    print("ğŸ’¡ PSNRè¶Šé«˜è¶Šå¥½ (é€šå¸¸ > 30dB è¡¨ç¤ºä¸é”™)ã€‚SSIM è¶Šæ¥è¿‘1è¶Šå¥½ã€‚")


    # 5. å¯è§†åŒ–å¯¹æ¯”
    print(f"ğŸ–¼ï¸ ç”Ÿæˆå¯¹æ¯”å›¾åƒ: {args.output_path}")
    fig, axes = plt.subplots(2, args.num_samples, figsize=(args.num_samples * 3, 6))
    
    for i in range(args.num_samples):
        # åŸå›¾
        axes[0, i].imshow(original_images_denorm[i])
        axes[0, i].set_title(f'Original {i+1}')
        axes[0, i].axis('off')
        
        # é‡å»ºå›¾
        axes[1, i].imshow(reconstructed_images_denorm[i])
        axes[1, i].set_title(f'Recon {i+1}')
        axes[1, i].axis('off')
        
    plt.tight_layout()
    plt.savefig(args.output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    # 6. FIDè¯„ä¼° (å¦‚æœå¯ç”¨)
    if args.calc_fid:
        print("\n--- FID (FrÃ©chet Inception Distance) è¯„ä¼° ---")
        print(f"å‡†å¤‡ {args.num_fid_samples} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
        
        # å¯¼å…¥ torch_fidelity
        try:
            from torch_fidelity.metrics import calculate_metrics
        except ImportError:
            print("ğŸ”¥ torch-fidelity æœªå®‰è£…ï¼Œæ— æ³•è®¡ç®—FIDã€‚è¯·é‡æ–°è¿è¡Œè„šæœ¬ä»¥è‡ªåŠ¨å®‰è£…ã€‚")
            return

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        real_dir = './fid_real_images'
        recon_dir = './fid_recon_images'
        if os.path.exists(real_dir): shutil.rmtree(real_dir)
        if os.path.exists(recon_dir): shutil.rmtree(recon_dir)
        os.makedirs(real_dir, exist_ok=True)
        os.makedirs(recon_dir, exist_ok=True)

        try:
            # ç¡®ä¿æˆ‘ä»¬ä¸ä¼šè¯·æ±‚æ¯”æ•°æ®é›†æ›´å¤šçš„æ ·æœ¬
            num_fid_samples = min(args.num_fid_samples, len(dataset))
            if num_fid_samples < args.num_fid_samples:
                print(f"âš ï¸  è¯·æ±‚çš„FIDæ ·æœ¬æ•° ({args.num_fid_samples}) å¤§äºæ•°æ®é›†ä¸­çš„æ ·æœ¬æ•° ({len(dataset)})ã€‚")
                print(f"å°†ä½¿ç”¨ {num_fid_samples} ä¸ªæ ·æœ¬è¿›è¡Œè®¡ç®—ã€‚")

            # æˆ‘ä»¬éœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œå­é›†é‡‡æ ·
            fid_subset = torch.utils.data.Subset(dataset, list(range(num_fid_samples)))

            fid_loader = torch.utils.data.DataLoader(
                fid_subset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=2
            )

            # ä¿å­˜å›¾åƒåˆ°ç›®å½•
            img_counter = 0
            pbar = tqdm(fid_loader, desc="ç”ŸæˆFIDè¯„ä¼°æ ·æœ¬")
            for batch_images, _ in pbar:
                batch_images = batch_images.to(device)
                
                # é‡å»º
                recon_batch = vae.decode(vae.encode(batch_images).latent_dist.sample()).sample
                
                # åå½’ä¸€åŒ–
                orig_denorm_batch = denormalize(batch_images).cpu()
                recon_denorm_batch = denormalize(recon_batch).cpu()

                for i in range(batch_images.size(0)):
                    orig_pil = transforms.ToPILImage()(orig_denorm_batch[i])
                    recon_pil = transforms.ToPILImage()(recon_denorm_batch[i])
                    
                    orig_pil.save(os.path.join(real_dir, f'img_{img_counter:05d}.png'))
                    recon_pil.save(os.path.join(recon_dir, f'img_{img_counter:05d}.png'))
                    
                    img_counter += 1
            
            print(f"âœ… å·²ç”Ÿæˆ {img_counter} å¼ çœŸå®å›¾åƒå’Œé‡å»ºå›¾åƒã€‚")
            print("â³ å¼€å§‹è®¡ç®—FIDåˆ†æ•°...")
            
            metrics = calculate_metrics(
                input1=real_dir, 
                input2=recon_dir, 
                cuda=torch.cuda.is_available(), 
                fid=True,
                verbose=False
            )
            
            fid_score = metrics['frechet_inception_distance']
            print(f"------------------------")
            print(f"ğŸ“Š FID åˆ†æ•°: {fid_score:.4f}")
            print("------------------------")
            print("ğŸ’¡ FIDè¶Šä½è¶Šå¥½ã€‚è¡¨ç¤ºé‡å»ºå›¾åƒçš„åˆ†å¸ƒä¸çœŸå®å›¾åƒçš„åˆ†å¸ƒæ›´æ¥è¿‘ã€‚")

        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            if os.path.exists(real_dir): shutil.rmtree(real_dir)
            if os.path.exists(recon_dir): shutil.rmtree(recon_dir)
            print("âœ… æ¸…ç†å®Œæˆã€‚")

    print("âœ… æµ‹è¯•å®Œæˆ!")


if __name__ == '__main__':
    # é¦–å…ˆæ£€æŸ¥ä¾èµ–
    check_and_install_dependencies()

    # --- ä¸ºKaggleç¯å¢ƒå®šåˆ¶çš„æ‰¹é‡æµ‹è¯•é…ç½® ---
    # æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æˆ–ä¿®æ”¹è¦æµ‹è¯•çš„æ¨¡å‹
    test_configs = [
        {
            "checkpoint_path": "/kaggle/input/fid-test/vae_finetuned_epoch_1.pth",
            "output_path": "./epoch_1_quality_report.png",
            "description": "ç¬¬1è½®è®­ç»ƒçš„æ¨¡å‹"
        },
        {
            "checkpoint_path": "/kaggle/input/fid-test/vae_finetuned_epoch_19.pth",
            "output_path": "./epoch_19_quality_report.png",
            "description": "ç¬¬19è½®è®­ç»ƒçš„æ¨¡å‹"
        }
    ]

    # --- é€šç”¨å‚æ•° ---
    # è¯·ç¡®ä¿è¿™äº›è·¯å¾„æŒ‡å‘æ‚¨çš„æ•°æ®é›†
    base_args = {
        "images_dir": "/kaggle/input/dataset-test/images/images",
        "annotations_dir": "/kaggle/input/dataset-test/annotations/annotations",
        "image_size": 256,
        "num_samples": 8,      # å¯è§†åŒ–å¯¹æ¯”çš„æ ·æœ¬æ•°
        "calc_fid": True,      # è‡ªåŠ¨è®¡ç®—FID
        "num_fid_samples": 1000, # ç”¨äºFIDè®¡ç®—çš„æ ·æœ¬æ•°
        "batch_size": 16,      # å¤„ç†FIDæ ·æœ¬æ—¶çš„æ‰¹æ¬¡å¤§å°
    }
    
    # --- å¼€å§‹æ‰§è¡Œæ‰¹é‡æµ‹è¯• ---
    for i, config in enumerate(test_configs):
        print("\n" + "="*60)
        print(f"ğŸ”¬ å¼€å§‹æµ‹è¯•é…ç½® #{i+1}: {config['description']}")
        print(f"   æ¨¡å‹è·¯å¾„: {config['checkpoint_path']}")
        print("="*60 + "\n")

        # åˆå¹¶é€šç”¨å‚æ•°å’Œå½“å‰æµ‹è¯•é…ç½®
        current_args_dict = {**base_args, **config}
        
        # å°†å­—å…¸è½¬æ¢ä¸º argparse.Namespace å¯¹è±¡ï¼Œä»¥å…¼å®¹ test_vae å‡½æ•°
        current_args = argparse.Namespace(**current_args_dict)
        
        try:
            test_vae(current_args)
        except Exception as e:
            print(f"ğŸ”¥ æµ‹è¯•é…ç½® #{i+1} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    print("\n" + "*"*60)
    print("ğŸ‰ æ‰€æœ‰è¯„ä¼°ä»»åŠ¡å·²å®Œæˆ!")
    print("è¯·åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šå›¾ç‰‡ã€‚")
    print("*"*60) 
