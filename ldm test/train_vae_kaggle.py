#!/usr/bin/env python3
"""
åœ¨Kaggleä¸Šè®­ç»ƒVAEæ¨¡å‹
ä¸ºP100 GPU (16GBæ˜¾å­˜)ä¼˜åŒ–
"""

import os
import sys
import argparse
import yaml
import time
import importlib
import torch
from omegaconf import OmegaConf
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# æ·»åŠ kaggleå·¥ä½œç›®å½•åˆ°è·¯å¾„
kaggle_working = "/kaggle/working"
if os.path.exists(kaggle_working) and kaggle_working not in sys.path:
    sys.path.insert(0, kaggle_working)

# æ·»åŠ latent-diffusionåˆ°ç³»ç»Ÿè·¯å¾„
LATENT_DIFFUSION_PATH = "/kaggle/working/latent-diffusion"
sys.path.append(LATENT_DIFFUSION_PATH)

# å¯¼å…¥VAEæ¨¡å—
try:
    from pet_dataset import PetDatasetTrain, PetDatasetValidation
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥pet_datasetæ¨¡å—: {e}")
    print(f"æœç´¢è·¯å¾„: {sys.path}")
    sys.exit(1)

# ä»ldmæ¨¡å—å¯¼å…¥
try:
    from ldm.util import instantiate_from_config
    from ldm.models.autoencoder import AutoencoderKL
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥ldmæ¨¡å—: {e}")
    print(f"å°è¯•ä» {LATENT_DIFFUSION_PATH} å¯¼å…¥")
    print(f"æœç´¢è·¯å¾„: {sys.path}")
    
    # å°è¯•æ£€æŸ¥ldmç›®å½•å†…å®¹
    ldm_dir = os.path.join(LATENT_DIFFUSION_PATH, "ldm")
    if os.path.exists(ldm_dir):
        print(f"ldmç›®å½•å­˜åœ¨ï¼Œå†…å®¹:")
        for root, dirs, files in os.walk(ldm_dir, topdown=True, maxdepth=2):
            print(f"- {root}")
            for f in files:
                if f.endswith(".py"):
                    print(f"  - {f}")
    else:
        print(f"ldmç›®å½•ä¸å­˜åœ¨: {ldm_dir}")
    
    sys.exit(1)

# ç¡®ä¿mainæ¨¡å—å­˜åœ¨
try:
    import main
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥mainæ¨¡å—ï¼Œå°è¯•åˆ›å»ºé»˜è®¤å®ç°...")
    
    # æ·»åŠ å¿…è¦çš„mainå‡½æ•°åˆ°å½“å‰æ¨¡å—
    def instantiate_from_config(config):
        if not "target" in config:
            raise KeyError("Expected key `target` to instantiate.")
        return get_obj_from_str(config["target"])(**config.get("params", dict()))

    def get_obj_from_str(string, reload=False):
        module, cls = string.rsplit(".", 1)
        if reload:
            module_imp = importlib.import_module(module)
            importlib.reload(module_imp)
        return getattr(importlib.import_module(module, package=None), cls)

    class DataModuleFromConfig(pl.LightningDataModule):
        def __init__(self, batch_size, train=None, validation=None, test=None,
                    wrap=False, num_workers=None):
            super().__init__()
            self.batch_size = batch_size
            self.dataset_configs = dict()
            self.num_workers = num_workers if num_workers is not None else batch_size*2
            if train is not None:
                self.dataset_configs["train"] = train
                self.train_dataloader = self._train_dataloader
            if validation is not None:
                self.dataset_configs["validation"] = validation
                self.val_dataloader = self._val_dataloader

            self.wrap = wrap

        def prepare_data(self):
            self.datasets = dict(
                (k, instantiate_from_config(self.dataset_configs[k]))
                for k in self.dataset_configs)

        def setup(self, stage=None):
            if self.wrap:
                for k in self.datasets:
                    self.datasets[k] = WrappedDataset(self.datasets[k])

        def _train_dataloader(self):
            return torch.utils.data.DataLoader(
                self.datasets["train"],
                batch_size=self.batch_size,
                num_workers=self.num_workers,
                shuffle=True
            )

        def _val_dataloader(self):
            return torch.utils.data.DataLoader(
                self.datasets["validation"],
                batch_size=self.batch_size,
                num_workers=self.num_workers,
                shuffle=False
            )

    class ImageLogger:
        def __init__(self, batch_frequency=1000, max_images=8, increase_log_steps=True):
            self.batch_freq = batch_frequency
            self.max_images = max_images
            self.log_steps = [batch_frequency]
            self.increase_log_steps = increase_log_steps
            
        def __call__(self, pl_module, batch, batch_idx, split="train"):
            pass  # ç®€åŒ–ç‰ˆå®ç°

    # æ·»åŠ åˆ°mainæ¨¡å—
    main = type('main', (), {})
    main.instantiate_from_config = instantiate_from_config
    main.DataModuleFromConfig = DataModuleFromConfig
    main.ImageLogger = ImageLogger
    sys.modules['main'] = main
    
    print("âœ“ å·²åˆ›å»ºmainæ¨¡å—çš„é»˜è®¤å®ç°")

def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config", 
        type=str, 
        default="/kaggle/working/VAE/vae_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--data_root", 
        type=str, 
        default="/kaggle/input/the-oxfordiiit-pet-dataset/images",
        help="æ•°æ®é›†æ ¹ç›®å½•"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=64,
        help="æ‰¹æ¬¡å¤§å°"
    )
    parser.add_argument(
        "--image_size", 
        type=int, 
        default=256,
        help="å›¾åƒå¤§å°"
    )
    parser.add_argument(
        "--max_epochs", 
        type=int, 
        default=100,
        help="æœ€å¤§è®­ç»ƒè½®æ•°"
    )
    parser.add_argument(
        "--precision", 
        type=str, 
        default="16-mixed",
        help="è®­ç»ƒç²¾åº¦ï¼Œå¯é€‰: 32, 16-mixed"
    )
    parser.add_argument(
        "--resume", 
        type=str, 
        default=None,
        help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„"
    )
    parser.add_argument(
        "--seed", 
        type=int, 
        default=42,
        help="éšæœºç§å­"
    )
    parser.add_argument(
        "--logdir", 
        type=str, 
        default="/kaggle/working/VAE/logs",
        help="æ—¥å¿—ç›®å½•"
    )
    parser.add_argument(
        "--debug", 
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )
    return parser

def main():
    """ä¸»å‡½æ•°"""
    parser = get_parser()
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.logdir, exist_ok=True)
    checkpoint_dir = os.path.join(args.logdir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºæ›´å¤šä¿¡æ¯
    if args.debug:
        print("\nè°ƒè¯•ä¿¡æ¯:")
        print(f"Pythonè·¯å¾„: {sys.path}")
        print(f"å½“å‰ç›®å½•: {os.getcwd()}")
        print(f"latent-diffusionè·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.exists(LATENT_DIFFUSION_PATH)}")
        print(f"é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(args.config)}")
        print(f"æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(args.data_root)}")
        
        # å°è¯•åˆ—å‡ºæ•°æ®ç›®å½•å†…å®¹
        if os.path.exists(args.data_root):
            print(f"\næ•°æ®ç›®å½•å†…å®¹ ({args.data_root}):")
            try:
                files = os.listdir(args.data_root)
                print(f"å‘ç° {len(files)} ä¸ªæ–‡ä»¶/ç›®å½•")
                print(f"å‰10ä¸ª: {files[:10]}")
            except Exception as e:
                print(f"åˆ—å‡ºç›®å½•å†…å®¹æ—¶å‡ºé”™: {e}")
        else:
            print(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_root}")
    
    # åŠ è½½é…ç½®
    try:
        config = OmegaConf.load(args.config)
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        print(f"æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„: {args.config}")
        sys.exit(1)
    
    # ä¿®æ”¹é…ç½®
    config.data.params.batch_size = args.batch_size
    if "size" in config.data.params.train.params:
        config.data.params.train.params.size = args.image_size
    if "size" in config.data.params.validation.params:
        config.data.params.validation.params.size = args.image_size
    
    # è®¾ç½®æ•°æ®é›†è·¯å¾„
    config.data.params.train.params.data_root = args.data_root
    config.data.params.validation.params.data_root = args.data_root
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒVAE (Kaggle P100 GPUä¼˜åŒ–)")
    print("=" * 60)
    print(f"æ•°æ®é›†è·¯å¾„: {args.data_root}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å›¾åƒå¤§å°: {args.image_size}x{args.image_size}")
    print(f"è®­ç»ƒè½®æ•°: {args.max_epochs}")
    print(f"ç²¾åº¦: {args.precision}")
    print(f"è¾“å‡ºç›®å½•: {args.logdir}")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ä½¿ç”¨GPU: {device_name} ({vram:.1f}GB)")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆéå¸¸æ…¢ï¼‰")
    
    # åˆ›å»ºæ¨¡å‹
    try:
        model = instantiate_from_config(config.model)
        print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸ: {type(model).__name__}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºé…ç½®æ–‡ä»¶ä¸ldmå®ç°ä¸åŒ¹é…å¯¼è‡´çš„")
        print("è¯·æ£€æŸ¥latent-diffusionåº“çš„ç‰ˆæœ¬å’Œé…ç½®æ–‡ä»¶çš„å…¼å®¹æ€§")
        sys.exit(1)
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    try:
        data = instantiate_from_config(config.data)
        data.prepare_data()
        data.setup()
        print(f"æ•°æ®æ¨¡å—åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒé›†å¤§å°: {len(data.datasets['train'])}, éªŒè¯é›†å¤§å°: {len(data.datasets['validation'])}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ•°æ®æ¨¡å—å¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºæ•°æ®è·¯å¾„é”™è¯¯æˆ–æ ¼å¼ä¸æ­£ç¡®å¯¼è‡´çš„")
        sys.exit(1)
    
    # åˆ›å»ºå›è°ƒ
    callbacks = []
    
    # æ·»åŠ æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="vae-{epoch:02d}-{val/rec_loss:.4f}",
        save_top_k=3,
        monitor="val/rec_loss",
        mode="min",
        save_last=True,
    )
    callbacks.append(checkpoint_callback)
    
    # æ·»åŠ å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval="step")
    callbacks.append(lr_monitor)
    
    # æ·»åŠ å›¾åƒè®°å½•å™¨
    if "image_logger" in config.lightning.callbacks:
        try:
            image_logger_config = config.lightning.callbacks.image_logger
            image_logger = instantiate_from_config(image_logger_config)
            callbacks.append(image_logger)
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºå›¾åƒè®°å½•å™¨å¤±è´¥: {e}")
            print("å°†ç»§ç»­è®­ç»ƒï¼Œä½†ä¸ä¼šè®°å½•é‡å»ºå›¾åƒ")
    
    # åˆ›å»ºæ—¥å¿—å™¨
    logger = TensorBoardLogger(
        save_dir=args.logdir,
        name="",
    )
    
    # è®­ç»ƒå™¨é…ç½®
    trainer_config = {
        "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
        "devices": 1 if torch.cuda.is_available() else 0,
        "precision": args.precision,
        "max_epochs": args.max_epochs,
        "log_every_n_steps": 50,
        "num_sanity_val_steps": 2,
        "check_val_every_n_epoch": 5,
        "benchmark": True,
    }
    
    # æ·»åŠ é…ç½®æ–‡ä»¶ä¸­çš„å…¶ä»–é…ç½®
    if "trainer" in config.lightning:
        for k, v in config.lightning.trainer.items():
            if k not in trainer_config:
                trainer_config[k] = v
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        logger=logger,
        callbacks=callbacks,
        **trainer_config,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    try:
        if args.resume:
            if os.path.exists(args.resume):
                print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
                trainer.fit(model, data, ckpt_path=args.resume)
            else:
                print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.resume}")
                sys.exit(1)
        else:
            trainer.fit(model, data)
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        training_time = time.time() - start_time
        hours, remainder = divmod(training_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        print(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’")
        
        # æ‰“å°æœ€ä½³æ¨¡å‹è·¯å¾„
        if hasattr(checkpoint_callback, "best_model_path") and checkpoint_callback.best_model_path:
            print(f"æœ€ä½³æ¨¡å‹: {checkpoint_callback.best_model_path}")
            print(f"æœ€ä½³æ€§èƒ½: {checkpoint_callback.best_model_score:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(args.logdir, "vae_final.ckpt")
        trainer.save_checkpoint(final_model_path)
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 
