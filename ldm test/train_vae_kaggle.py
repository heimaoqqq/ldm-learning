#!/usr/bin/env python3
"""
åœ¨Kaggleä¸Šè®­ç»ƒVAEæ¨¡å‹
ä¸ºP100 GPU (16GBæ˜¾å­˜)ä¼˜åŒ–
"""

import os
import sys
import argparse
import yaml
import time
import torch
from omegaconf import OmegaConf
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger

# æ·»åŠ latent-diffusionåˆ°ç³»ç»Ÿè·¯å¾„
LATENT_DIFFUSION_PATH = "/kaggle/working/latent-diffusion"
sys.path.append(LATENT_DIFFUSION_PATH)

# å¯¼å…¥VAEæ¨¡å—
from VAE.pet_dataset import PetDatasetTrain, PetDatasetValidation

# ä»ldmæ¨¡å—å¯¼å…¥
try:
    from ldm.util import instantiate_from_config
    from ldm.models.autoencoder import AutoencoderKL
except ImportError:
    print(f"âŒ æ— æ³•å¯¼å…¥ldmæ¨¡å—ï¼Œè¯·ç¡®è®¤latent-diffusionè·¯å¾„: {LATENT_DIFFUSION_PATH}")
    sys.exit(1)

def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config", 
        type=str, 
        default="/kaggle/working/VAE/vae_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--data_root", 
        type=str, 
        default="/kaggle/input/the-oxfordiiit-pet-dataset/images",
        help="æ•°æ®é›†æ ¹ç›®å½•"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=64,
        help="æ‰¹æ¬¡å¤§å°"
    )
    parser.add_argument(
        "--image_size", 
        type=int, 
        default=256,
        help="å›¾åƒå¤§å°"
    )
    parser.add_argument(
        "--max_epochs", 
        type=int, 
        default=100,
        help="æœ€å¤§è®­ç»ƒè½®æ•°"
    )
    parser.add_argument(
        "--precision", 
        type=str, 
        default="16-mixed",
        help="è®­ç»ƒç²¾åº¦ï¼Œå¯é€‰: 32, 16-mixed"
    )
    parser.add_argument(
        "--resume", 
        type=str, 
        default=None,
        help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„"
    )
    parser.add_argument(
        "--seed", 
        type=int, 
        default=42,
        help="éšæœºç§å­"
    )
    parser.add_argument(
        "--logdir", 
        type=str, 
        default="/kaggle/working/VAE/logs",
        help="æ—¥å¿—ç›®å½•"
    )
    return parser

def main():
    """ä¸»å‡½æ•°"""
    parser = get_parser()
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.logdir, exist_ok=True)
    checkpoint_dir = os.path.join(args.logdir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.config)
    
    # ä¿®æ”¹é…ç½®
    config.data.params.batch_size = args.batch_size
    if "size" in config.data.params.train.params:
        config.data.params.train.params.size = args.image_size
    if "size" in config.data.params.validation.params:
        config.data.params.validation.params.size = args.image_size
    
    # è®¾ç½®æ•°æ®é›†è·¯å¾„
    config.data.params.train.params.data_root = args.data_root
    config.data.params.validation.params.data_root = args.data_root
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒVAE (Kaggle P100 GPUä¼˜åŒ–)")
    print("=" * 60)
    print(f"æ•°æ®é›†è·¯å¾„: {args.data_root}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å›¾åƒå¤§å°: {args.image_size}x{args.image_size}")
    print(f"è®­ç»ƒè½®æ•°: {args.max_epochs}")
    print(f"ç²¾åº¦: {args.precision}")
    print(f"è¾“å‡ºç›®å½•: {args.logdir}")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ä½¿ç”¨GPU: {device_name} ({vram:.1f}GB)")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆéå¸¸æ…¢ï¼‰")
    
    # åˆ›å»ºæ¨¡å‹
    model = instantiate_from_config(config.model)
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸ: {type(model).__name__}")
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data = instantiate_from_config(config.data)
    data.prepare_data()
    data.setup()
    print(f"æ•°æ®æ¨¡å—åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒé›†å¤§å°: {len(data.datasets['train'])}, éªŒè¯é›†å¤§å°: {len(data.datasets['validation'])}")
    
    # åˆ›å»ºå›è°ƒ
    callbacks = []
    
    # æ·»åŠ æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="vae-{epoch:02d}-{val/rec_loss:.4f}",
        save_top_k=3,
        monitor="val/rec_loss",
        mode="min",
        save_last=True,
    )
    callbacks.append(checkpoint_callback)
    
    # æ·»åŠ å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval="step")
    callbacks.append(lr_monitor)
    
    # æ·»åŠ å›¾åƒè®°å½•å™¨
    if "image_logger" in config.lightning.callbacks:
        image_logger_config = config.lightning.callbacks.image_logger
        image_logger = instantiate_from_config(image_logger_config)
        callbacks.append(image_logger)
    
    # åˆ›å»ºæ—¥å¿—å™¨
    logger = TensorBoardLogger(
        save_dir=args.logdir,
        name="",
    )
    
    # è®­ç»ƒå™¨é…ç½®
    trainer_config = {
        "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
        "devices": 1 if torch.cuda.is_available() else 0,
        "precision": args.precision,
        "max_epochs": args.max_epochs,
        "log_every_n_steps": 50,
        "num_sanity_val_steps": 2,
        "check_val_every_n_epoch": 5,
        "benchmark": True,
    }
    
    # æ·»åŠ é…ç½®æ–‡ä»¶ä¸­çš„å…¶ä»–é…ç½®
    if "trainer" in config.lightning:
        for k, v in config.lightning.trainer.items():
            if k not in trainer_config:
                trainer_config[k] = v
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        logger=logger,
        callbacks=callbacks,
        **trainer_config,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    if args.resume:
        if os.path.exists(args.resume):
            print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
            trainer.fit(model, data, ckpt_path=args.resume)
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.resume}")
            sys.exit(1)
    else:
        trainer.fit(model, data)
    
    # è®¡ç®—è®­ç»ƒæ—¶é—´
    training_time = time.time() - start_time
    hours, remainder = divmod(training_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    print(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’")
    
    # æ‰“å°æœ€ä½³æ¨¡å‹è·¯å¾„
    if hasattr(checkpoint_callback, "best_model_path") and checkpoint_callback.best_model_path:
        print(f"æœ€ä½³æ¨¡å‹: {checkpoint_callback.best_model_path}")
        print(f"æœ€ä½³æ€§èƒ½: {checkpoint_callback.best_model_score:.4f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(args.logdir, "vae_final.ckpt")
    trainer.save_checkpoint(final_model_path)
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

if __name__ == "__main__":
    main() 
