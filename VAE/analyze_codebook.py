import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import yaml
from adv_vq_vae import AdvVQVAE
from dataset import build_dataloader
from collections import Counter
import seaborn as sns

def analyze_codebook_usage():
    """è¯¦ç»†åˆ†æVQ-VAEç æœ¬çš„ä½¿ç”¨æƒ…å†µ"""
    
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    train_loader, val_loader, train_len, val_len = build_dataloader(
        config['dataset']['root_dir'],
        batch_size=config['dataset']['batch_size'],
        num_workers=0,
        shuffle_train=False,
        shuffle_val=False,
        val_split=0.3
    )
    
    # å®šä¹‰è¦åˆ†æçš„æ¨¡å‹
    model_configs = [
        {
            'name': 'é¢„è®­ç»ƒFIDæ¨¡å‹',
            'filename': '/kaggle/input/vae-best-fid2/adv_vqvae_best_fid2.pth',
            'short_name': 'pretrained_fid'
        }
    ]
    
    # åˆ†ææ¯ä¸ªå¯ç”¨çš„æ¨¡å‹
    analysis_results = {}
    
    for model_config in model_configs:
        model_path = model_config['filename']
        if not os.path.exists(model_path):
            print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")
            continue
            
        print(f"\n{'='*60}")
        print(f"ğŸ“Š åˆ†ææ¨¡å‹: {model_config['name']}")
        print(f"{'='*60}")
        
        # åŠ è½½æ¨¡å‹
        model = AdvVQVAE(
            in_channels=config['model']['in_channels'],
            latent_dim=config['model']['latent_dim'],
            num_embeddings=config['model']['num_embeddings'],
            beta=config['model']['beta'],
            decay=config['model'].get('vq_ema_decay', 0.99),
            groups=config['model']['groups'],
            disc_ndf=64
        ).to(device)
        
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
        
        # æ”¶é›†ç¼–ç ç´¢å¼•
        print("ğŸ” æ”¶é›†ç¼–ç ç´¢å¼•...")
        all_indices = []
        sample_count = 0
        max_samples = 1000  # åˆ†æ1000ä¸ªæ ·æœ¬
        
        with torch.no_grad():
            # åˆ†æè®­ç»ƒé›†
            for images, _ in train_loader:
                if sample_count >= max_samples:
                    break
                    
                images = images.to(device)
                z = model.encoder(images)
                
                # ç›´æ¥è®¡ç®—ç¼–ç ç´¢å¼•ï¼Œä¸ä¾èµ–äºusage_info
                z_flattened = z.permute(0,2,3,1).contiguous().view(-1, model.vq.embedding_dim)
                dist = (z_flattened.pow(2).sum(1, keepdim=True)
                        - 2 * torch.matmul(z_flattened, model.vq.embedding.weight.t())
                        + model.vq.embedding.weight.pow(2).sum(1))
                encoding_indices = torch.argmin(dist, dim=1)
                
                indices = encoding_indices.cpu().numpy()
                all_indices.extend(indices.flatten())
                sample_count += images.size(0)
            
            # åˆ†æéªŒè¯é›†
            val_sample_count = 0
            for images, _ in val_loader:
                if val_sample_count >= 200:  # éªŒè¯é›†åˆ†æ200ä¸ªæ ·æœ¬
                    break
                    
                images = images.to(device)
                z = model.encoder(images)
                
                # ç›´æ¥è®¡ç®—ç¼–ç ç´¢å¼•
                z_flattened = z.permute(0,2,3,1).contiguous().view(-1, model.vq.embedding_dim)
                dist = (z_flattened.pow(2).sum(1, keepdim=True)
                        - 2 * torch.matmul(z_flattened, model.vq.embedding.weight.t())
                        + model.vq.embedding.weight.pow(2).sum(1))
                encoding_indices = torch.argmin(dist, dim=1)
                
                indices = encoding_indices.cpu().numpy()
                all_indices.extend(indices.flatten())
                val_sample_count += images.size(0)
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†åˆ°äº†ç´¢å¼•
        if len(all_indices) == 0:
            print("âŒ æœªèƒ½æ”¶é›†åˆ°ä»»ä½•ç¼–ç ç´¢å¼•ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
            continue
            
        # ç»Ÿè®¡åˆ†æ
        print(f"ğŸ“ˆ æ€»å…±åˆ†æäº† {len(all_indices)} ä¸ªç¼–ç ")
        
        # åŸºæœ¬ç»Ÿè®¡
        unique_indices = set(all_indices)
        total_codes = config['model']['num_embeddings']
        usage_rate = len(unique_indices) / total_codes
        
        print(f"ğŸ“Š ç æœ¬ä½¿ç”¨ç»Ÿè®¡:")
        print(f"  æ€»ç å­—æ•°: {total_codes}")
        print(f"  ä½¿ç”¨çš„ç å­—æ•°: {len(unique_indices)}")
        print(f"  ä½¿ç”¨ç‡: {usage_rate:.2%}")
        
        # ä½¿ç”¨é¢‘ç‡åˆ†æ
        usage_counts = Counter(all_indices)
        most_common = usage_counts.most_common(10)
        least_common = usage_counts.most_common()[-10:]
        
        print(f"\nğŸ“ˆ ä½¿ç”¨é¢‘ç‡åˆ†æ:")
        print(f"  æœ€å¸¸ç”¨çš„10ä¸ªç å­—:")
        for idx, count in most_common:
            percentage = count / len(all_indices) * 100
            print(f"    ç å­— {idx}: ä½¿ç”¨ {count} æ¬¡ ({percentage:.2f}%)")
        
        print(f"  æœ€å°‘ç”¨çš„10ä¸ªç å­—:")
        for idx, count in least_common:
            percentage = count / len(all_indices) * 100
            print(f"    ç å­— {idx}: ä½¿ç”¨ {count} æ¬¡ ({percentage:.2f}%)")
        
        # ç»Ÿè®¡åˆ†å¸ƒåˆ†æ
        counts_array = np.array(list(usage_counts.values()))
        
        # æ£€æŸ¥counts_arrayæ˜¯å¦ä¸ºç©º
        if len(counts_array) == 0:
            print(f"\nâš ï¸ è­¦å‘Š: æ²¡æœ‰æ”¶é›†åˆ°ç å­—ä½¿ç”¨ç»Ÿè®¡ï¼Œè·³è¿‡ç»Ÿè®¡åˆ†æ")
            continue
            
        print(f"\nğŸ“ˆ ä½¿ç”¨åˆ†å¸ƒç»Ÿè®¡:")
        print(f"  å¹³å‡ä½¿ç”¨æ¬¡æ•°: {counts_array.mean():.2f}")
        print(f"  ä½¿ç”¨æ¬¡æ•°æ ‡å‡†å·®: {counts_array.std():.2f}")
        print(f"  ä½¿ç”¨æ¬¡æ•°ä¸­ä½æ•°: {np.median(counts_array):.2f}")
        print(f"  æœ€å¤§ä½¿ç”¨æ¬¡æ•°: {counts_array.max()}")
        print(f"  æœ€å°ä½¿ç”¨æ¬¡æ•°: {counts_array.min()}")
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_results[model_config['short_name']] = {
            'usage_rate': usage_rate,
            'used_codes': len(unique_indices),
            'total_codes': total_codes,
            'usage_counts': usage_counts,
            'counts_array': counts_array,
            'all_indices': all_indices
        }
        
        # ç”Ÿæˆå¯è§†åŒ–
        os.makedirs("codebook_analysis", exist_ok=True)
        
        # 1. ä½¿ç”¨é¢‘ç‡ç›´æ–¹å›¾
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.hist(counts_array, bins=50, alpha=0.7, color='skyblue')
        plt.xlabel('ä½¿ç”¨æ¬¡æ•°')
        plt.ylabel('ç å­—æ•°é‡')
        plt.title(f'{model_config["name"]} - ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)
        
        # 2. ç´¯ç§¯åˆ†å¸ƒ
        plt.subplot(2, 2, 2)
        sorted_counts = np.sort(counts_array)[::-1]
        cumulative_usage = np.cumsum(sorted_counts) / np.sum(sorted_counts)
        plt.plot(range(len(sorted_counts)), cumulative_usage, linewidth=2)
        plt.xlabel('ç å­—æ’åºï¼ˆæŒ‰ä½¿ç”¨é¢‘ç‡ï¼‰')
        plt.ylabel('ç´¯ç§¯ä½¿ç”¨æ¯”ä¾‹')
        plt.title('ç´¯ç§¯ä½¿ç”¨åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)
        
        # 3. Top 50 ç å­—ä½¿ç”¨æƒ…å†µ
        plt.subplot(2, 2, 3)
        top_50 = usage_counts.most_common(50)
        indices, counts = zip(*top_50)
        plt.bar(range(len(indices)), counts, alpha=0.7, color='lightcoral')
        plt.xlabel('ç å­—æ’åºï¼ˆæŒ‰ä½¿ç”¨é¢‘ç‡ï¼‰')
        plt.ylabel('ä½¿ç”¨æ¬¡æ•°')
        plt.title('Top 50 æœ€å¸¸ç”¨ç å­—')
        plt.grid(True, alpha=0.3)
        
        # 4. ä½¿ç”¨ç‡vsæœªä½¿ç”¨ç å­—
        plt.subplot(2, 2, 4)
        used_codes = len(unique_indices)
        unused_codes = total_codes - used_codes
        plt.pie([used_codes, unused_codes], 
                labels=[f'å·²ä½¿ç”¨: {used_codes}', f'æœªä½¿ç”¨: {unused_codes}'],
                autopct='%1.1f%%',
                colors=['lightgreen', 'lightgray'])
        plt.title('ç å­—ä½¿ç”¨ç‡')
        
        plt.tight_layout()
        save_path = f"codebook_analysis/{model_config['short_name']}_analysis.png"
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“ åˆ†æå›¾è¡¨å·²ä¿å­˜: {save_path}")
    
    # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼Œç”Ÿæˆå¯¹æ¯”åˆ†æ
    if len(analysis_results) > 1:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ")
        print(f"{'='*60}")
        
        plt.figure(figsize=(15, 10))
        
        model_names = []
        usage_rates = []
        used_codes_list = []
        mean_usage = []
        std_usage = []
        
        for short_name, results in analysis_results.items():
            model_names.append(short_name)
            usage_rates.append(results['usage_rate'])
            used_codes_list.append(results['used_codes'])
            mean_usage.append(results['counts_array'].mean())
            std_usage.append(results['counts_array'].std())
        
        # 1. ä½¿ç”¨ç‡å¯¹æ¯”
        plt.subplot(2, 3, 1)
        bars = plt.bar(model_names, usage_rates, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        plt.ylabel('ç æœ¬ä½¿ç”¨ç‡')
        plt.title('ä¸åŒæ¨¡å‹çš„ç æœ¬ä½¿ç”¨ç‡å¯¹æ¯”')
        plt.ylim(0, max(usage_rates) * 1.2)
        for i, (bar, rate) in enumerate(zip(bars, usage_rates)):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01, 
                    f'{rate:.1%}', ha='center', va='bottom')
        
        # 2. ä½¿ç”¨ç å­—æ•°å¯¹æ¯”
        plt.subplot(2, 3, 2)
        bars = plt.bar(model_names, used_codes_list, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        plt.ylabel('ä½¿ç”¨çš„ç å­—æ•°')
        plt.title('ä¸åŒæ¨¡å‹ä½¿ç”¨çš„ç å­—æ•°å¯¹æ¯”')
        for i, (bar, count) in enumerate(zip(bars, used_codes_list)):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1, 
                    f'{count}', ha='center', va='bottom')
        
        # 3. å¹³å‡ä½¿ç”¨æ¬¡æ•°å¯¹æ¯”
        plt.subplot(2, 3, 3)
        bars = plt.bar(model_names, mean_usage, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        plt.ylabel('å¹³å‡ä½¿ç”¨æ¬¡æ•°')
        plt.title('ä¸åŒæ¨¡å‹çš„å¹³å‡ç å­—ä½¿ç”¨æ¬¡æ•°')
        for i, (bar, mean_val) in enumerate(zip(bars, mean_usage)):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5, 
                    f'{mean_val:.1f}', ha='center', va='bottom')
        
        # 4. ä½¿ç”¨åˆ†å¸ƒçš„æ ‡å‡†å·®å¯¹æ¯”
        plt.subplot(2, 3, 4)
        bars = plt.bar(model_names, std_usage, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        plt.ylabel('ä½¿ç”¨æ¬¡æ•°æ ‡å‡†å·®')
        plt.title('ä¸åŒæ¨¡å‹çš„ä½¿ç”¨åˆ†å¸ƒå‡åŒ€æ€§')
        for i, (bar, std_val) in enumerate(zip(bars, std_usage)):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5, 
                    f'{std_val:.1f}', ha='center', va='bottom')
        
        # 5. ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒå¯¹æ¯”
        plt.subplot(2, 3, 5)
        for short_name, results in analysis_results.items():
            counts_array = results['counts_array']
            plt.hist(counts_array, bins=30, alpha=0.5, label=short_name, density=True)
        plt.xlabel('ä½¿ç”¨æ¬¡æ•°')
        plt.ylabel('å¯†åº¦')
        plt.title('ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒå¯¹æ¯”')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. æ€»ç»“è¡¨æ ¼
        plt.subplot(2, 3, 6)
        plt.axis('off')
        
        table_data = []
        headers = ['æ¨¡å‹', 'ä½¿ç”¨ç‡', 'ä½¿ç”¨ç å­—', 'å¹³å‡ä½¿ç”¨æ¬¡æ•°', 'æ ‡å‡†å·®']
        
        for i, short_name in enumerate(model_names):
            results = analysis_results[short_name]
            table_data.append([
                short_name,
                f'{results["usage_rate"]:.1%}',
                f'{results["used_codes"]}',
                f'{results["counts_array"].mean():.1f}',
                f'{results["counts_array"].std():.1f}'
            ])
        
        table = plt.table(cellText=table_data, colLabels=headers, 
                         cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.5)
        plt.title('ç æœ¬ä½¿ç”¨ç»Ÿè®¡æ±‡æ€»', pad=20)
        
        plt.tight_layout()
        comparison_path = "codebook_analysis/model_comparison.png"
        plt.savefig(comparison_path, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“ å¯¹æ¯”åˆ†æå›¾è¡¨å·²ä¿å­˜: {comparison_path}")
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ“‹ æ€»ç»“:")
        print(f"  æœ€é«˜ä½¿ç”¨ç‡æ¨¡å‹: {model_names[np.argmax(usage_rates)]} ({max(usage_rates):.1%})")
        print(f"  æœ€å‡åŒ€åˆ†å¸ƒæ¨¡å‹: {model_names[np.argmin(std_usage)]} (æ ‡å‡†å·®: {min(std_usage):.1f})")
        print(f"  ä½¿ç”¨ç å­—æœ€å¤šæ¨¡å‹: {model_names[np.argmax(used_codes_list)]} ({max(used_codes_list)} ä¸ªç å­—)")

if __name__ == "__main__":
    analyze_codebook_usage() 
