#!/usr/bin/env python3
"""
åŸºäºå®˜æ–¹taming-transformersçš„Kaggle VQ-VAEè®­ç»ƒç¯å¢ƒ
å®Œå…¨å…¼å®¹å®˜æ–¹å®ç°ï¼Œé’ˆå¯¹P100 16GB GPUä¼˜åŒ–
"""

import os
import sys
import subprocess
import torch
import shutil
from pathlib import Path

def setup_kaggle_taming_environment():
    """è®¾ç½®Kaggleç¯å¢ƒ - åŸºäºå®˜æ–¹taming-transformers"""
    print("ğŸ”§ åŸºäºå®˜æ–¹taming-transformersè®¾ç½®Kaggle VQ-VAEç¯å¢ƒ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("ğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    print()
    
    # æ£€æŸ¥taming-transformers-masteræ˜¯å¦å­˜åœ¨
    taming_dir = Path("../taming-transformers-master").resolve()
    if not taming_dir.exists():
        print(f"âŒ æœªæ‰¾åˆ°taming-transformers-masterç›®å½•: {taming_dir}")
        print("è¯·ç¡®ä¿taming-transformers-masterç›®å½•åœ¨æ­£ç¡®ä½ç½®")
        return False
    
    print(f"âœ… æ‰¾åˆ°taming-transformers: {taming_dir}")
    
    # å®‰è£…å¿…è¦ä¾èµ–
    print("ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...")
    packages = [
        "omegaconf",
        "einops", 
        "lpips",
        "transformers",
        "albumentations",
        "kornia",
        "pytorch-lightning"
    ]
    
    for package in packages:
        try:
            print(f"å®‰è£… {package}...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", package, "--quiet"
            ])
        except subprocess.CalledProcessError as e:
            print(f"å®‰è£… {package} å¤±è´¥: {e}")
    
    # æ·»åŠ taming-transformersåˆ°Pythonè·¯å¾„
    sys.path.insert(0, str(taming_dir))
    
    # éªŒè¯èƒ½å¦å¯¼å…¥tamingæ¨¡å—
    try:
        from taming.models.vqgan import VQModel
        print("âœ… taming.models.vqganå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    work_dirs = [
        "checkpoints",
        "logs", 
        "results",
        "configs"
    ]
    
    for directory in work_dirs:
        Path(directory).mkdir(exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")
    
    print("\nğŸ‰ Kaggle taming-transformersç¯å¢ƒè®¾ç½®å®Œæˆ!")
    return True

def create_kaggle_custom_dataset():
    """åˆ›å»ºKaggleè‡ªå®šä¹‰æ•°æ®é›†ç±»"""
    dataset_code = '''
import os
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import glob
import numpy as np
from pathlib import Path

class KaggleImageDataset(Dataset):
    """Kaggleç¯å¢ƒè‡ªå®šä¹‰å›¾åƒæ•°æ®é›† - å…¼å®¹taming-transformers"""
    
    def __init__(self, data_path="/kaggle/input/dataset", size=256, 
                 random_crop=False, interpolation="bicubic"):
        self.data_path = data_path
        self.size = size
        self.random_crop = random_crop
        self.interpolation = interpolation
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.image_paths = self._collect_image_paths()
        print(f"ğŸ“¸ Kaggleæ•°æ®é›†: {len(self.image_paths)} å¼ å›¾åƒ")
        
        # è®¾ç½®å›¾åƒå˜æ¢
        self.transform = self._get_transform()
    
    def _collect_image_paths(self):
        """æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„"""
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.webp']
        image_paths = []
        
        data_path = Path(self.data_path)
        for ext in extensions:
            paths = list(data_path.rglob(ext))  # é€’å½’æœç´¢
            image_paths.extend([str(p) for p in paths])
        
        return sorted(image_paths)
    
    def _get_transform(self):
        """è·å–å›¾åƒå˜æ¢ - ä¸tamingå…¼å®¹"""
        if self.random_crop:
            # è®­ç»ƒæ—¶çš„å˜æ¢
            transform_list = [
                transforms.Resize(self.size),
                transforms.RandomCrop(self.size),
                transforms.RandomHorizontalFlip(p=0.5),
            ]
        else:
            # éªŒè¯æ—¶çš„å˜æ¢
            transform_list = [
                transforms.Resize(self.size),
                transforms.CenterCrop(self.size),
            ]
        
        transform_list.extend([
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # æ ‡å‡†åŒ–åˆ°[-1,1]
        ])
        
        return transforms.Compose(transform_list)
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        try:
            image_path = self.image_paths[idx]
            image = Image.open(image_path).convert('RGB')
            image = self.transform(image)
            
            # è¿”å›tamingæœŸæœ›çš„æ ¼å¼
            return {"image": image}
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥ {self.image_paths[idx]}: {e}")
            # è¿”å›éšæœºå›¾åƒä½œä¸ºå¤‡ç”¨
            random_image = torch.randn(3, self.size, self.size)
            random_image = torch.tanh(random_image)  # é™åˆ¶åˆ°[-1,1]
            return {"image": random_image}

class KaggleDataModule:
    """Kaggleæ•°æ®æ¨¡å— - æ›¿ä»£å®˜æ–¹DataModuleFromConfig"""
    
    def __init__(self, data_path="/kaggle/input/dataset", batch_size=12, 
                 num_workers=2, image_size=256):
        self.data_path = data_path
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.image_size = image_size
        
        # åˆ›å»ºæ•°æ®é›†
        self.setup_datasets()
    
    def setup_datasets(self):
        """è®¾ç½®è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†"""
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = KaggleImageDataset(
            data_path=self.data_path,
            size=self.image_size,
            random_crop=True  # è®­ç»ƒæ—¶éšæœºè£å‰ª
        )
        
        # åˆ†å‰²æ•°æ®é›†
        total_size = len(full_dataset)
        train_size = int(total_size * 0.8)
        val_size = int(total_size * 0.1)
        test_size = total_size - train_size - val_size
        
        self.train_dataset, self.val_dataset, self.test_dataset = \\
            torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²:")
        print(f"  è®­ç»ƒé›†: {train_size} å¼ ")
        print(f"  éªŒè¯é›†: {val_size} å¼ ")
        print(f"  æµ‹è¯•é›†: {test_size} å¼ ")
    
    def train_dataloader(self):
        """è®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            drop_last=True
        )
    
    def val_dataloader(self):
        """éªŒè¯æ•°æ®åŠ è½½å™¨"""
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )
    
    def test_dataloader(self):
        """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        return torch.utils.data.DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )
'''
    
    with open("kaggle_dataset.py", "w", encoding='utf-8') as f:
        f.write(dataset_code)
    
    print("âœ… Kaggleè‡ªå®šä¹‰æ•°æ®é›†åˆ›å»ºå®Œæˆ")

def create_p100_vqgan_config():
    """åˆ›å»ºP100 GPUä¼˜åŒ–çš„VQ-GANé…ç½®"""
    config = '''# Kaggle P100 16GB VQ-GANé…ç½®
# åŸºäºå®˜æ–¹taming-transformersï¼Œé’ˆå¯¹P100ä¼˜åŒ–

model:
  base_learning_rate: 4.5e-6   # å®˜æ–¹æ¨èå­¦ä¹ ç‡
  target: taming.models.vqgan.VQModel
  params:
    embed_dim: 256
    n_embed: 8192              # é€‚ä¸­ç æœ¬å¤§å°ï¼Œå¹³è¡¡è´¨é‡å’Œå†…å­˜
    
    ddconfig:
      double_z: false
      z_channels: 256
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128                  # åŸºç¡€é€šé“æ•°
      ch_mult: [1,1,2,4]      # P100å†…å­˜ä¼˜åŒ–ï¼šå‡å°‘å±‚æ•°
      num_res_blocks: 2
      attn_resolutions: [32]   # åªåœ¨32x32åˆ†è¾¨ç‡ä½¿ç”¨æ³¨æ„åŠ›
      dropout: 0.0

    lossconfig:
      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
      params:
        disc_conditional: false
        disc_in_channels: 3
        disc_start: 30001       # è¾ƒæ—©å¯åŠ¨åˆ¤åˆ«å™¨
        disc_weight: 0.6        # é€‚ä¸­çš„åˆ¤åˆ«å™¨æƒé‡
        codebook_weight: 1.0
        pixelloss_weight: 1.0
        perceptual_weight: 1.0

# Lightningè®­ç»ƒå™¨é…ç½®
lightning:
  trainer:
    accelerator: "gpu"
    devices: 1
    precision: 16             # æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
    max_epochs: 100
    check_val_every_n_epoch: 2
    accumulate_grad_batches: 2  # æ¢¯åº¦ç´¯ç§¯ï¼šç­‰æ•ˆbatch_size=24
    gradient_clip_val: 1.0
    log_every_n_steps: 50
    enable_progress_bar: true
    
  callbacks:
    model_checkpoint:
      dirpath: "checkpoints"
      filename: "vqgan-p100-{epoch:02d}-{val_rec_loss:.4f}"
      monitor: "val/rec_loss"
      mode: "min"
      save_top_k: 3
      save_last: true
      every_n_epochs: 5
      
    image_logger:
      batch_frequency: 500     # æ¯500æ­¥è®°å½•ä¸€æ¬¡å›¾åƒ
      max_images: 8

# P100æ€§èƒ½é¢„æœŸ
performance_expectations:
  memory_usage: "~14GB"
  time_per_epoch: "15-25åˆ†é’Ÿ"
  recommended_batch_size: 12
  effective_batch_size: 24    # é€šè¿‡æ¢¯åº¦ç´¯ç§¯
  
# è®­ç»ƒæç¤º
training_tips:
  - "ä½¿ç”¨mixed precision (16-bit) èŠ‚çœå†…å­˜"
  - "batch_size=12æ˜¯P100çš„æœ€ä¼˜è®¾ç½®"
  - "accumulate_grad_batches=2å¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°"
  - "ç›‘æ§val/rec_lossä½œä¸ºä¸»è¦æŒ‡æ ‡"
  - "å»ºè®®è®­ç»ƒ80-100ä¸ªepoch"
'''
    
    with open("configs/kaggle_p100_vqgan.yaml", "w", encoding='utf-8') as f:
        f.write(config)
    
    print("âœ… P100ä¼˜åŒ–VQ-GANé…ç½®åˆ›å»ºå®Œæˆ")

def create_kaggle_training_script():
    """åˆ›å»ºKaggleè®­ç»ƒè„šæœ¬"""
    script = '''#!/usr/bin/env python3
"""
Kaggle VQ-GANè®­ç»ƒè„šæœ¬
åŸºäºå®˜æ–¹taming-transformersï¼ŒP100 16GBä¼˜åŒ–
"""

import os
import sys
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
from omegaconf import OmegaConf
from pathlib import Path

# æ·»åŠ taming-transformersåˆ°è·¯å¾„
sys.path.insert(0, str(Path("../taming-transformers-master").resolve()))

def main():
    print("ğŸš€ å¼€å§‹Kaggle VQ-GANè®­ç»ƒ (åŸºäºå®˜æ–¹taming-transformers)")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°GPU")
        return
        
    print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # å¯¼å…¥tamingæ¨¡å—
    try:
        from taming.models.vqgan import VQModel
        print("âœ… æˆåŠŸå¯¼å…¥taming.models.vqgan.VQModel")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥taming-transformers-masterè·¯å¾„")
        return
    
    # å¯¼å…¥è‡ªå®šä¹‰æ•°æ®æ¨¡å—
    try:
        from kaggle_dataset import KaggleDataModule
        print("âœ… æˆåŠŸå¯¼å…¥KaggleDataModule")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥kaggle_datasetå¤±è´¥: {e}")
        return
    
    # åŠ è½½é…ç½®
    config_path = "configs/kaggle_p100_vqgan.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
        
    config = OmegaConf.load(config_path)
    print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    try:
        data_module = KaggleDataModule(
            data_path="/kaggle/input/dataset",
            batch_size=12,  # P100ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
            num_workers=2,
            image_size=256
        )
        print("âœ… æ•°æ®æ¨¡å—åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®æ¨¡å—åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ¨¡å‹
    try:
        model = VQModel(**config.model.params)
        model.learning_rate = config.model.base_learning_rate
        print(f"âœ… VQ-GANæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # è®¡ç®—æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°: {total_params/1e6:.2f}M")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.2f}M")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åˆ›å»ºå›è°ƒå‡½æ•°
    callbacks = []
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath="checkpoints",
        filename="vqgan-kaggle-{epoch:02d}-{val_rec_loss:.4f}",
        monitor="val/rec_loss",
        mode="min",
        save_top_k=3,
        save_last=True,
        every_n_epochs=5,
        save_weights_only=False
    )
    callbacks.append(checkpoint_callback)
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = TensorBoardLogger(
        save_dir="logs",
        name="vqgan_kaggle_p100",
        version=None
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        accelerator="gpu",
        devices=1,
        precision=16,  # æ··åˆç²¾åº¦
        max_epochs=100,
        callbacks=callbacks,
        logger=logger,
        log_every_n_steps=50,
        check_val_every_n_epoch=2,
        accumulate_grad_batches=2,  # æ¢¯åº¦ç´¯ç§¯
        gradient_clip_val=1.0,
        enable_progress_bar=True,
        benchmark=True
    )
    
    print("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        print("\\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("ğŸ“Š è®­ç»ƒé…ç½®:")
        print(f"  - æ‰¹æ¬¡å¤§å°: 12 (æœ‰æ•ˆ: 24)")
        print(f"  - å­¦ä¹ ç‡: {config.model.base_learning_rate}")
        print(f"  - ç æœ¬å¤§å°: {config.model.params.n_embed}")
        print(f"  - æœ€å¤§epochs: 100")
        print(f"  - æ··åˆç²¾åº¦: 16-bit")
        
        trainer.fit(model, data_module)
        print("\\nâœ… è®­ç»ƒå®Œæˆ!")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = "checkpoints/final_vqgan_model.ckpt"
        trainer.save_checkpoint(final_path)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
'''
    
    with open("train_kaggle_vqgan.py", "w", encoding='utf-8') as f:
        f.write(script)
    
    os.chmod("train_kaggle_vqgan.py", 0o755)
    print("âœ… Kaggleè®­ç»ƒè„šæœ¬åˆ›å»ºå®Œæˆ")

def create_evaluation_script():
    """åˆ›å»ºæ¨¡å‹è¯„ä¼°è„šæœ¬"""
    eval_script = '''#!/usr/bin/env python3
"""
VQ-GANæ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨äºæ£€æŸ¥è®­ç»ƒç»“æœå’Œç”Ÿæˆæ ·æœ¬
"""

import os
import sys
import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from PIL import Image
import torchvision.transforms as T

# æ·»åŠ taming-transformersåˆ°è·¯å¾„
sys.path.insert(0, str(Path("../taming-transformers-master").resolve()))

def load_model(checkpoint_path):
    """åŠ è½½è®­ç»ƒå¥½çš„VQ-GANæ¨¡å‹"""
    try:
        from taming.models.vqgan import VQModel
        
        # ä»æ£€æŸ¥ç‚¹åŠ è½½
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model = VQModel.load_from_checkpoint(checkpoint_path)
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {checkpoint_path}")
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def evaluate_reconstruction(model, image_path, device='cuda'):
    """è¯„ä¼°é‡å»ºè´¨é‡"""
    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    transform = T.Compose([
        T.Resize(256),
        T.CenterCrop(256),
        T.ToTensor(),
        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    
    x = transform(image).unsqueeze(0).to(device)
    model = model.to(device)
    
    with torch.no_grad():
        # ç¼–ç 
        quant, _, info = model.encode(x)
        
        # è§£ç 
        xrec = model.decode(quant)
        
        # è®¡ç®—é‡å»ºæŸå¤±
        rec_loss = torch.nn.functional.mse_loss(x, xrec)
        
        # ç æœ¬ä½¿ç”¨æƒ…å†µ
        indices = info[2].flatten()
        unique_indices = torch.unique(indices)
        codebook_usage = len(unique_indices) / model.quantize.n_embed
        
    return {
        'original': x,
        'reconstructed': xrec,
        'rec_loss': rec_loss.item(),
        'codebook_usage': codebook_usage,
        'quantized_indices': indices
    }

def visualize_results(results, save_path="evaluation_results.png"):
    """å¯è§†åŒ–é‡å»ºç»“æœ"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # åå½’ä¸€åŒ–å‡½æ•°
    def denorm(x):
        return (x + 1) / 2
    
    # åŸå§‹å›¾åƒ
    orig_img = denorm(results['original'].squeeze().cpu()).permute(1, 2, 0)
    axes[0].imshow(orig_img.numpy())
    axes[0].set_title("åŸå§‹å›¾åƒ")
    axes[0].axis('off')
    
    # é‡å»ºå›¾åƒ
    rec_img = denorm(results['reconstructed'].squeeze().cpu()).permute(1, 2, 0)
    axes[1].imshow(rec_img.numpy())
    axes[1].set_title(f"é‡å»ºå›¾åƒ\\nMSE Loss: {results['rec_loss']:.4f}")
    axes[1].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… ç»“æœå¯è§†åŒ–å·²ä¿å­˜: {save_path}")

def main():
    print("ğŸ“Š VQ-GANæ¨¡å‹è¯„ä¼°")
    print("=" * 40)
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
    checkpoint_dir = Path("checkpoints")
    if not checkpoint_dir.exists():
        print("âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨")
        return
    
    checkpoints = list(checkpoint_dir.glob("*.ckpt"))
    if not checkpoints:
        print("âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„æ£€æŸ¥ç‚¹
    latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ” ä½¿ç”¨æ£€æŸ¥ç‚¹: {latest_checkpoint.name}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(str(latest_checkpoint))
    if model is None:
        return
    
    # æŸ¥æ‰¾æµ‹è¯•å›¾åƒ
    test_images = list(Path("/kaggle/input/dataset").rglob("*.jpg"))[:5]
    if not test_images:
        test_images = list(Path("/kaggle/input/dataset").rglob("*.png"))[:5]
    
    if not test_images:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ")
        return
    
    print(f"ğŸ–¼ï¸ æ‰¾åˆ° {len(test_images)} å¼ æµ‹è¯•å›¾åƒ")
    
    # è¯„ä¼°æ¯å¼ å›¾åƒ
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    all_losses = []
    all_codebook_usage = []
    
    for i, img_path in enumerate(test_images):
        print(f"è¯„ä¼°å›¾åƒ {i+1}/{len(test_images)}: {img_path.name}")
        
        try:
            results = evaluate_reconstruction(model, str(img_path), device)
            all_losses.append(results['rec_loss'])
            all_codebook_usage.append(results['codebook_usage'])
            
            # ä¿å­˜å¯è§†åŒ–ç»“æœ
            save_path = f"evaluation_image_{i+1}.png"
            visualize_results(results, save_path)
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            continue
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    if all_losses:
        print("\\nğŸ“ˆ è¯„ä¼°æ€»ç»“:")
        print(f"å¹³å‡é‡å»ºæŸå¤±: {np.mean(all_losses):.4f} Â± {np.std(all_losses):.4f}")
        print(f"å¹³å‡ç æœ¬ä½¿ç”¨ç‡: {np.mean(all_codebook_usage):.2%} Â± {np.std(all_codebook_usage):.2%}")
        
        # æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters()) / 1e6
        print(f"æ¨¡å‹å‚æ•°: {total_params:.2f}M")
        print(f"ç æœ¬å¤§å°: {model.quantize.n_embed}")
        print(f"åµŒå…¥ç»´åº¦: {model.quantize.embed_dim}")

if __name__ == "__main__":
    main()
'''
    
    with open("evaluate_vqgan.py", "w", encoding='utf-8') as f:
        f.write(eval_script)
    
    os.chmod("evaluate_vqgan.py", 0o755)
    print("âœ… è¯„ä¼°è„šæœ¬åˆ›å»ºå®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ åŸºäºå®˜æ–¹taming-transformersçš„Kaggle VQ-VAEç¯å¢ƒ")
    print("=" * 60)
    
    # 1. ç¯å¢ƒè®¾ç½®
    if not setup_kaggle_taming_environment():
        print("âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥")
        return
    
    # 2. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†
    create_kaggle_custom_dataset()
    
    # 3. åˆ›å»ºP100ä¼˜åŒ–é…ç½®
    create_p100_vqgan_config()
    
    # 4. åˆ›å»ºè®­ç»ƒè„šæœ¬
    create_kaggle_training_script()
    
    # 5. åˆ›å»ºè¯„ä¼°è„šæœ¬
    create_evaluation_script()
    
    print("\nğŸ‰ å®Œæ•´çš„Kaggle VQ-GANè®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ!")
    print("\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
    print("1. ç¡®ä¿taming-transformers-masteråœ¨ä¸Šçº§ç›®å½•")
    print("2. åœ¨Kaggle Notebookä¸­è¿è¡Œ:")
    print("   !cd VQ-VAE && python kaggle_vqvae_setup.py")
    print("3. å¼€å§‹è®­ç»ƒ:")
    print("   !cd VQ-VAE && python train_kaggle_vqgan.py")
    print("4. è¯„ä¼°æ¨¡å‹:")
    print("   !cd VQ-VAE && python evaluate_vqgan.py")
    
    print("\nğŸ’¡ å…³é”®ç‰¹æ€§:")
    print("âœ… 100%åŸºäºå®˜æ–¹taming-transformers")
    print("âœ… P100 16GBä¸“é—¨ä¼˜åŒ– (batch_size=12)")
    print("âœ… æ··åˆç²¾åº¦è®­ç»ƒèŠ‚çœå†…å­˜")
    print("âœ… æ¢¯åº¦ç´¯ç§¯å¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°")
    print("âœ… è‡ªåŠ¨é€‚é… /kaggle/input/dataset")
    print("âœ… å®Œæ•´çš„è®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–")
    
    print("\nğŸ”§ P100ä¼˜åŒ–é…ç½®:")
    print(f"- ç æœ¬å¤§å°: 8192")
    print(f"- æ‰¹æ¬¡å¤§å°: 12 (æœ‰æ•ˆ: 24)")
    print(f"- å†…å­˜ä½¿ç”¨: ~14GB")
    print(f"- é¢„è®¡è®­ç»ƒæ—¶é—´: 15-25åˆ†é’Ÿ/epoch")

if __name__ == "__main__":
    main() 
